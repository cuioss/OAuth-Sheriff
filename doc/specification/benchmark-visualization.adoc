= JWT Validation Benchmark Visualization
:toc:
:toclevels: 3
:toc-title: Table of Contents
:sectnums:

This document describes the benchmark visualization system implemented for the cui-jwt-validation project.

== Overview

The benchmark visualization system provides an interactive way to view and analyze JMH benchmark results. The project uses JMH Visualizer (https://jmh.morethan.io/), a powerful visualization tool for JMH benchmark results.

== Accessing Benchmark Results

Benchmark results are available in two main ways:

=== GitHub Pages

Benchmark results are automatically published to the cuioss.github.io site after each benchmark run. They can be accessed at:

`https://cuioss.github.io/cui-jwt-validation/benchmarks/`

The page will automatically load the latest benchmark results into JMH Visualizer.

=== Local Viewing

For local development and testing, you can view benchmark results by directly uploading them to the JMH Visualizer website:

1. Go to https://jmh.morethan.io/
2. Click "Choose File" or drag and drop your benchmark results JSON file
3. Explore the visualization

Alternatively, you can open the visualizer with your results file directly in your browser by opening this URL (replacing the path with your actual results file path):

[source]
----
https://jmh.morethan.io/?source=file:///path/to/your/jmh-result.json
----

JMH Visualizer will load your benchmark data and display it with interactive charts and analysis tools.

*Note:* The JMH benchmark tool generates results files in JSON format, typically named `jmh-result.json` or similar, in the target directory after running benchmarks.

== JMH Visualizer Features

JMH Visualizer provides the following features:

1. *Interactive Charts*: Visual representation of benchmark results with different chart types
2. *Benchmark Filtering*: Filter benchmarks by name or type
3. *Detailed Results*: Comprehensive view of benchmark scores, error margins, and units
4. *Comparison Views*: Compare benchmark results across different modes
5. *Export Options*: Download charts as images or raw data

== Running Benchmarks

=== Automated Runs

Benchmarks are automatically run:

* On pushes to the main branch
* On version tag pushes (e.g., v1.2.3)

=== Manual Trigger

To manually trigger a benchmark run:

1. Go to the "Actions" tab in the GitHub repository
2. Select "JMH Benchmark" from the workflows list
3. Click "Run workflow"
4. Select the branch to run on and click "Run workflow"

== Using Performance Badges

The benchmark system generates performance badges that can be included in the project's README or other documentation.

=== Available Badges

* *Access Token Validation*: Shows the average time for validating access tokens

=== Adding Badges to Documentation

Badge markdown is available in the GitHub Pages deployment under `badge-markdown.txt`. To add a badge to your documentation:

1. Copy the markdown from `badge-markdown.txt`
2. Paste it into your README.adoc or other documentation file

Example badge markdown:

[source,markdown]
----
[![Access Token Validation](https://img.shields.io/endpoint?url=https://cuioss.github.io/cui-jwt-validation/benchmarks/validator-badge.json)](https://cuioss.github.io/cui-jwt-validation/benchmarks/)
----

== Interpreting Results

=== Understanding Benchmark Metrics

* *Score*: The primary metric value (lower is better for average time, higher is better for throughput)
* *Error*: The statistical error margin (±)
* *Unit*: The unit of measurement (ops/s for throughput, ms or μs for time)

=== Performance Goals

Compare benchmark results against the performance goals defined in link:benchmark.adoc#_performance_requirements[Performance Requirements].

== Implementation Details

The benchmark visualization system is implemented using:

* *GitHub Actions*: For running benchmarks and deploying results
* *External Repository Deployment*: Results are deployed to cuioss.github.io
* *Chart.js*: For interactive charts
* *JMH JSON Output*: For structured benchmark data
* *Shields.io*: For dynamic performance badges

For technical details of the implementation, see:

* `.github/workflows/benchmark.yml`: GitHub Actions workflow configuration
* `.github/workflows/README.md`: Documentation for GitHub workflows

== Related Documentation

* link:benchmark.adoc[Benchmark Specification]
* xref:../Requirements.adoc#CUI-JWT-5[CUI-JWT-5: Performance Requirements]
