= JWT Validation Benchmark Specification
:toc:
:toclevels: 3
:toc-title: Table of Contents
:sectnums:

xref:../Specification.adoc[Back to Main Specification]

== Overview
_See Requirement xref:../Requirements.adoc#CUI-JWT-5[CUI-JWT-5: Performance]_

This document provides detailed specifications for the performance benchmarking aspects of the JWT Token Validation library.

=== Document Navigation

* xref:../../README.adoc[README] - Project overview and introduction
* xref:../../cui-jwt-validation/README.adoc[Usage Guide] - How to use the library with code examples
* xref:../Requirements.adoc[Requirements] - Functional and non-functional requirements
* xref:../Specification.adoc[Specification] - Technical specifications
* xref:../LogMessages.adoc[Log Messages] - Reference for all log messages
* xref:../Build.adoc[Building and Development] - Information for contributors
* link:technical-components.adoc[Technical Components] - Implementation details
* link:testing.adoc[Testing] - Testing approach and coverage
* link:security-specifications.adoc[Security] - Security specifications

[[performance_requirements]]
== Performance Requirements

=== Token Validation Performance
_See Requirement xref:../Requirements.adoc#CUI-JWT-5.1[CUI-JWT-5.1: Token Validation Performance]_

==== Status: IMPLEMENTED

The library requires efficient token validation to ensure minimal performance impact on applications:

* Access token validation: < 1ms average time
* ID token validation: < 1.5ms average time
* 99th percentile: < 5ms

The following classes implement this specification:

* xref:../../cui-jwt-validation/src/main/java/de/cuioss/jwt/validation/TokenValidator.java[TokenValidator]
* xref:../../cui-jwt-validation/src/main/java/de/cuioss/jwt/validation/pipeline/TokenSignatureValidator.java[TokenSignatureValidator]

The following tests verify the implementation:

* JMH benchmark classes:
  * `de.cuioss.jwt.validation.benchmark.TokenValidatorBenchmark`
  * `de.cuioss.jwt.validation.benchmark.FailureScenarioBenchmark`

=== JWKS Key Retrieval Performance
_See Requirement xref:../Requirements.adoc#CUI-JWT-5.3[CUI-JWT-5.3: Performance Monitoring]_

==== Status: IMPLEMENTED

The JWKS client must efficiently retrieve and cache keys to support high-volume token validation:

* Key retrieval (cached): < 0.1ms
* Key retrieval (uncached): < 50ms
* JWKS refresh: < 200ms

The following classes implement this specification:

* xref:../../cui-jwt-validation/src/main/java/de/cuioss/jwt/validation/jwks/http/HttpJwksLoader.java[HttpJwksLoader]
* xref:../../cui-jwt-validation/src/main/java/de/cuioss/jwt/validation/jwks/JwksLoaderFactory.java[JwksLoaderFactory]

The following tests verify the implementation:

* JMH benchmark classes:
  * `de.cuioss.jwt.validation.benchmark.JwksClientBenchmark`
  * `de.cuioss.jwt.validation.benchmark.JwksClientFailureBenchmark`

=== Concurrent Performance
_See Requirement xref:../Requirements.adoc#CUI-JWT-5.2[CUI-JWT-5.2: Concurrent Token Processing]_

==== Status: IMPLEMENTED

The library must scale efficiently with concurrent validation requests:

* Linear scaling up to 16 threads
* No more than 50% degradation at 100 threads

The following class implements this specification:

* xref:../../cui-jwt-validation/src/main/java/de/cuioss/jwt/validation/TokenValidator.java[TokenValidator]

The following tests verify the implementation:

* JMH benchmark classes:
  * `de.cuioss.jwt.validation.benchmark.ConcurrentTokenValidationBenchmark`

== Running Benchmarks

=== Maven Profile

The project provides a Maven profile for running benchmarks:

[source,bash]
----
mvn clean verify -Pbenchmark
----

This executes all benchmarks and generates results in `jmh-result.json`.

=== Visualizing Results

The project uses JMH Visualizer (https://jmh.morethan.io/) for benchmark visualization. There are two ways to visualize your results:

1. **GitHub Pages**: Benchmark results are automatically published to:
   [https://cuioss.github.io/cui-jwt-validation/benchmarks/](https://cuioss.github.io/cui-jwt-validation/benchmarks/)

2. **Local Visualization**: After running benchmarks:
   - Go to [https://jmh.morethan.io/](https://jmh.morethan.io/)
   - Upload your benchmark results file (e.g., `target/jmh-result.json`)
   - Explore the interactive visualizations

See link:benchmark-visualization.adoc[Benchmark Visualization] for more details.

=== Failure Handling Performance

==== Status: IMPLEMENTED

Error cases must be handled efficiently to prevent performance degradation during attack scenarios:

* Invalid token validation: < 2ms average time
* Non-existent key lookup: < 0.5ms (cached mode)
* Server error recovery: < 100ms
* Exception generation overhead: < 0.5ms per exception
* Maximum throughput degradation during 50% error rate: < 40%

The following classes implement this specification:

* xref:../../cui-jwt-validation/src/main/java/de/cuioss/jwt/validation/TokenValidator.java[TokenValidator]
* xref:../../cui-jwt-validation/src/main/java/de/cuioss/jwt/validation/jwks/http/HttpJwksLoader.java[HttpJwksLoader]

The following benchmark classes will verify the implementation:

* `de.cuioss.jwt.validation.benchmark.FailureScenarioBenchmark`
* `de.cuioss.jwt.validation.benchmark.JwksClientFailureBenchmark`
* `de.cuioss.jwt.validation.benchmark.ErrorLoadBenchmark`

== Benchmark Methodology

=== JMH Implementation

The library uses JMH (Java Microbenchmark Harness) to provide accurate and reliable performance measurements:

* Proper warm-up phases to avoid measurement of JVM warm-up effects
* Multiple iterations to ensure statistical significance
* Appropriate benchmark modes for different metrics:
  * `Mode.Throughput` for concurrent performance
  * `Mode.AverageTime` for latency measurements
  * `Mode.SampleTime` for percentile calculations

=== Benchmark Scenarios

The following key scenarios are benchmarked:

1. *Token Validation Performance*
   * Access token validation (varying sizes)
   * ID token validation (varying sizes)
   * Multi-issuer validation

2. *JWKS Client Performance*
   * Key retrieval (cached)
   * Key retrieval (uncached)
   * JWKS refresh operations

3. *Concurrent Performance*
   * Sequential vs. concurrent validation
   * Different thread counts (1, 2, 4, 8, 16, 32, 64, 100)

4. *Failure Scenarios*
   * Invalid signatures
   * Expired tokens
   * Wrong issuer/audience
   * Malformed tokens
   * Missing key IDs (kid)
   * Non-existent key lookups
   * Server errors
   * High error rates (10%, 50%, 90%)

=== Result Analysis

Benchmark results are analyzed to:

1. Verify compliance with performance requirements
2. Identify performance bottlenecks
3. Track performance trends over time
4. Detect performance regressions

=== Benchmark Execution Environment

For consistent and comparable results, benchmarks are executed in a controlled environment:

* Dedicated CI/CD runner for performance tests
* Consistent hardware specifications
* Isolated from other workloads
* Standardized JVM settings

== Integration

=== CI/CD Integration

Benchmarks are integrated into the CI/CD pipeline:

* Scheduled weekly benchmark runs
* Performance regression alerts
* Benchmark result visualization
* Result history tracking

=== Performance Badges

Key performance metrics are displayed as badges in the project documentation:

* Token validation throughput
* Key retrieval latency
* Failure handling efficiency

== Related Documentation

* xref:../Requirements.adoc#CUI-JWT-5[CUI-JWT-5: Performance] - Performance requirements
* link:security-specifications.adoc[Security] - Security specifications with performance implications
* link:technical-components.adoc[Technical Components] - Component specifications

== Token Generation Strategy for Benchmarks

=== In-Memory vs. Integration Token Generation

For performance benchmarking, two approaches for token generation were evaluated:

1. *In-Memory Test Token Generators* (`TestTokenGenerators` approach)
   * *Advantages*:
     ** Fast and predictable token generation
     ** No external dependencies
     ** Fine-grained control over token properties and claims
     ** Consistent token sizes and structures
     ** Repeatable results across environments
     ** No network latency or availability concerns
   * *Disadvantages*:
     ** May not fully represent real-world token complexity
     ** Could miss validation issues that occur with real tokens

2. *Live Tokens from Identity Providers* (Quarkus Integration Tests approach)
   * *Advantages*:
     ** Uses actual OIDC tokens from a real provider
     ** Tests against real-world token structures
     ** Better validates compatibility with external systems
     ** Tests full HTTP client behavior and certificate validation
   * *Disadvantages*:
     ** Adds network latency to benchmarks
     ** Requires external container setup
     ** Less predictable token sizes and structures
     ** More complexity in benchmark setup
     ** May cause test variability due to network conditions

=== Recommended Approach

For benchmarking purposes, the **in-memory token generation** approach is recommended based on the following criteria:

1. *Performance Consistency*: Benchmarks should measure the validation logic itself, not the network or external systems. In-memory generators provide consistent, reproducible results.

2. *Isolation*: Benchmarks should isolate the component being measured. External dependencies can introduce variability that obscures the actual performance characteristics.

3. *Control*: Using in-memory generators allows precise control over token properties, sizes, and claim structures, which is essential for systematic benchmarking across different scenarios.

4. *Simplicity*: Benchmarks should be simple to set up and run. In-memory generators don't require external infrastructure.

5. *JMH Requirements*: JMH microbenchmarks work best with stable, reproducible test cases that can be executed many times with minimal overhead.

=== Implementation Strategy

The benchmark implementation includes:

1. *Token Generators*: The `TestTokenGenerators` class provides comprehensive token generation capabilities with control over token size, claim complexity, and signing algorithms.

2. *Parameterized Benchmarks*: Measures performance across different token sizes, claim structures, and validation scenarios.

3. *Real-World Token Profiles*: Includes test cases that mimic real-world tokens based on observations from actual identity providers.

4. *Supplementary Comparison Tests*: While not part of the core benchmarks, the implementation includes specific comparison tests between in-memory and real tokens to validate that the in-memory approach provides representative results.

While benchmarks use in-memory token generation, integration testing with real identity providers (such as Keycloak) remains an essential part of the overall test strategy to ensure compatibility with real-world systems.
