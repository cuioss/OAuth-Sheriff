= JMH Microbenchmark Implementation Plan
:toc:
:toclevels: 3
:toc-title: Table of Contents
:sectnums:

This document outlines the plan for implementing microbenchmarks using https://github.com/openjdk/jmh[JMH (Java Microbenchmark Harness)] for the cui-jwt-validation project.

== Overview

The goal is to replace the existing performance tests with proper microbenchmarks that provide more accurate, consistent, and comprehensive performance metrics. 

Specifically, we will replace:

* `src/test/java/de/cuioss/jwt/validation/TokenValidatorPerformanceTest.java`
* `src/test/java/de/cuioss/jwt/validation/jwks/JwksClientBenchmarkTest.java`

All benchmarks will be moved to a new package: `de/cuioss/jwt/validation/benchmark`

== Task Structure

=== Categories and Numbering

Tasks are organized into categories:

* *S* - Setup tasks (e.g., S1, S2, S3)
* *B* - Benchmark Implementation tasks (e.g., B1, B2, B3)
* *A* - Analysis and Documentation tasks (e.g., A1, A2, A3)
* *I* - Integration tasks (e.g., I1, I2, I3)

=== Task Format

Each task follows a consistent format:

[source]
----
=== [Category][Number]. [Task Title]
[ ] *Priority:* [High/Medium/Low]

*Description:* [Detailed description of the task]

*Rationale:* [Explanation of why this task is important]
----

== Implementation Tasks

=== S1. Set up JMH Dependencies
[x] *Priority:* High

*Description:* Add JMH dependencies to the Maven pom.xml file:

```xml
<!-- JMH -->
<dependency>
    <groupId>org.openjdk.jmh</groupId>
    <artifactId>jmh-core</artifactId>
    <version>1.37</version>
    <scope>test</scope>
</dependency>
<dependency>
    <groupId>org.openjdk.jmh</groupId>
    <artifactId>jmh-generator-annprocess</artifactId>
    <version>1.37</version>
    <scope>test</scope>
</dependency>
```

*Rationale:* JMH is the industry standard for Java microbenchmarking and will provide more accurate performance measurements than custom test implementations.

=== S2. Configure Maven Plugin for JMH
[x] *Priority:* High

*Description:* Add the JMH Maven plugin to simplify running benchmarks:

```xml
<plugin>
    <groupId>org.apache.maven.plugins</groupId>
    <artifactId>maven-shade-plugin</artifactId>
    <executions>
        <execution>
            <phase>package</phase>
            <goals>
                <goal>shade</goal>
            </goals>
            <configuration>
                <finalName>benchmarks</finalName>
                <transformers>
                    <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                        <mainClass>org.openjdk.jmh.Main</mainClass>
                    </transformer>
                </transformers>
                <filters>
                    <filter>
                        <artifact>*:*</artifact>
                        <excludes>
                            <exclude>META-INF/*.SF</exclude>
                            <exclude>META-INF/*.DSA</exclude>
                            <exclude>META-INF/*.RSA</exclude>
                        </excludes>
                    </filter>
                </filters>
            </configuration>
        </execution>
    </executions>
</plugin>
```

*Rationale:* The Maven plugin will make it easier to build and run benchmarks from the command line and during CI/CD processes.

=== S3. Create Base Benchmark Structure
[x] *Priority:* High

*Description:* Create the base package structure and shared utility classes for benchmarks:
1. Create package `de.cuioss.jwt.validation.benchmark`
2. Create package `de.cuioss.jwt.validation.benchmark.util` for shared utilities
3. Create a `BenchmarkRunner` class to simplify benchmark execution

*Rationale:* A consistent structure will make it easier to add, maintain, and run benchmarks.

=== B1. Implement TokenValidator Benchmarks
[x] *Priority:* High

*Description:* Create a comprehensive benchmark for TokenValidator that covers:
1. Create `TokenValidatorBenchmark.java` in the new benchmark package
2. Implement benchmarks for validating access tokens
3. Implement benchmarks for validating ID tokens
4. Add parameterization for different token sizes and types
5. Test both "cold start" and "warmed up" scenarios
6. Use the in-memory `TestTokenGenerators` approach for consistent, reproducible results

Sample structure:
```java
@State(Scope.Benchmark)
@BenchmarkMode(Mode.AverageTime)
@OutputTimeUnit(TimeUnit.MICROSECONDS)
@Fork(value = 1, warmups = 1)
@Warmup(iterations = 5, time = 1)
@Measurement(iterations = 5, time = 1)
public class TokenValidatorBenchmark {
    
    private TokenValidator tokenValidator;
    private String accessToken;
    private String idToken;
    private String refreshToken;
    
    @Setup
    public void setup() {
        // Setup code for TokenValidator with an InMemoryJWKSFactory
        String jwksContent = InMemoryJWKSFactory.createDefaultJwks();
        IssuerConfig issuerConfig = IssuerConfig.builder()
                .issuer("Benchmark-testIssuer")
                .expectedAudience("benchmark-client")
                .expectedClientId("benchmark-client")
                .jwksContent(jwksContent)
                .build();
        tokenValidator = new TokenValidator(issuerConfig);
        
        // Generate tokens using TestTokenGenerators
        accessToken = TestTokenGenerators.accessTokens().next().getRawToken();
        idToken = TestTokenGenerators.idTokens().next().getRawToken();
        refreshToken = TestTokenGenerators.refreshTokens().next().getRawToken();
    }
    
    @Benchmark
    public ValidationResult validateAccessToken() {
        return tokenValidator.createAccessToken(accessToken);
    }
    
    @Benchmark
    public ValidationResult validateIdToken() {
        return tokenValidator.createIdToken(idToken);
    }
    
    @Benchmark
    public ValidationResult validateRefreshToken() {
        return tokenValidator.createRefreshToken(refreshToken);
    }
    
    // Additional benchmark methods with parameterized token sizes
}
```

*Rationale:* TokenValidator is a critical component, and its performance directly impacts application response times. Using in-memory test token generators provides consistent and reproducible benchmark results without external dependencies.

=== B2. Implement JwksClient Benchmarks
[x] *Priority:* High

*Description:* Create benchmarks for JwksClient:
1. Create `JwksClientBenchmark.java`
2. Benchmark key retrieval performance
3. Test cached vs. non-cached scenarios
4. Simulate different load patterns

Sample structure:
```java
@State(Scope.Benchmark)
@BenchmarkMode(Mode.AverageTime)
@OutputTimeUnit(TimeUnit.MICROSECONDS)
@Fork(value = 1, warmups = 1)
@Warmup(iterations = 5, time = 1)
@Measurement(iterations = 5, time = 1)
public class JwksClientBenchmark {
    
    private JwksClient jwksClient;
    private String keyId;
    
    @Setup
    public void setup() {
        // Setup code
    }
    
    @Benchmark
    public Optional<Key> retrieveKey() {
        return jwksClient.getKeyInfo(keyId).map(KeyInfo::getKey);
    }
    
    // Additional benchmark methods
}
```

*Rationale:* JWKS key retrieval is a common operation that should be optimized for performance, especially in high-volume scenarios.

=== B7. Implement JwksClient Failure Scenario Benchmarks
[x] *Priority:* Medium

*Description:* Create benchmarks specifically for JwksClient failure scenarios:
1. Create `JwksClientFailureBenchmark.java`
2. Benchmark performance for common JWKS failure scenarios:
   a. Non-existent key ID lookups
   b. Server errors (HTTP 500)
   c. Server unavailability (connection timeouts)
   d. Malformed JWKS responses
   e. Rate limiting scenarios
3. Measure performance impact of retry mechanisms
4. Test cache behavior during failure scenarios

Sample structure:
```java
@State(Scope.Benchmark)
@BenchmarkMode(Mode.AverageTime)
@OutputTimeUnit(TimeUnit.MICROSECONDS)
@Fork(value = 1, warmups = 1)
@Warmup(iterations = 5, time = 1)
@Measurement(iterations = 5, time = 1)
public class JwksClientFailureBenchmark {
    
    private JwksClient jwksClient;
    private String existingKeyId;
    private String nonExistentKeyId;
    
    @Setup
    public void setup() {
        // Setup code including mock server for different failure scenarios
    }
    
    @Benchmark
    public Optional<Key> retrieveExistingKey() {
        return jwksClient.getKeyInfo(existingKeyId).map(KeyInfo::getKey);
    }
    
    @Benchmark
    public Optional<Key> retrieveNonExistentKey() {
        return jwksClient.getKeyInfo(nonExistentKeyId).map(KeyInfo::getKey);
    }
    
    // Additional benchmark methods for other failure scenarios
}
```

*Rationale:* JWKS key retrieval failures can significantly impact system performance, especially in distributed systems. Understanding the performance characteristics during failure scenarios helps design more resilient systems.

=== B3. Implement Multi-Issuer Benchmarks
[ ] *Priority:* Medium

*Description:* Create benchmarks for multi-issuer token validation scenarios:
1. Create `MultiIssuerValidatorBenchmark.java`
2. Test performance with varying numbers of configured issuers
3. Measure impact of issuer resolution logic

*Rationale:* Multi-issuer support introduces additional complexity that could impact performance and should be measured.

=== B4. Implement Token Generation Benchmarks
[ ] *Priority:* Medium

*Description:* Create benchmarks for token generation processes used in tests:
1. Create `TokenGenerationBenchmark.java`
2. Measure performance of different token generation approaches
3. Identify bottlenecks in the token creation process

*Rationale:* Efficient token generation is important for testing and can impact the overall test execution time.

=== B6. Implement Failure Scenario Benchmarks
[x] *Priority:* High

*Description:* Create benchmarks that measure performance during failure scenarios:
1. Create `FailureScenarioBenchmark.java`
2. Benchmark common failure scenarios:
   a. Invalid signatures
   b. Expired tokens
   c. Wrong issuer/audience
   d. Malformed tokens
   e. Missing key IDs (kid)
   f. Key not found scenarios
3. Compare failure handling performance to successful validation

Sample structure:
```java
@State(Scope.Benchmark)
@BenchmarkMode(Mode.AverageTime)
@OutputTimeUnit(TimeUnit.MICROSECONDS)
@Fork(value = 1, warmups = 1)
@Warmup(iterations = 5, time = 1)
@Measurement(iterations = 5, time = 1)
public class FailureScenarioBenchmark {
    
    private TokenValidator tokenValidator;
    private String validToken;
    private String expiredToken;
    private String wrongIssuerToken;
    private String malformedToken;
    private String missingKidToken;
    
    @Setup
    public void setup() {
        // Setup code to create tokenValidator and various invalid tokens
    }
    
    @Benchmark
    public Object validateValidToken() {
        try {
            return tokenValidator.validateToken(validToken);
        } catch (TokenValidationException e) {
            return e;
        }
    }
    
    @Benchmark
    public Object validateExpiredToken() {
        try {
            return tokenValidator.validateToken(expiredToken);
        } catch (TokenValidationException e) {
            return e;
        }
    }
    
    // Additional benchmark methods for other failure scenarios
}
```

*Rationale:* In real-world applications, failure scenarios occur frequently and must be handled efficiently. Performance degradation during error handling can lead to denial of service vulnerabilities if not properly managed.

=== B5. Implement Concurrent Load Benchmarks
[x] *Priority:* High

*Description:* Create benchmarks that simulate concurrent access patterns:
1. Create `ConcurrentTokenValidationBenchmark.java`
2. Use JMH thread states to simulate concurrent validation
3. Test different thread counts and access patterns

Sample structure:
```java
@State(Scope.Benchmark)
@BenchmarkMode(Mode.Throughput)
@OutputTimeUnit(TimeUnit.SECONDS)
@Threads(Threads.MAX)
@Fork(value = 1, warmups = 1)
@Warmup(iterations = 3, time = 1)
@Measurement(iterations = 5, time = 1)
public class ConcurrentTokenValidationBenchmark {
    
    // Implementation
}
```

*Rationale:* Real-world applications will have concurrent token validation requests, and it's important to measure performance under these conditions.

=== B8. Implement Error-Load Benchmarks
[ ] *Priority:* Medium

*Description:* Create benchmarks that measure system performance under high rates of validation errors:
1. Create `ErrorLoadBenchmark.java`
2. Simulate scenarios with varying percentages of invalid tokens (10%, 50%, 90%, 100%)
3. Measure impact on overall system throughput
4. Evaluate memory usage patterns during high error rates
5. Test how error handling affects parallel performance

Sample structure:
```java
@State(Scope.Benchmark)
@BenchmarkMode({Mode.Throughput, Mode.AverageTime})
@OutputTimeUnit(TimeUnit.MILLISECONDS)
@Fork(value = 1, warmups = 1)
@Warmup(iterations = 3, time = 1)
@Measurement(iterations = 5, time = 1)
public class ErrorLoadBenchmark {
    
    private TokenValidator tokenValidator;
    private List<String> validTokens;
    private List<String> invalidTokens;
    
    @Param({"0", "10", "50", "90", "100"})
    private int errorPercentage;
    
    @Setup
    public void setup() {
        // Setup code to initialize validator and token lists
        // Prepare mix of valid and invalid tokens based on errorPercentage
    }
    
    @Benchmark
    public Object validateMixedTokens(Blackhole blackhole) {
        // Select token based on current iteration and errorPercentage
        String token = selectToken();
        try {
            ValidationResult result = tokenValidator.validateToken(token);
            blackhole.consume(result);
            return result;
        } catch (TokenValidationException e) {
            blackhole.consume(e);
            return e;
        }
    }
}
```

*Rationale:* In security-critical applications, high rates of invalid tokens might be part of an attack scenario. Understanding performance characteristics during these scenarios is crucial for maintaining system availability and security.

=== A1. Create Performance Analysis Report Template
[ ] *Priority:* Medium

*Description:* Create a template for analyzing and reporting benchmark results:
1. Define key metrics to track
2. Create visualization templates
3. Establish baseline performance expectations

*Rationale:* Consistent reporting will make it easier to track performance over time and identify regressions.

=== A2. Document Benchmark Methodology
[ ] *Priority:* Medium

*Description:* Document the benchmark methodology, including:
1. How benchmarks are designed
2. What metrics are collected
3. How to interpret results
4. How to run benchmarks

*Rationale:* Clear documentation ensures that benchmarks are used correctly and results are interpreted properly.

=== A3. Analyze Real-World Token Characteristics
[ ] *Priority:* Medium

*Description:* Analyze real-world tokens from Keycloak and other OIDC providers to inform benchmark test cases:

1. Collect sample tokens from different OIDC providers (Keycloak, Auth0, Azure AD, etc.)
2. Analyze token sizes, claim structures, and signing algorithms
3. Create realistic benchmark profiles based on the analysis
4. Document findings in a report

*Rationale:* While in-memory token generators provide consistency for benchmarks, they should still represent real-world token characteristics. This analysis will ensure the benchmarks test scenarios that match production usage.

=== I1. Integrate Benchmarks with CI/CD Pipeline
[ ] *Priority:* Low

*Description:* Set up integration with CI/CD to run benchmarks on a regular schedule:
1. Create a separate CI job for benchmarks
2. Configure result storage and comparison
3. Set up alerting for performance regressions

*Rationale:* Automated benchmark execution helps catch performance regressions early.

=== I2. Replace Existing Performance Tests
[ ] *Priority:* High

*Description:* Remove the old performance tests once the new benchmarks are implemented:
1. Remove `TokenValidatorPerformanceTest.java`
2. Remove `JwksClientBenchmarkTest.java`
3. Update any references to these classes. Do not forget documentation and README files.
4. Make a full build after removing the old benchmarks to ensure everything works correctly.

*Rationale:* Maintaining both old and new benchmarks would create confusion and maintenance overhead.

=== I3. Create Comparison Test Between In-Memory and Real-World Tokens
[ ] *Priority:* Low

*Description:* Create a test that compares validation performance between in-memory generated tokens and real tokens from Keycloak:

1. Create `TokenValidatorComparisonTest.java` 
2. Configure test to run both with in-memory tokens and Keycloak tokens
3. Compare validation performance and detect any significant differences
4. Document findings and potential improvements to the in-memory generators

*Rationale:* This comparison validates that the in-memory generators produce tokens that have similar validation characteristics to real-world tokens, ensuring benchmark relevance.

== JMH Best Practices

=== State Management

* Use appropriate `@State` scopes:
  * `Scope.Benchmark` - shared across all threads
  * `Scope.Thread` - one instance per thread
  * `Scope.Group` - shared across thread groups

=== Measurement Configuration

* Configure appropriate warm-up:
  * `@Warmup(iterations = 5, time = 1)`
* Set measurement parameters:
  * `@Measurement(iterations = 5, time = 1)`
* Choose appropriate benchmark modes:
  * `Mode.Throughput` - operations per unit time
  * `Mode.AverageTime` - average time per operation
  * `Mode.SampleTime` - samples the time for each operation
  * `Mode.SingleShotTime` - time for a single operation (cold measurement)
  * `Mode.All` - all of the above

=== Avoiding Common Pitfalls

* Dead code elimination - return benchmark operation results
* Constant folding - use parameters that can't be optimized away
* Loop optimizations - be aware of JIT optimizations

== Running Benchmarks

=== Command Line Execution

```bash
# Run all benchmarks
mvn clean package
java -jar target/benchmarks.jar

# Run specific benchmark
java -jar target/benchmarks.jar TokenValidatorBenchmark

# Run with specific parameters
java -jar target/benchmarks.jar TokenValidatorBenchmark -f 1 -wi 5 -i 5 -r 1s
```

=== Maven Execution

```bash
mvn clean verify -Pbenchmark
```

== Performance Goals

=== Token Validation Performance

* Access token validation: < 1ms average time
* ID token validation: < 1.5ms average time
* 99th percentile: < 5ms

=== JwksClient Performance

* Key retrieval (cached): < 0.1ms
* Key retrieval (uncached): < 50ms
* JWKS refresh: < 200ms

=== Concurrent Performance

* Linear scaling up to 16 threads
* No more than 50% degradation at 100 threads

=== Failure Handling Performance

* Invalid token validation: < 2ms average time
* Non-existent key lookup: < 0.5ms (cached mode)
* Server error recovery: < 100ms
* Exception generation overhead: < 0.5ms per exception
* Maximum throughput degradation during 50% error rate: < 40%

== GitHub Results Visualization

=== I3. Set up Benchmark Results Visualization on GitHub
[ ] *Priority:* Medium

*Description:* Implement a GitHub-based benchmark results visualization system:
1. Configure JMH to output results in JSON format
2. Create a GitHub workflow to run benchmarks and store results
3. Set up a visualization system using one of the following approaches:
   a. Use JMH Visualizer with GitHub Pages
   b. Generate badges with benchmark metrics
   c. Create GitHub Actions workflow that generates and commits charts

Sample GitHub workflow for running benchmarks and storing results:
```yaml
name: JMH Benchmarks

on:
  workflow_dispatch:
  schedule:
    - cron: '0 0 * * 0'  # Run weekly on Sundays at midnight

jobs:
  run-benchmarks:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up JDK 17
        uses: actions/setup-java@v3
        with:
          java-version: '17'
          distribution: 'temurin'
          cache: maven
      
      - name: Run benchmarks
        run: |
          mvn clean verify -Pbenchmark
          mkdir -p benchmark-results
          cp target/jmh-result.json benchmark-results/jmh-result-$(date +%Y%m%d).json
      
      - name: Archive benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: benchmark-results
          path: benchmark-results/
      
      - name: Generate visualization
        run: |
          # Generate charts from benchmark results
          # This could use a script that processes the JMH JSON results
          # and creates charts using tools like matplotlib, Chart.js, etc.
          
      - name: Deploy to GitHub Pages
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./benchmark-results
```

*Rationale:* Visualizing benchmark results on GitHub makes performance trends easily accessible to all contributors, helping to identify performance regressions and improvements over time.

=== A3. Set up Benchmark Badges
[ ] *Priority:* Low

*Description:* Create dynamic badges that show key benchmark metrics:
1. Extract critical metrics from benchmark results (e.g., throughput, average time)
2. Generate SVG badges using shields.io or similar service
3. Add badges to the project README.md

Example implementation:
```bash
# Extract benchmark metrics
THROUGHPUT=$(jq '.[] | select(.benchmark | contains("KeyRetrieval")) | .primaryMetric.score' jmh-result.json)

# Generate badge URL
BADGE_URL="https://img.shields.io/badge/Key%20Retrieval-${THROUGHPUT}%20ops%2Fs-blue"

# Update README with badge
sed -i "s|<!-- BENCHMARK_BADGE -->|[![Benchmark](${BADGE_URL})](path/to/benchmark/results)|g" README.md
```

*Rationale:* Badges provide an immediate visual indicator of performance metrics directly in the project documentation.

=== A4. Create Benchmark Specification Document
[x] *Priority:* High

*Description:* Create a proper specification document for benchmarks at `doc/specification/benchmark.adoc` that follows the project's documentation standards:
1. Define performance requirements and constraints
2. Link to implementation classes instead of duplicating implementation details
3. Explain the benchmark methodology without duplicating code
4. Cross-reference other specification documents (security.adoc, technical-components.adoc)
5. Include proper references to verification tests

COMPLETED: The specification document has been created at `doc/specification/benchmark.adoc` and follows the project documentation standards.

*Rationale:* A proper specification document following the project standards ensures that performance requirements are clearly documented without duplicating implementation details, and provides appropriate cross-references to other documentation and code.

=== B3. Implement Enhanced Token Generators for Benchmarks
[ ] *Priority:* Medium

*Description:* Create enhanced token generators for benchmarks that extend the existing `TestTokenGenerators` approach:

1. Create `BenchmarkTokenGenerators.java` in the benchmark utilities package
2. Implement configurable token size generation (small, medium, large tokens)
3. Support different signing algorithms (RS256, RS384, RS512, ES256, ES384, ES512)
4. Allow control of claim complexity (simple vs. complex token structures)
5. Create realistic token generation profiles based on observed real-world tokens

*Rationale:* Benchmarks need to test with a variety of token types and sizes to provide comprehensive performance data. The enhanced generators will allow testing different real-world scenarios while maintaining reproducibility.

Sample implementation:
```java
public class BenchmarkTokenGenerators {
    
    /**
     * Generate access tokens with configurable size and complexity.
     * 
     * @param size the size profile (SMALL, MEDIUM, LARGE)
     * @param complexity the complexity profile (SIMPLE, COMPLEX)
     * @param algorithm the signing algorithm to use
     * @return a TypedGenerator for TestTokenHolder objects
     */
    public static TypedGenerator<TestTokenHolder> accessTokens(
            TokenSize size, 
            TokenComplexity complexity,
            SigningAlgorithm algorithm) {
        
        ClaimControlParameter params = ClaimControlParameter.builder()
                .tokenType(TokenType.ACCESS_TOKEN)
                .sizeProfile(size)
                .complexityProfile(complexity)
                .signingAlgorithm(algorithm)
                .build();
                
        return () -> new TestTokenHolder(TokenType.ACCESS_TOKEN, params);
    }
    
    // Similar methods for ID and refresh tokens
    
    public enum TokenSize {
        SMALL,   // ~1KB
        MEDIUM,  // ~5KB
        LARGE    // ~20KB
    }
    
    public enum TokenComplexity {
        SIMPLE,  // Basic claims
        COMPLEX  // Many nested claims, arrays, etc.
    }
    
    public enum SigningAlgorithm {
        RS256, RS384, RS512,
        ES256, ES384, ES512
    }
}
```
