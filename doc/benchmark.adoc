= JMH Microbenchmark Implementation Plan
:toc:
:toclevels: 3
:toc-title: Table of Contents
:sectnums:

This document outlines the plan for implementing microbenchmarks using https://github.com/openjdk/jmh[JMH (Java Microbenchmark Harness)] for the cui-jwt-validation project.

== Overview

The goal is to replace the existing performance tests with proper microbenchmarks that provide more accurate, consistent, and comprehensive performance metrics. 

Specifically, we will replace:

* `src/test/java/de/cuioss/jwt/validation/TokenValidatorPerformanceTest.java`
* `src/test/java/de/cuioss/jwt/validation/jwks/JwksClientBenchmarkTest.java`

All benchmarks will be moved to a new package: `de/cuioss/jwt/validation/benchmark`

== Task Structure

=== Categories and Numbering

Tasks are organized into categories:

* *S* - Setup tasks (e.g., S1, S2, S3)
* *B* - Benchmark Implementation tasks (e.g., B1, B2, B3)
* *A* - Analysis and Documentation tasks (e.g., A1, A2, A3)
* *I* - Integration tasks (e.g., I1, I2, I3)

=== Task Format

Each task follows a consistent format:

[source]
----
=== [Category][Number]. [Task Title]
[ ] *Priority:* [High/Medium/Low]

*Description:* [Detailed description of the task]

*Rationale:* [Explanation of why this task is important]
----

== Implementation Tasks

=== S1. Set up JMH Dependencies
[x] *Priority:* High

*Description:* Add JMH dependencies to the Maven pom.xml file.

*Rationale:* JMH is the industry standard for Java microbenchmarking and will provide more accurate performance measurements than custom test implementations.

=== S2. Configure Maven Plugin for JMH
[x] *Priority:* High

*Description:* Add the Maven Shade plugin configured for JMH to simplify running benchmarks.

*Rationale:* The Maven plugin will make it easier to build and run benchmarks from the command line and during CI/CD processes.

=== S3. Create Base Benchmark Structure
[x] *Priority:* High

*Description:* Create the base package structure and shared utility classes for benchmarks.

*Rationale:* A consistent structure will make it easier to add, maintain, and run benchmarks.

=== B1. Implement TokenValidator Benchmarks
[x] *Priority:* High

*Description:* Create a comprehensive benchmark for TokenValidator that measures performance of validating different token types with various configurations.

*Rationale:* TokenValidator is a critical component, and its performance directly impacts application response times. Using in-memory test token generators provides consistent and reproducible benchmark results without external dependencies.

=== B2. Implement JwksClient Benchmarks
[x] *Priority:* High

*Description:* Create benchmarks for JwksClient that measure key retrieval performance in various scenarios.

*Rationale:* JWKS key retrieval is a common operation that should be optimized for performance, especially in high-volume scenarios.

=== B7. Implement JwksClient Failure Scenario Benchmarks
[x] *Priority:* Medium

*Description:* Create benchmarks specifically for JwksClient failure scenarios to measure performance during error conditions.

*Rationale:* JWKS key retrieval failures can significantly impact system performance, especially in distributed systems. Understanding the performance characteristics during failure scenarios helps design more resilient systems.

=== B3. Implement Multi-Issuer Benchmarks
[x] *Priority:* Medium

*Description:* Create benchmarks for multi-issuer token validation scenarios to measure performance with varying numbers of configured issuers.

*Rationale:* Multi-issuer support introduces additional complexity that could impact performance and should be measured.

=== B4. Implement Token Generation Benchmarks
[x] *Priority:* Medium

*Description:* Create benchmarks for token generation processes used in tests to measure performance of different approaches.

*Rationale:* Efficient token generation is important for testing and can impact the overall test execution time.

=== B6. Implement Failure Scenario Benchmarks
[x] *Priority:* High

*Description:* Create benchmarks that measure performance during common token validation failure scenarios.

*Rationale:* In real-world applications, failure scenarios occur frequently and must be handled efficiently. Performance degradation during error handling can lead to denial of service vulnerabilities if not properly managed.

=== B5. Implement Concurrent Load Benchmarks
[x] *Priority:* High

*Description:* Create benchmarks that simulate concurrent token validation access patterns with different thread counts.

*Rationale:* Real-world applications will have concurrent token validation requests, and it's important to measure performance under these conditions.

=== B8. Implement Error-Load Benchmarks
[x] *Priority:* Medium

*Description:* Create benchmarks that measure system performance under high rates of validation errors with varying percentages of invalid tokens.

*Rationale:* In security-critical applications, high rates of invalid tokens might be part of an attack scenario. Understanding performance characteristics during these scenarios is crucial for maintaining system availability and security.

=== A1. Create Performance Analysis Report Template
[ ] *Priority:* Medium

*Description:* Create a template for analyzing and reporting benchmark results:
1. Define key metrics to track
2. Create visualization templates
3. Establish baseline performance expectations

*Rationale:* Consistent reporting will make it easier to track performance over time and identify regressions.

=== A2. Document Benchmark Methodology
[x] *Priority:* Medium

*Description:* Document the benchmark methodology, including:
1. How benchmarks are designed
2. What metrics are collected
3. How to interpret results
4. How to run benchmarks

*Rationale:* Clear documentation ensures that benchmarks are used correctly and results are interpreted properly.

=== A3. Analyze Real-World Token Characteristics
[x] *Priority:* Medium

*Description:* Analyze real-world tokens from Keycloak and other OIDC providers to inform benchmark test cases:

1. Collect sample tokens from different OIDC providers (Keycloak, Auth0, Azure AD, etc.)
2. Analyze token sizes, claim structures, and signing algorithms
3. Create realistic benchmark profiles based on the analysis
4. Document findings in a report

*Rationale:* While in-memory token generators provide consistency for benchmarks, they should still represent real-world token characteristics. This analysis will ensure the benchmarks test scenarios that match production usage.

=== I1. Integrate Benchmarks with CI/CD Pipeline
[ ] *Priority:* Low

*Description:* Set up integration with CI/CD to run benchmarks on a regular schedule:
1. Create a separate CI job for benchmarks
2. Configure result storage and comparison
3. Set up alerting for performance regressions

*Rationale:* Automated benchmark execution helps catch performance regressions early.

=== I2. Replace Existing Performance Tests
[x] *Priority:* High

*Description:* Remove the old performance tests once the new benchmarks are implemented:
1. Remove `TokenValidatorPerformanceTest.java`
2. Remove `JwksClientBenchmarkTest.java`
3. Update any references to these classes. Do not forget documentation and README files.
4. Make a full build after removing the old benchmarks to ensure everything works correctly.

*Rationale:* Maintaining both old and new benchmarks would create confusion and maintenance overhead.

=== I3. Create Comparison Test Between In-Memory and Real-World Tokens
[ ] *Priority:* Low

*Description:* Create a test that compares validation performance between in-memory generated tokens and real tokens from Keycloak:

1. Create `TokenValidatorComparisonTest.java` 
2. Configure test to run both with in-memory tokens and Keycloak tokens
3. Compare validation performance and detect any significant differences
4. Document findings and potential improvements to the in-memory generators

*Rationale:* This comparison validates that the in-memory generators produce tokens that have similar validation characteristics to real-world tokens, ensuring benchmark relevance.

== JMH Best Practices

=== State Management

* Use appropriate `@State` scopes:
  * `Scope.Benchmark` - shared across all threads
  * `Scope.Thread` - one instance per thread
  * `Scope.Group` - shared across thread groups

=== Measurement Configuration

* Configure appropriate warm-up:
  * `@Warmup(iterations = 5, time = 1)`
* Set measurement parameters:
  * `@Measurement(iterations = 5, time = 1)`
* Choose appropriate benchmark modes:
  * `Mode.Throughput` - operations per unit time
  * `Mode.AverageTime` - average time per operation
  * `Mode.SampleTime` - samples the time for each operation
  * `Mode.SingleShotTime` - time for a single operation (cold measurement)
  * `Mode.All` - all of the above

=== Avoiding Common Pitfalls

* Dead code elimination - return benchmark operation results
* Constant folding - use parameters that can't be optimized away
* Loop optimizations - be aware of JIT optimizations

== Running Benchmarks

=== Command Line Execution

Run all benchmarks:
`mvn clean package && java -jar target/benchmarks.jar`

Run specific benchmark:
`java -jar target/benchmarks.jar TokenValidatorBenchmark`

=== Maven Execution

`mvn clean verify -Pbenchmark`

== Performance Goals

=== Token Validation Performance

* Access token validation: < 1ms average time
* ID token validation: < 1.5ms average time
* 99th percentile: < 5ms

=== JwksClient Performance

* Key retrieval (cached): < 0.1ms
* Key retrieval (uncached): < 50ms
* JWKS refresh: < 200ms

=== Concurrent Performance

* Linear scaling up to 16 threads
* No more than 50% degradation at 100 threads

=== Failure Handling Performance

* Invalid token validation: < 2ms average time
* Non-existent key lookup: < 0.5ms (cached mode)
* Server error recovery: < 100ms
* Exception generation overhead: < 0.5ms per exception
* Maximum throughput degradation during 50% error rate: < 40%

== GitHub Results Visualization

=== I3. Set up Benchmark Results Visualization on GitHub
[x] *Priority:* Medium

*Description:* Implement a GitHub-based benchmark results visualization system that stores and displays benchmark results.

*Rationale:* Visualizing benchmark results on GitHub makes performance trends easily accessible to all contributors, helping to identify performance regressions and improvements over time.

*Implementation:* The benchmark visualization system has been implemented using GitHub Actions and GitHub Pages. For details, see link:benchmark-visualization.adoc[Benchmark Visualization Documentation].

=== A3. Set up Benchmark Badges
[ ] *Priority:* Low

*Description:* Create dynamic badges that show key benchmark metrics in the project documentation.

*Rationale:* Badges provide an immediate visual indicator of performance metrics directly in the project documentation.

=== A4. Create Benchmark Specification Document
[x] *Priority:* High

*Description:* Create a proper specification document for benchmarks at `doc/specification/benchmark.adoc` that follows the project's documentation standards.

*Rationale:* A proper specification document following the project standards ensures that performance requirements are clearly documented without duplicating implementation details, and provides appropriate cross-references to other documentation and code.

=== B3. Implement Enhanced Token Generators for Benchmarks
[x] *Priority:* Medium

*Description:* Create enhanced token generators for benchmarks that extend the existing `TestTokenGenerators` approach with configurable token sizes, signing algorithms, and claim complexity.

*Rationale:* Benchmarks need to test with a variety of token types and sizes to provide comprehensive performance data. The enhanced generators will allow testing different real-world scenarios while maintaining reproducibility.
