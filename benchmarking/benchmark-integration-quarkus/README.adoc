= Quarkus Integration JMH Benchmarks
:toc: left
:toclevels: 3
:source-highlighter: highlight.js

Real-world JWT validation performance testing against live Quarkus applications in containers.

== Quick Start

[source,bash]
----
# Quick benchmark (1 fork, 2 iterations, 5s each, skip container lifecycle)
./mvnw clean verify -Pbenchmark,quick -pl benchmarking/benchmark-integration-quarkus

# Full benchmarks with automatic container management (from project root)
./mvnw clean verify -Pbenchmark -pl benchmarking/benchmark-integration-quarkus

# Full benchmarks - skip container start/stop (containers must be running)
./mvnw clean verify -Pbenchmark -Dskip.container.lifecycle=true -pl benchmarking/benchmark-integration-quarkus

# Custom validation run
./mvnw clean verify -Pbenchmark -Djmh.iterations=1 -Djmh.threads=1 -pl benchmarking/benchmark-integration-quarkus

# With JFR profiling (local development only - not available in CI)
./mvnw clean verify -Pbenchmark-jfr -pl benchmarking/benchmark-integration-quarkus

# Force rebuild of Quarkus native container
./mvnw clean verify -Prebuild-container,benchmark -pl benchmarking/benchmark-integration-quarkus

# Start containers manually for fast mode
cd cui-jwt-quarkus-parent/cui-jwt-quarkus-integration-tests
./scripts/start-integration-container.sh
----

== Architecture

=== Benchmarks

* `JwtValidationBenchmark` - JWT validation endpoint performance
* `JwtHealthBenchmark` - Health check baseline measurements

=== Token Management

Pre-initialized `TokenRepository` with 100-token pool for realistic cache testing (~10% hit ratio).

=== Service URLs

* Quarkus: https://localhost:10443
* Keycloak: https://localhost:1443

== Configuration

=== Maven Profiles

[cols="2,3,3", options="header"]
|===
|Profile
|Description
|Properties Set

|`benchmark`
|Enables benchmarking
|`skip.benchmark=false`

|`quick`
|Quick testing mode
|`jmh.iterations=2`, `jmh.forks=1`, `jmh.time=5s`, `jmh.warmupTime=2s`, `skip.container.lifecycle=true`

|`benchmark-jfr`
|Benchmarks with JFR profiling
|Enables JFR recording

|`rebuild-container`
|Forces container rebuild
|Cleans integration tests
|===

Example usage:
[source,bash]
----
# Quick benchmarks (total ~30s, assumes containers are running)
./mvnw clean verify -Pbenchmark,quick -pl benchmarking/benchmark-integration-quarkus

# Custom duration with container skip
./mvnw clean verify -Pbenchmark -Djmh.time=30s -Djmh.iterations=2 -Dskip.container.lifecycle=true -pl benchmarking/benchmark-integration-quarkus
----

=== Container Lifecycle Management

The `skip.container.lifecycle` property optimizes benchmark iteration:

[options="header"]
|===
|Value|Behavior|Use Case

|`false` (default)
|Full lifecycle: build, start, stop containers
|CI/CD, first run, clean environment

|`true`
|Skip container operations, only run benchmarks
|Fast iteration, containers already running
|===

=== JMH Settings

[source,bash]
----
# Default settings
-Djmh.iterations=4         # Measurement iterations (default: 4, quick: 2)
-Djmh.warmupIterations=1   # Warmup iterations (default: 1)
-Djmh.threads=24          # Concurrent threads (default: 24)
-Djmh.time=12s            # Time per iteration (default: 12s, quick: 5s)
-Djmh.warmupTime=3s       # Warmup time (default: 3s, quick: 2s)
-Djmh.forks=2             # JVM forks (default: 2, quick: 1)
----

== Output

Results in `target/benchmark-results/`:

* `integration-result.json` - Raw JMH results
* `benchmark-summary.json` - Overall summary with quality gates
* `badges/` - Performance badges (score, trend, last-run)
* `data/` - Individual benchmark metrics JSON files
* `gh-pages-ready/` - GitHub Pages deployment structure
* `benchmark-run_*.log` - Complete console output

== Analysis

=== View Results

[source,bash]
----
# Serve results locally with web UI
cd benchmarking/scripts
./serve-reports.sh quarkus  # Opens http://localhost:8080

# Or analyze JSON directly
# View throughput
jq '.[] | select(.mode == "thrpt") | {benchmark: .benchmark, score: .primaryMetric.score}' \
  target/benchmark-results/integration-result.json

# View latency percentiles
jq '.[] | select(.mode == "sample") | .primaryMetric.scorePercentiles' \
  target/benchmark-results/integration-result.json

# JFR analysis (if enabled)
jfr summary target/benchmark-results/jfr-recordings/benchmark.jfr
----

== Expected Performance

* Health Check: 10,000-15,000 ops/sec, <1ms median
* JWT Validation: 5,000-10,000 ops/sec, 1-3ms median

== Development

=== Adding Benchmarks

1. Extend `AbstractIntegrationBenchmark`
2. Use `@Benchmark` with appropriate `@BenchmarkMode`
3. Override `getBenchmarkName()` for metrics

=== Troubleshooting

[source,bash]
----
# Check containers
docker ps

# Verify services
curl -k https://localhost:10443/q/health
curl -k https://localhost:1443/realms/benchmark

# Review logs
tail -f target/benchmark-results/benchmark-run_*.log
----

== Documentation

For comprehensive benchmarking documentation:

* link:../doc/README.adoc[Main Documentation Hub]
* link:../doc/Architecture.adoc[Module Architecture] - Detailed architecture and code placement guidelines
* link:../doc/workflow.adoc[Benchmark Workflow] - Complete execution workflow
* link:../doc/local-testing.adoc[Local Testing] - How to view results locally
* link:../benchmark-library/README.adoc[Library Benchmarks] - Micro benchmark documentation