= Understanding Benchmark Results
:toc: left
:toclevels: 3
:source-highlighter: highlight.js

== Result Files Structure

=== JMH Results (`integration-benchmark-result.json`)

Contains raw JMH benchmark data in JSON format:

[source,json]
----
[{
  "benchmark": "de.cuioss.jwt.quarkus.benchmark.benchmarks.JwtValidationBenchmark.validateJwtThroughput",
  "mode": "thrpt",
  "threads": 24,
  "forks": 2,
  "primaryMetric": {
    "score": 8543.21,
    "scoreUnit": "ops/s",
    "scoreConfidence": [8421.15, 8665.27]
  }
}]
----

Key fields:

* `mode`: Benchmark mode (`thrpt` for throughput, `sample` for latency sampling)
* `primaryMetric.score`: Main measurement value
* `scoreUnit`: Units (ops/s, ms/op, etc.)
* `scorePercentiles`: Latency distribution (only for `sample` mode)

=== HTTP Metrics (`http-metrics.json`)

Processed metrics from Quarkus Micrometer integration:

[source,json]
----
{
  "JwtValidationBenchmark": {
    "throughput": 8543.21,
    "avgResponseTime": 2.8,
    "unit": "ms"
  }
}
----

Generated by correlating JMH results with Prometheus metrics.

=== Integration Metrics (`integration-metrics.json`)

Comprehensive metrics including security events:

[source,json]
----
{
  "timestamp": "2025-01-01T10:00:00Z",
  "benchmarks": [{
    "name": "JwtValidationBenchmark",
    "metrics": {
      "http_requests_total": 125000,
      "jwt_validation_success": 124950,
      "jwt_validation_errors": 50,
      "memory_usage_bytes": 268435456
    }
  }]
}
----

== Interpreting Results

=== Throughput Benchmarks

Higher is better:

* *Excellent*: >10,000 ops/s
* *Good*: 5,000-10,000 ops/s  
* *Acceptable*: 1,000-5,000 ops/s
* *Poor*: <1,000 ops/s

=== Latency Percentiles

Lower is better:

* *p50* (median): Typical request latency
* *p99*: Latency for 99% of requests
* *p99.9*: Worst-case latency (excluding outliers)

Example analysis:
[source,bash]
----
# Extract p99 latency
jq '.[] | select(.mode == "sample") | 
  {benchmark: .benchmark, p99: .primaryMetric.scorePercentiles."99.0"}' \
  integration-benchmark-result.json
----

=== Cache Hit Rates

Calculate from security metrics:
[source,bash]
----
# Cache effectiveness = (total - cache_misses) / total
jq '.benchmarks[] | 
  {name: .name, 
   hit_rate: (1 - (.metrics.cache_misses / .metrics.jwt_validation_success))}' \
  integration-metrics.json
----

== Performance Baselines

=== Hardware Dependencies

Results vary by:

* CPU cores and speed
* Network latency (container overhead)
* JVM heap size and GC settings
* Container resource limits

=== Comparative Analysis

Use `JwtHealthBenchmark` as baseline:

* Health check represents minimal overhead
* JWT validation adds cryptographic verification cost
* Compare ratios, not absolute numbers across environments

== JFR Analysis

When using `benchmark-jfr` profile:

=== CPU Profiling
[source,bash]
----
# Top CPU consumers
jfr print --events jdk.ExecutionSample \
  --stack-depth 10 \
  target/benchmark-results/jfr-recordings/benchmark.jfr | \
  head -50
----

=== Memory Analysis
[source,bash]
----
# Allocation hotspots
jfr print --events jdk.ObjectAllocationInNewTLAB \
  target/benchmark-results/jfr-recordings/benchmark.jfr | \
  grep "de.cuioss"
----

=== Lock Contention
[source,bash]
----
# Thread blocking analysis
jfr print --events jdk.JavaMonitorWait \
  target/benchmark-results/jfr-recordings/benchmark.jfr
----

== Common Patterns

=== Degradation Indicators

1. **Increasing p99.9 latency**: GC pressure or resource exhaustion
2. **Throughput plateau**: Connection pool limits reached
3. **High error rates**: Token expiration or network issues

=== Optimization Opportunities

1. **Cache misses >20%**: Increase cache size or TTL
2. **p99 >10x p50**: Thread contention or GC issues
3. **Low CPU usage**: I/O bound, consider connection pooling

== Visualization

Generate charts from results:

[source,bash]
----
# Create CSV for plotting
jq -r '.[] | [.benchmark, .primaryMetric.score] | @csv' \
  integration-benchmark-result.json > benchmark-scores.csv

# Plot latency distribution
jq -r '.[] | select(.mode == "sample") | 
  .primaryMetric.scorePercentiles | 
  to_entries | 
  map([.key, .value]) | 
  .[] | @csv' integration-benchmark-result.json > latency-percentiles.csv
----