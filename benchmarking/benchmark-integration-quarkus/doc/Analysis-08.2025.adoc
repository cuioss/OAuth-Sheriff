= JWT Integration Performance Analysis - August 2025
:source-highlighter: highlight.js
:toc: left
:toclevels: 3
:toc-title: Table of Contents
:sectnums:

== Executive Summary

Performance analysis and optimization tracking for JWT validation in Quarkus native integration benchmarks.

**Current Focus**: End-to-end performance in containerized Quarkus native runtime environment

**Key Insights**:

* Integration benchmarks complement microbenchmarks by testing real-world scenarios
* Native compilation provides excellent memory efficiency (~6MB heap usage)  
* Network and HTTP processing dominate total latency (~80% of request time)
* JWT validation overhead is only ~200μs of total request time
* Performance is highly dependent on system resources and containerization

== Integration vs Micro Benchmark Comparison

**Different Focus Areas**:

|===
|Aspect |Micro Benchmarks |Integration Benchmarks

|**Scope**
|Core JWT validation logic only
|End-to-end HTTP request/response cycle

|**Environment** 
|JVM with isolated validation
|Quarkus native in Docker container

|**Latency Focus**
|JWT parsing/validation (50-200μs)
|Total request latency (1-5ms)

|**Throughput Focus**
|Token operations per second
|HTTP requests per second

|**Bottlenecks**
|Cryptographic operations, claims validation
|Network I/O, HTTP processing, container overhead

|**Optimization Targets**
|Algorithm efficiency, JCA performance
|Baseline performance measurement
|===

== Performance Baseline History

=== August 2025 Performance Evolution

The integration benchmarks have shown various performance patterns:

**Health Endpoint Performance**:

* Consistently achieves 11,000-12,000+ ops/s
* Provides baseline for HTTP/container overhead
* P95 latencies typically 1.5-2.0ms
* Memory usage remains very low (~6MB)

**JWT Validation Performance**:

* Target range: 8,000-11,000 ops/s  
* More variable due to cryptographic complexity
* P95 latencies typically 2-4ms
* Includes full JWT validation overhead

**Resource Utilization Patterns**:

* CPU usage varies from 50-75% under load
* Memory remains consistently low due to native image
* Container CPU allocation impacts throughput significantly
* Thread pool configuration affects concurrency

== Performance Analysis Framework

=== Key Metrics Tracked

**Throughput Metrics**:

* Health endpoint: ops/s (baseline system capacity)
* JWT validation endpoint: ops/s (with crypto overhead)
* Throughput gap analysis (efficiency measure)

**Latency Distribution**:

* P50, P95, P99 percentiles for both endpoints
* JWT validation component breakdown
* HTTP processing vs JWT validation time

**Resource Efficiency**:

* CPU utilization and load patterns
* Memory usage (heap + non-heap)
* Thread pool saturation metrics
* Container resource allocation

**Test Characteristics**:

* Only positive test cases (valid tokens)
* No error rate tracking (not testing invalid tokens)
* Focus on successful validation performance

=== Measurement Methodology

**Test Environment Standardization**:

* Apple M4 processor (4 CPU cores)
* OrbStack containerization platform
* Quarkus native runtime compilation
* Java HttpClient for load generation

**Load Generation**:

* Configurable thread pools (typically 10-30 threads)
* Valid token testing only
* Sustained load duration for reliable measurements
* JFR profiling integration for deep analysis

**Data Collection**:

* JMH integration for statistical accuracy
* Prometheus metrics extraction during runs
* JFR recordings for CPU/memory profiling
* Container resource monitoring

== Optimization Strategies

=== Current Configuration

**Test Setup**:

* Native Quarkus compilation with standard flags
* Container resource allocation based on available system resources
* Default HTTP client configuration (no custom connection pooling)
* Cache deliberately disabled for consistent benchmark results


=== JWT Validation Characteristics

**Current Implementation**:

* Cache disabled for benchmark consistency
* Sequential validation pipeline
* Standard Quarkus JWT validation
* No custom optimizations applied

== Current Performance Characteristics

=== Typical Performance Ranges

**Good Performance Conditions** (M4, 4 CPU, optimal config):

* Health endpoint: 11,000-12,500 ops/s
* JWT validation: 9,000-11,000 ops/s  
* P95 latencies: 1.5-2.5ms
* CPU usage: 60-75%

**Resource-Constrained Conditions** (lower CPU/memory):

* Health endpoint: 8,000-10,000 ops/s
* JWT validation: 6,000-8,000 ops/s
* P95 latencies: 3-5ms
* CPU usage: 80-95%

**Performance Variance Factors**:

* Container CPU allocation (major impact)
* Thread pool configuration (moderate impact)
* System background load (moderate impact)
* Token complexity/size (minor impact)

=== Latency Breakdown Analysis

**Typical 2ms JWT Request Breakdown**:

[source,text]
----
Total Request Time: 2,000μs (P95)
├─ HTTP/TLS Processing: ~1,600μs (80%)
│   ├─ Network latency: ~400μs
│   ├─ TLS handshake amortized: ~200μs  
│   ├─ HTTP parsing: ~500μs
│   └─ Response generation: ~500μs
├─ JWT Validation: ~300μs (15%)
│   ├─ Token parsing: ~50μs
│   ├─ Signature verification: ~200μs
│   └─ Claims validation: ~50μs
└─ Quarkus Framework: ~100μs (5%)
    ├─ CDI injection: ~30μs
    ├─ Security filters: ~40μs
    └─ Routing/dispatch: ~30μs
----

== Monitoring and Observability

=== Key Performance Indicators

**Continuous Monitoring Metrics**:

. **Throughput Trends**: ops/s over time for both endpoints
. **Latency Percentiles**: P50/P95/P99 tracking with alerting
. **Resource Utilization**: CPU, memory, thread pool usage

**Alert Thresholds** (based on M4 baseline):

* JWT throughput < 8,000 ops/s (sustained)
* P95 latency > 4ms (sustained)
* P99 latency > 8ms (sustained)
* CPU utilization > 85% (sustained)

=== Profiling and Deep Analysis

**JFR Integration**:

[source,bash]
----
# Run with JFR profiling enabled
./mvnw clean verify -Pbenchmark-jfr -pl benchmarking/benchmark-integration-quarkus

# Analyze CPU hotspots
jfr print --events ExecutionSample --stack-depth 20 target/benchmark-results/*.jfr

# Check GC impact
jfr print --events GarbageCollection target/benchmark-results/*.jfr
----

**Performance Regression Detection**:

[source,bash]
----
# Compare performance between builds
git checkout <baseline-commit>
./mvnw clean package -DskipTests
# Run baseline benchmarks and save results

git checkout <current-commit>  
./mvnw clean package -DskipTests
# Run current benchmarks and compare
----

== Performance Focus Areas

=== Current Benchmark Focus

**What We Measure**:

* Raw throughput under sustained load
* Latency distribution (P50, P95, P99)
* Resource utilization patterns
* Comparison between health and JWT endpoints

**Deliberately Excluded**:

* Cache optimization (cache disabled)
* Connection pooling optimizations
* Native compilation tuning beyond defaults
* Error handling paths (only positive cases)
* Alternative architectures or frameworks

=== Performance Baseline Purpose

The integration benchmarks establish a baseline for:

* Understanding end-to-end performance characteristics
* Comparing HTTP overhead vs JWT processing time
* Validating that micro benchmark improvements translate to real-world gains
* Detecting performance regressions in the integration layer

== Integration with Micro Benchmarks

=== Complementary Analysis

**Micro Benchmark Insights for Integration**:

* JWT validation component costs (50-200μs baseline)
* Algorithm performance comparisons
* Memory allocation patterns

**Integration Insights for Micro Benchmarks**:

* Real-world latency budgets for validation
* Network vs compute optimization priorities  
* Error handling performance requirements
* Production-like load patterns

=== Validation Pipeline

**Performance Regression Prevention**:

. **Micro benchmarks**: Detect core algorithm regressions
. **Integration benchmarks**: Detect system-level regressions
. **Load testing**: Validate under realistic conditions
. **Production monitoring**: Continuous validation

== Conclusion

Integration performance benchmarking provides essential insights into real-world JWT validation performance. The key findings show that:

**System Performance**:

* Native Quarkus achieves excellent throughput (8K-12K ops/s)
* Memory efficiency is outstanding (~6MB total usage)
* HTTP processing dominates total request time (80%)
* JWT validation overhead is reasonable (~15% of request time)

**Optimization Focus**:

* Container and network optimization has highest impact for integration benchmarks
* Baseline measurements enable regression detection
* Monitoring and observability enable continuous optimization

**Future Directions**:

* Enhanced profiling integration with JFR
* Automated performance regression detection
* Production workload simulation improvements
* Integration with CI/CD performance gates

The integration benchmarks complement micro benchmarks by providing end-to-end performance validation and ensuring that core optimizations translate to real-world performance improvements.