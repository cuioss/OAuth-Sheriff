= Benchmark Module Refactoring Plan
:toc: left
:toclevels: 3
:toc-title: Table of Contents
:sectnums:
:source-highlighter: highlight.js

[IMPORTANT]
====
This document follows the refactoring process standards defined in the CUI process documentation.
====

== Purpose

This document outlines the refactoring tasks identified for the cui-jwt benchmark module to improve code reuse, reduce duplication, and enhance maintainability through better distribution of responsibilities between modules.

== Code Structure and Design

=== C1. Consolidate Abstract Base Classes
* [x] *Priority:* High - *COMPLETED* (Commit: 64ad45cd)

*Description:* Extract common functionality from `AbstractBaseBenchmark` (benchmark-library) and `AbstractBaseBenchmark` (benchmark-integration-quarkus) into a unified base class in cui-benchmarking-common module.

*Rationale:* Eliminates code duplication and provides a single source of truth for benchmark base functionality, reducing maintenance overhead.

*Implementation Checklist:*

* [x] Analyze both `AbstractBaseBenchmark` classes for common functionality
* [x] Create new `AbstractBenchmarkBase` in `cui-benchmarking-common/src/main/java/de/cuioss/benchmarking/common/base/`
* [x] Extract common fields: `serviceUrl`, `benchmarkResultsDir`, `httpClient`
* [x] Extract common methods: `setupBase()`, `tearDown()`, `createBaseRequest()`
* [x] Update `benchmark-library/AbstractBenchmark` to extend new base class
* [x] Update `benchmark-integration-quarkus/AbstractBaseBenchmark` to extend new base class
* [x] Update all concrete benchmark classes to use new hierarchy
* [x] Add unit tests for new base class in cui-benchmarking-common

*Verification Steps:*

* [x] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [x] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-library`
* [x] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-integration-quarkus`
* [x] Execute library benchmarks (DO NOT cancel or shortcut - let it run completely): `./mvnw clean verify -pl benchmarking/benchmark-library -Pbenchmark`
* [x] Execute integration benchmarks (DO NOT cancel or shortcut - let it run completely): `./mvnw clean verify -pl benchmarking/benchmark-integration-quarkus -Pbenchmark-testing`
* [x] Verify benchmark results in `target/benchmark-results/`
* [x] Ensure no regression in benchmark performance metrics
* [x] Commit with message: `refactor: C1. Consolidate Abstract Base Classes`

=== C2. Unify Metrics Exporter Implementations
* [x] *Priority:* High - *COMPLETED* (Commit: 8b1e0ad6)

*Description:* Create a common `MetricsExporter` interface in cui-benchmarking-common with specific implementations for library benchmarks (`SimplifiedMetricsExporter`) and integration benchmarks (`SimpleMetricsExporter`).

*Rationale:* Standardizes metrics export functionality while allowing for implementation-specific variations through a clean interface design.

*Implementation Checklist:*

* [x] Create `MetricsExporter` interface in `cui-benchmarking-common/src/main/java/de/cuioss/benchmarking/common/metrics/`
* [x] Define interface methods: `exportMetrics()`, `exportToFile()`, `getExportFormat()`
* [x] Create `AbstractMetricsExporter` with common implementation
* [x] Refactor `SimplifiedMetricsExporter` to implement new interface
* [x] Refactor `SimpleMetricsExporter` to implement new interface
* [x] Extract common JSON serialization logic to abstract class
* [x] Update all benchmark classes to use interface type
* [x] Create factory class `MetricsExporterFactory` for instantiation

*Verification Steps:*
* [x] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [x] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-library`
* [x] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-integration-quarkus`
* [x] Execute benchmarks (DO NOT cancel or shortcut) and verify metrics export files are created correctly
* [x] Validate JSON structure of exported metrics matches expected format
* [x] Compare metrics values between old and new implementation
* [x] Run all metrics exporter tests
* [x] Commit with message: `refactor: C2. Unify Metrics Exporter Implementations`

=== C3. Clarify Token Repository Implementations
* [x] *Priority:* Medium - *COMPLETED*

*Description:* Rename and reorganize the two TokenRepository classes to clearly distinguish their purposes: `MockTokenRepository` for in-memory token generation and `KeycloakTokenRepository` for real token fetching. Consider creating a common `TokenProvider` interface.

*Rationale:* Current naming creates confusion about the purpose of each repository. Clear naming and potential interface extraction improves code clarity.

*Implementation Checklist:*


* [x] Create `TokenProvider` interface in `cui-benchmarking-common/src/main/java/de/cuioss/benchmarking/common/token/`
* [x] Define interface methods: `getNextToken()`, `getTokenPoolSize()`, `refreshTokens()`
* [x] Rename `benchmark-library/TokenRepository` to `MockTokenRepository`
* [x] Rename `cui-benchmarking-common/TokenRepository` to `KeycloakTokenRepository`
* [x] Both classes implement `TokenProvider` interface
* [x] Update all references in benchmark classes
* [x] Update configuration classes to use appropriate implementation
* [x] Add Javadoc clearly explaining the purpose of each implementation

*Verification Steps:*
* [x] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [x] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-library`
* [x] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-integration-quarkus`
* [x] Verify library benchmarks use MockTokenRepository
* [x] Verify integration benchmarks can use KeycloakTokenRepository
* [x] Run all token repository tests
* [x] Commit with message: `refactor: C3. Clarify Token Repository Implementations`

=== C4. Consolidate Constants
* [x] *Priority:* High - *COMPLETED*

*Description:* Create a unified `BenchmarkConstants` class in cui-benchmarking-common with nested classes for organization (e.g., `BenchmarkConstants.Metrics`, `BenchmarkConstants.Report`, `BenchmarkConstants.Integration`).

*Rationale:* Constants are currently scattered across modules. Centralization improves discoverability and reduces duplication.

*Implementation Checklist:*

* [x] Create `BenchmarkConstants` class in `cui-benchmarking-common/src/main/java/de/cuioss/benchmarking/common/constants/`
* [x] Create nested class `BenchmarkConstants.Metrics` with metric-related constants
* [x] Create nested class `BenchmarkConstants.Report` with report-related constants
* [x] Create nested class `BenchmarkConstants.Integration` with integration-related constants
* [x] Create nested class `BenchmarkConstants.Files` with file/directory constants
* [x] Migrate constants from `MetricConstants` (benchmark-integration-quarkus)
* [x] Migrate constants from `ReportConstants` (cui-benchmarking-common)
* [x] Update all references to use new constant locations
* [x] Remove deprecated constant classes

*Verification Steps:*
* [x] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [x] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-library`
* [x] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-integration-quarkus`
* [x] Verify no compilation errors from constant references
* [x] Check IDE for any unresolved constant references
* [x] Run grep to ensure no old constant class references remain
* [x] Commit with message: `refactor: C4. Consolidate Constants`

=== C5. Strengthen Runner Infrastructure
* [ ] *Priority:* Medium

*Description:* Enhance `AbstractBenchmarkRunner` with template method pattern to better support the various runner implementations (LibraryBenchmarkRunner, JfrBenchmarkRunner, QuarkusIntegrationRunner).

*Rationale:* Current abstract runner is underutilized. A proper template method pattern would reduce code duplication in concrete runners.

*Implementation Checklist:*

* [ ] Define template method `runBenchmark()` in `AbstractBenchmarkRunner`
* [ ] Add abstract methods: `prepareBenchmark()`, `executeBenchmark()`, `processResults()`, `cleanup()`
* [ ] Add hook methods: `beforeBenchmark()`, `afterBenchmark()` with default empty implementations
* [ ] Extract common JMH options building to base class
* [ ] Extract common result processing to base class
* [ ] Update `LibraryBenchmarkRunner` to use template pattern
* [ ] Update `JfrBenchmarkRunner` to use template pattern
* [ ] Update `QuarkusIntegrationRunner` to use template pattern
* [ ] Add configuration validation in base class

*Verification Steps:*
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-library`
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-integration-quarkus`
* [ ] Execute each runner type and verify correct execution flow (DO NOT cancel or shortcut any runs)
* [ ] Verify benchmark results are generated correctly
* [ ] Check that runner-specific features still work (JFR events, integration metrics)
* [ ] Commit with message: `refactor: C5. Strengthen Runner Infrastructure`

=== C6. Create Metrics Processing Pipeline
* [ ] *Priority:* Medium

*Description:* Implement chain-of-responsibility or pipeline pattern for metrics processing to replace multiple overlapping `MetricsPostProcessor` implementations.

*Rationale:* Current implementations have significant overlap. A pipeline approach allows for modular, reusable processing stages.

*Implementation Checklist:*

* [ ] Create `MetricsProcessor` interface with `process(MetricsContext context)` method
* [ ] Create `MetricsContext` class to hold metrics data through pipeline
* [ ] Create `MetricsPipeline` class to manage processor chain
* [ ] Implement processors: `ValidationProcessor`, `AggregationProcessor`, `EnrichmentProcessor`
* [ ] Implement processors: `FormatProcessor`, `ExportProcessor`
* [ ] Refactor existing `MetricsPostProcessor` classes to use pipeline
* [ ] Add pipeline configuration support
* [ ] Create builder pattern for pipeline construction

*Verification Steps:*
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-integration-quarkus`
* [ ] Execute benchmarks with metrics processing (DO NOT cancel or shortcut)
* [ ] Verify all metrics are processed correctly through pipeline
* [ ] Check that processed metrics match expected format
* [ ] Run all metrics processor tests
* [ ] Commit with message: `refactor: C6. Create Metrics Processing Pipeline`

=== C7. Extract JFR Utilities to Common Module
* [ ] *Priority:* Low

*Description:* Move JFR event handling and instrumentation utilities from benchmark-library to cui-benchmarking-common for potential reuse in integration benchmarks.

*Rationale:* JFR functionality could benefit integration benchmarks. Centralization enables broader usage.

*Implementation Checklist:*

* [ ] Create package `de.cuioss.benchmarking.common.jfr` in cui-benchmarking-common
* [ ] Move `JfrInstrumentation` class to common module
* [ ] Move `JfrVarianceAnalyzer` class to common module
* [ ] Move JFR event classes to common module
* [ ] Update package references in benchmark-library
* [ ] Add JFR support detection utility
* [ ] Create JFR configuration class for common settings
* [ ] Update documentation for JFR usage

*Verification Steps:*
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-library`
* [ ] Execute JFR benchmarks (DO NOT cancel or shortcut - let it run completely): `./mvnw clean verify -pl benchmarking/benchmark-library -Pbenchmark-jfr`
* [ ] Verify JFR events are recorded correctly
* [ ] Check JFR analysis reports are generated
* [ ] Commit with message: `refactor: C7. Extract JFR Utilities to Common Module`

=== C8. Separate Report Generation from Data Processing
* [ ] *Priority:* Low

*Description:* Refactor report generation to separate rendering logic from data aggregation using visitor pattern or similar approach.

*Rationale:* Current implementation mixes concerns. Separation improves testability and allows for alternative report formats.

*Implementation Checklist:*

* [ ] Create `ReportData` model classes for report data structure
* [ ] Create `ReportRenderer` interface with `render(ReportData data)` method
* [ ] Implement `HtmlReportRenderer` for HTML output
* [ ] Implement `JsonReportRenderer` for JSON output
* [ ] Implement `MarkdownReportRenderer` for Markdown output
* [ ] Extract data aggregation logic to `ReportDataBuilder`
* [ ] Update `ReportGenerator` to use renderer pattern
* [ ] Add renderer factory for format selection

*Verification Steps:*
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Generate reports in all formats (HTML, JSON, Markdown)
* [ ] Verify report content is consistent across formats
* [ ] Check HTML report renders correctly in browser
* [ ] Validate JSON report structure
* [ ] Review Markdown report formatting
* [ ] Commit with message: `refactor: C8. Separate Report Generation from Data Processing`

=== C9. Merge or Clarify Metrics Computation Responsibilities
* [ ] *Priority:* Low

*Description:* Review and refactor `MetricsComputer` and `TrendDataProcessor` to either merge overlapping functionality or clearly separate statistical computation from trend analysis.

*Rationale:* Overlapping responsibilities create confusion about which component to use for specific computations.

*Implementation Checklist:*

* [ ] Analyze current responsibilities of `MetricsComputer`
* [ ] Analyze current responsibilities of `TrendDataProcessor`
* [ ] Identify overlapping functionality
* [ ] Create `StatisticsCalculator` for pure statistical computations
* [ ] Refactor `MetricsComputer` to focus on metric-specific calculations
* [ ] Refactor `TrendDataProcessor` to focus on time-series analysis
* [ ] Update all usages to use appropriate component
* [ ] Add clear Javadoc explaining when to use each component

*Verification Steps:*
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Execute benchmarks with trend analysis (DO NOT cancel or shortcut)
* [ ] Verify statistical calculations are correct
* [ ] Check trend detection works properly
* [ ] Compare results with previous implementation
* [ ] Commit with message: `refactor: C9. Merge or Clarify Metrics Computation Responsibilities`

== Performance Improvements

=== P1. Enhance HTTP Client Management
* [ ] *Priority:* Medium

*Description:* Add connection pooling configuration and timeout presets to `HttpClientFactory` for different benchmark scenarios (short-lived vs long-running benchmarks).

*Rationale:* Current implementation uses basic clients. Connection pooling and scenario-specific configurations would improve benchmark performance.

*Implementation Checklist:*

* [ ] Add `HttpClientConfig` class with pooling and timeout settings
* [ ] Create preset configurations: `SHORT_LIVED`, `LONG_RUNNING`, `HIGH_CONCURRENCY`
* [ ] Implement connection pool management in `HttpClientFactory`
* [ ] Add methods: `getPooledClient(HttpClientConfig config)`
* [ ] Add connection pool monitoring/metrics
* [ ] Update existing client creation to use pooling
* [ ] Add configuration through system properties
* [ ] Document recommended settings for different scenarios

*Verification Steps:*
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-integration-quarkus`
* [ ] Execute integration benchmarks with different client configurations (DO NOT cancel or shortcut)
* [ ] Monitor connection pool usage during benchmarks
* [ ] Verify performance improvement with pooling enabled
* [ ] Check no connection leaks occur
* [ ] Commit with message: `refactor: P1. Enhance HTTP Client Management`

=== P2. Optimize Template Processing
* [ ] *Priority:* Low

*Description:* Create template engine abstraction for report generation to support multiple formats (HTML, Markdown, JSON) with caching of compiled templates.

*Rationale:* Current HTML-only approach limits flexibility. Template abstraction with caching improves performance and extensibility.

*Implementation Checklist:*

* [ ] Create `TemplateEngine` interface with `render(template, context)` method
* [ ] Implement `MustacheTemplateEngine` for HTML templates
* [ ] Implement `FreemarkerTemplateEngine` as alternative
* [ ] Add template caching mechanism
* [ ] Create `TemplateContext` for passing data to templates
* [ ] Add template precompilation support
* [ ] Update report generation to use template engine
* [ ] Add configuration for template engine selection

*Verification Steps:*
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Generate reports using different template engines
* [ ] Measure template processing performance
* [ ] Verify template caching reduces processing time
* [ ] Check generated output matches expected format
* [ ] Commit with message: `refactor: P2. Optimize Template Processing`

== Security Enhancements

=== S1. Add Configuration Validation
* [ ] *Priority:* Medium

*Description:* Implement comprehensive validation for benchmark configurations to ensure security-relevant settings (SSL verification, token handling) are properly configured.

*Rationale:* Configuration errors can lead to security vulnerabilities or misleading benchmark results.

*Implementation Checklist:*

* [ ] Create `ConfigurationValidator` class
* [ ] Add validation for SSL/TLS settings
* [ ] Add validation for token handling configuration
* [ ] Add validation for endpoint URLs (prevent SSRF)
* [ ] Add validation for file paths (prevent path traversal)
* [ ] Implement validation annotations for configuration classes
* [ ] Add startup validation in runners
* [ ] Create detailed validation error messages
* [ ] Add configuration schema documentation

*Verification Steps:*
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Test with invalid configurations and verify proper error handling
* [ ] Test with missing required configurations
* [ ] Verify SSL validation works correctly
* [ ] Check that insecure configurations are rejected in production mode
* [ ] Run security scanning tools on configuration handling
* [ ] Commit with message: `refactor: S1. Add Configuration Validation`

== Testing Improvements

=== T1. Consolidate Test Data Management
* [ ] *Priority:* High

*Description:* Create test data factory in cui-benchmarking-common test utilities to centralize test resource management and reduce duplication.

*Rationale:* Test resources are currently scattered across modules with significant duplication. Centralization improves test maintainability.

*Implementation Checklist:*

* [ ] Create `TestDataFactory` in `cui-benchmarking-common/src/test/java`
* [ ] Add methods for creating test tokens
* [ ] Add methods for creating test metrics
* [ ] Add methods for creating test benchmark results
* [ ] Add methods for loading test JSON files
* [ ] Create `TestResourceLoader` for file resources
* [ ] Consolidate duplicate test JSON files
* [ ] Update all test classes to use factory
* [ ] Remove duplicate test data files

*Verification Steps:*
* [ ] Run `./mvnw test -pl benchmarking/cui-benchmarking-common`
* [ ] Run `./mvnw test -pl benchmarking/benchmark-library`
* [ ] Run `./mvnw test -pl benchmarking/benchmark-integration-quarkus`
* [ ] Verify all tests pass with new test data factory
* [ ] Check no duplicate test resources remain
* [ ] Ensure test coverage remains the same or improves
* [ ] Commit with message: `refactor: T1. Consolidate Test Data Management`

=== T2. Reduce MetricsPostProcessor Test Redundancy
* [ ] *Priority:* Medium

*Description:* Create parameterized tests or test fixtures for MetricsPostProcessor testing to eliminate duplicate test patterns.

*Rationale:* Multiple test classes implement similar test patterns. Parameterized tests reduce code duplication.

*Implementation Checklist:*

* [ ] Identify common test patterns across MetricsPostProcessor tests
* [ ] Create `AbstractMetricsProcessorTest` base class
* [ ] Implement parameterized test methods
* [ ] Create test fixtures for common test scenarios
* [ ] Extract test data sets to shared constants
* [ ] Update existing tests to use parameterized approach
* [ ] Remove redundant test methods
* [ ] Add test documentation explaining parameterization

*Verification Steps:*
* [ ] Run `./mvnw test -pl benchmarking/benchmark-integration-quarkus`
* [ ] Verify all test scenarios are still covered
* [ ] Check test execution time (should be similar or faster)
* [ ] Ensure test failure messages are still clear
* [ ] Review code coverage reports
* [ ] Commit with message: `refactor: T2. Reduce MetricsPostProcessor Test Redundancy`

=== T3. Create Pluggable Validation Rules
* [ ] *Priority:* Low

*Description:* Extend `BenchmarkResultValidator` with a pluggable validation rules system to support custom validation requirements.

*Rationale:* Current validator has fixed rules. Pluggable system allows for project-specific validation needs.

*Implementation Checklist:*

* [ ] Create `ValidationRule` interface with `validate(BenchmarkResult)` method
* [ ] Create `ValidationContext` for passing validation state
* [ ] Implement default rules: `ThresholdRule`, `ConsistencyRule`, `OutlierRule`
* [ ] Add rule registration mechanism
* [ ] Add rule configuration support
* [ ] Create `ValidationReport` for detailed results
* [ ] Update `BenchmarkResultValidator` to use rule system
* [ ] Add custom rule examples in documentation

*Verification Steps:*
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Test with various validation rules
* [ ] Verify custom rules can be added
* [ ] Check validation reports are comprehensive
* [ ] Test rule configuration changes
* [ ] Commit with message: `refactor: T3. Create Pluggable Validation Rules`

== Dependency Management

=== D1. Centralize Logging Configuration
* [ ] *Priority:* High

*Description:* Move duplicate `benchmark-logging.properties` files to cui-benchmarking-common with support for environment-specific overrides.

*Rationale:* Duplicate configuration files increase maintenance burden. Centralization with override capability provides flexibility.

*Implementation Checklist:*

* [ ] Move `benchmark-logging.properties` to `cui-benchmarking-common/src/main/resources`
* [ ] Create `benchmark-logging-dev.properties` for development
* [ ] Create `benchmark-logging-prod.properties` for production
* [ ] Add profile-based loading mechanism
* [ ] Remove duplicate logging configuration files
* [ ] Update logging initialization in all modules
* [ ] Add system property for custom logging config
* [ ] Document logging configuration approach

*Verification Steps:*
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-library`
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-integration-quarkus`
* [ ] Verify logging works correctly in all modules
* [ ] Test with different logging profiles
* [ ] Check log output format is consistent
* [ ] Commit with message: `refactor: D1. Centralize Logging Configuration`

=== D2. Standardize JSON Processing
* [ ] *Priority:* Medium

*Description:* Review and standardize JSON serialization approach - either use Gson features more effectively or migrate to Jackson for consistency with other CUI projects.

*Rationale:* Current `JsonSerializationHelper` reinvents some Gson functionality. Standardization reduces code and improves consistency.

*Implementation Checklist:*

* [ ] Audit current JSON processing usage across modules
* [ ] Evaluate Gson vs Jackson for CUI project consistency
* [ ] If keeping Gson: optimize `JsonSerializationHelper` to use Gson features
* [ ] If migrating to Jackson: create migration plan
* [ ] Update JSON serialization to use chosen approach
* [ ] Add custom serializers/deserializers as needed
* [ ] Remove redundant JSON utility methods
* [ ] Update all JSON processing tests

*Verification Steps:*
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-library`
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-integration-quarkus`
* [ ] Verify JSON output format remains compatible
* [ ] Test JSON round-trip serialization
* [ ] Check performance of JSON processing
* [ ] Commit with message: `refactor: D2. Standardize JSON Processing`

== Documentation Improvements

=== DOC1. Document Module Responsibilities
* [ ] *Priority:* High

*Description:* Create clear documentation defining the responsibilities and boundaries of each benchmark module (benchmark-library, benchmark-integration-quarkus, cui-benchmarking-common).

*Rationale:* Current module boundaries are unclear, leading to code placement confusion and duplication.

*Implementation Checklist:*

* [ ] Create `ARCHITECTURE.adoc` in benchmarking root
* [ ] Document cui-benchmarking-common responsibilities
* [ ] Document benchmark-library responsibilities
* [ ] Document benchmark-integration-quarkus responsibilities
* [ ] Create module dependency diagram
* [ ] Define clear rules for code placement
* [ ] Add examples of what belongs in each module
* [ ] Update README files in each module

*Verification Steps:*
* [ ] Review documentation for clarity and completeness
* [ ] Validate module dependencies match documentation
* [ ] Check for any circular dependencies
* [ ] Ensure examples are accurate
* [ ] Get team review of architecture documentation
* [ ] Commit with message: `docs: DOC1. Document Module Responsibilities`

=== DOC2. Create Benchmark Development Guide
* [ ] *Priority:* Medium

*Description:* Document how to create new benchmarks, including which base classes to use, how to configure metrics, and how to integrate with the reporting system.

*Rationale:* Lack of documentation makes it difficult for new developers to contribute benchmarks correctly.

*Implementation Checklist:*

* [ ] Create `DEVELOPMENT-GUIDE.adoc` in benchmarking root
* [ ] Document benchmark types (library vs integration)
* [ ] Explain base class selection criteria
* [ ] Provide step-by-step benchmark creation guide
* [ ] Document metrics configuration options
* [ ] Explain report integration process
* [ ] Add troubleshooting section
* [ ] Include example benchmark implementation

*Verification Steps:*
* [ ] Follow guide to create a sample benchmark
* [ ] Verify all steps are accurate and complete
* [ ] Test example code compiles and runs
* [ ] Check metrics and reports generate correctly
* [ ] Get feedback from team members
* [ ] Commit with message: `docs: DOC2. Create Benchmark Development Guide`

== Future Enhancements

=== F1. Configuration File Support
* [ ] *Priority:* Low

*Description:* Add YAML/JSON configuration file support with profiles (dev, ci, prod) to supplement system property configuration.

*Rationale:* System property configuration is cumbersome for complex setups. File-based configuration with profiles improves usability.

*Implementation Checklist:*

* [ ] Add YAML parser dependency (SnakeYAML or similar)
* [ ] Create `BenchmarkConfig` YAML schema
* [ ] Implement `ConfigurationLoader` for file reading
* [ ] Add profile support (dev, ci, prod)
* [ ] Create default configuration files
* [ ] Add configuration override mechanism
* [ ] Update runners to load file configuration
* [ ] Document configuration file format

*Verification Steps:*
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Test configuration loading from files
* [ ] Verify profile selection works
* [ ] Check property override mechanism
* [ ] Test with invalid configuration files
* [ ] Commit with message: `feature: F1. Configuration File Support`

=== F2. Enhanced Badge Generation
* [ ] *Priority:* Low

*Description:* Extract and enhance BadgeGenerator with configurable thresholds, styles, and support for multiple badge formats.

*Rationale:* Current implementation is basic. Enhanced badges provide better visual feedback for benchmark results.

*Implementation Checklist:*

* [ ] Extract `BadgeGenerator` to separate package
* [ ] Add configurable threshold support
* [ ] Implement multiple badge styles (flat, flat-square, for-the-badge)
* [ ] Add SVG generation support
* [ ] Add shields.io compatible JSON output
* [ ] Create badge configuration class
* [ ] Add color scheme customization
* [ ] Document badge usage and embedding

*Verification Steps:*
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Generate badges in all formats
* [ ] Verify SVG badges render correctly
* [ ] Test threshold configuration
* [ ] Check color schemes work properly
* [ ] Commit with message: `feature: F2. Enhanced Badge Generation`

=== F3. Historical Data Versioning
* [ ] *Priority:* Low

*Description:* Add data versioning and migration support to `HistoricalDataManager` for handling benchmark result schema changes.

*Rationale:* Schema evolution is not currently handled. Versioning ensures historical data remains accessible after schema changes.

*Implementation Checklist:*

* [ ] Add version field to benchmark result schema
* [ ] Create `DataMigration` interface
* [ ] Implement migration registry
* [ ] Add automatic migration on data load
* [ ] Create migration for current schema
* [ ] Add backwards compatibility support
* [ ] Document migration process
* [ ] Add migration testing utilities

*Verification Steps:*
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Test data migration with sample data
* [ ] Verify backwards compatibility
* [ ] Check migration performance
* [ ] Test with multiple version jumps
* [ ] Commit with message: `feature: F3. Historical Data Versioning`

=== F4. Configuration Registry Pattern
* [ ] *Priority:* Low

*Description:* Implement configuration factory/registry pattern for managing different benchmark profiles and scenarios.

*Rationale:* Current configuration is monolithic. Registry pattern allows for dynamic configuration selection based on context.

*Implementation Checklist:*

* [ ] Create `ConfigurationRegistry` class
* [ ] Implement `ConfigurationFactory` interface
* [ ] Add configuration registration mechanism
* [ ] Implement profile-based selection
* [ ] Add configuration inheritance support
* [ ] Create predefined configuration sets
* [ ] Add runtime configuration switching
* [ ] Document registry usage patterns

*Verification Steps:*
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Test configuration registration
* [ ] Verify profile selection works
* [ ] Check configuration inheritance
* [ ] Test runtime switching
* [ ] Commit with message: `feature: F4. Configuration Registry Pattern`

== Implementation Approach

=== Prioritization Strategy

Tasks should be implemented in priority order:

1. *High Priority Tasks* (C1, C2, C4, T1, D1, DOC1): These provide immediate value with minimal disruption
2. *Medium Priority Tasks* (C3, C5, C6, P1, S1, T2, D2, DOC2): Good value with moderate effort
3. *Low Priority Tasks* (C7, C8, C9, P2, T3, F1-F4): Nice-to-have improvements

=== Implementation Guidelines

* Focus on one task at a time
* Complete all verification steps before marking task complete
* Run full benchmark suite after each task
* Update documentation as part of task completion
* Use task identifiers in commit messages
* Ensure no performance regression occurs

== Success Criteria

Each task is considered complete when:

1. All implementation checklist items are checked
2. All verification steps pass successfully
3. Pre-commit build passes: `./mvnw -Ppre-commit clean install`
4. Full benchmark execution completes without errors
5. Performance metrics show no regression
6. Documentation is updated
7. Changes are committed with appropriate message

== Conclusion

This refactoring plan addresses the identified opportunities for improvement in the benchmark module, focusing on code consolidation, reusability, and maintainability. The detailed checklists and verification steps ensure systematic implementation with quality assurance at each stage.