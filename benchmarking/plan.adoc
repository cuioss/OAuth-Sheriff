= Benchmark Module Refactoring Plan
:toc: left
:toclevels: 3
:toc-title: Table of Contents
:sectnums:
:source-highlighter: highlight.js

[IMPORTANT]
====
This document follows the refactoring process standards defined in the CUI process documentation.
Pre-1.0 rule: No deprecation nor transitionally comments
====

== Purpose

This document outlines the refactoring tasks identified for the cui-jwt benchmark module to improve code reuse, reduce duplication, and enhance maintainability through better distribution of responsibilities between modules.

== Code Structure and Design

=== C6. Create Metrics Processing Pipeline
* [x] *Priority:* Medium

*Description:* Implement chain-of-responsibility or pipeline pattern for metrics processing to replace multiple overlapping `MetricsPostProcessor` implementations.

*Rationale:* Current implementations have significant overlap. A pipeline approach allows for modular, reusable processing stages.

*Implementation Checklist:*

* [x] Create `MetricsProcessor` interface with `process(MetricsContext context)` method
* [x] Create `MetricsContext` class to hold metrics data through pipeline
* [x] Create `MetricsPipeline` class to manage processor chain
* [x] Implement processors: `ValidationProcessor`, `AggregationProcessor`, `EnrichmentProcessor`
* [x] Implement processors: `FormatProcessor`, `ExportProcessor`
* [x] Refactor existing `MetricsPostProcessor` classes to use pipeline
* [x] Add pipeline configuration support
* [x] Create builder pattern for pipeline construction

*Verification Steps:*

* [x] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [x] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-integration-quarkus`
* [x] Execute benchmarks with metrics processing (DO NOT cancel or shortcut)
* [x] Verify all metrics are processed correctly through pipeline
* [x] Check that processed metrics match expected format
* [x] Run all metrics processor tests
* [x] Commit with message: `refactor: C6. Create Metrics Processing Pipeline`

=== C7. Extract JFR Utilities to Common Module
* [ ] *Priority:* Low

*Description:* Move JFR event handling and instrumentation utilities from benchmark-library to cui-benchmarking-common for potential reuse in integration benchmarks.

*Rationale:* JFR functionality could benefit integration benchmarks. Centralization enables broader usage.

*Implementation Checklist:*

* [ ] Create package `de.cuioss.benchmarking.common.jfr` in cui-benchmarking-common
* [ ] Move `JfrInstrumentation` class to common module
* [ ] Move `JfrVarianceAnalyzer` class to common module
* [ ] Move JFR event classes to common module
* [ ] Update package references in benchmark-library
* [ ] Add JFR support detection utility
* [ ] Create JFR configuration class for common settings
* [ ] Update documentation for JFR usage

*Verification Steps:*

* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-library`
* [ ] Execute JFR benchmarks (DO NOT cancel or shortcut - let it run completely): `./mvnw clean verify -pl benchmarking/benchmark-library -Pbenchmark-jfr`
* [ ] Verify JFR events are recorded correctly
* [ ] Check JFR analysis reports are generated
* [ ] Commit with message: `refactor: C7. Extract JFR Utilities to Common Module`


=== C9. Merge or Clarify Metrics Computation Responsibilities
* [ ] *Priority:* Low

*Description:* Review and refactor `MetricsComputer` and `TrendDataProcessor` to either merge overlapping functionality or clearly separate statistical computation from trend analysis.

*Rationale:* Overlapping responsibilities create confusion about which component to use for specific computations.

*Implementation Checklist:*

* [ ] Analyze current responsibilities of `MetricsComputer`
* [ ] Analyze current responsibilities of `TrendDataProcessor`
* [ ] Identify overlapping functionality
* [ ] Create `StatisticsCalculator` for pure statistical computations
* [ ] Refactor `MetricsComputer` to focus on metric-specific calculations
* [ ] Refactor `TrendDataProcessor` to focus on time-series analysis
* [ ] Update all usages to use appropriate component
* [ ] Add clear Javadoc explaining when to use each component

*Verification Steps:*

* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Execute benchmarks with trend analysis (DO NOT cancel or shortcut)
* [ ] Verify statistical calculations are correct
* [ ] Check trend detection works properly
* [ ] Compare results with previous implementation
* [ ] Commit with message: `refactor: C9. Merge or Clarify Metrics Computation Responsibilities`

== Performance Improvements

=== P1. Enhance HTTP Client Management
* [ ] *Priority:* Medium

*Description:* Add connection pooling configuration and timeout presets to `HttpClientFactory` for different benchmark scenarios (short-lived vs long-running benchmarks).

*Rationale:* Current implementation uses basic clients. Connection pooling and scenario-specific configurations would improve benchmark performance.

*Implementation Checklist:*

* [ ] Add `HttpClientConfig` class with pooling and timeout settings
* [ ] Create preset configurations: `SHORT_LIVED`, `LONG_RUNNING`, `HIGH_CONCURRENCY`
* [ ] Implement connection pool management in `HttpClientFactory`
* [ ] Add methods: `getPooledClient(HttpClientConfig config)`
* [ ] Add connection pool monitoring/metrics
* [ ] Update existing client creation to use pooling
* [ ] Add configuration through system properties
* [ ] Document recommended settings for different scenarios

*Verification Steps:*

* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-integration-quarkus`
* [ ] Execute integration benchmarks with different client configurations (DO NOT cancel or shortcut)
* [ ] Monitor connection pool usage during benchmarks
* [ ] Verify performance improvement with pooling enabled
* [ ] Check no connection leaks occur
* [ ] Commit with message: `refactor: P1. Enhance HTTP Client Management`

== Testing Improvements

=== T1. Consolidate Test Data Management
* [x] *Priority:* High

*Description:* Create test data factory in cui-benchmarking-common test utilities to centralize test resource management and reduce duplication.

*Rationale:* Test resources are currently scattered across modules with significant duplication. Centralization improves test maintainability.

*Implementation Checklist:*

* [x] Create `TestDataFactory` in `cui-benchmarking-common/src/test/java`
* [x] Add methods for creating test tokens
* [x] Add methods for creating test metrics
* [x] Add methods for creating test benchmark results
* [x] Add methods for loading test JSON files
* [x] Create `TestResourceLoader` for file resources
* [x] Consolidate duplicate test JSON files
* [x] Update all test classes to use factory
* [x] Remove duplicate test data files

*Verification Steps:*

* [x] Run `./mvnw test -pl benchmarking/cui-benchmarking-common`
* [x] Run `./mvnw test -pl benchmarking/benchmark-library`
* [x] Run `./mvnw test -pl benchmarking/benchmark-integration-quarkus`
* [x] Verify all tests pass with new test data factory
* [x] Check no duplicate test resources remain
* [x] Ensure test coverage remains the same or improves
* [x] Commit with message: `refactor: T1. Consolidate Test Data Management`

=== T2. Reduce MetricsPostProcessor Test Redundancy
* [x] *Priority:* Medium

*Description:* Create parameterized tests or test fixtures for MetricsPostProcessor testing to eliminate duplicate test patterns.

*Rationale:* Multiple test classes implement similar test patterns. Parameterized tests reduce code duplication.

*Implementation Checklist:*

* [x] Identify common test patterns across MetricsPostProcessor tests
* [x] Create `AbstractMetricsProcessorTest` base class
* [x] Implement parameterized test methods
* [x] Create test fixtures for common test scenarios
* [x] Extract test data sets to shared constants
* [x] Update existing tests to use parameterized approach
* [x] Remove redundant test methods
* [x] Add test documentation explaining parameterization

*Verification Steps:*

* [x] Run `./mvnw test -pl benchmarking/benchmark-integration-quarkus`
* [x] Verify all test scenarios are still covered
* [x] Check test execution time (should be similar or faster)
* [x] Ensure test failure messages are still clear
* [x] Review code coverage reports
* [x] Commit with message: `refactor: T2. Reduce MetricsPostProcessor Test Redundancy`

== Dependency Management

=== D1. Centralize Logging Configuration
* [ ] *Priority:* High

*Description:* Move duplicate `benchmark-logging.properties` files to cui-benchmarking-common with support for environment-specific overrides.

*Rationale:* Duplicate configuration files increase maintenance burden. Centralization with override capability provides flexibility.

*Implementation Checklist:*

* [ ] Move `benchmark-logging.properties` to `cui-benchmarking-common/src/main/resources`
* [ ] Create `benchmark-logging-dev.properties` for development
* [ ] Create `benchmark-logging-prod.properties` for production
* [ ] Add profile-based loading mechanism
* [ ] Remove duplicate logging configuration files
* [ ] Update logging initialization in all modules
* [ ] Add system property for custom logging config
* [ ] Document logging configuration approach

*Verification Steps:*

* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-library`
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-integration-quarkus`
* [ ] Verify logging works correctly in all modules
* [ ] Test with different logging profiles
* [ ] Check log output format is consistent
* [ ] Commit with message: `refactor: D1. Centralize Logging Configuration`

=== D2. Standardize JSON Processing with GSON
* [ ] *Priority:* Medium

*Description:* Optimize JSON serialization by leveraging GSON features more effectively. GSON remains the standard JSON processing library for this project.

*Rationale:* Current `JsonSerializationHelper` reinvents some GSON functionality. Better utilization of GSON features reduces code and improves maintainability.

*Implementation Checklist:*

* [ ] Audit current JSON processing usage across modules
* [ ] Identify GSON features that can replace custom implementations
* [ ] Optimize `JsonSerializationHelper` to use GSON features more effectively
* [ ] Leverage GSON's built-in type adapters and serializers
* [ ] Configure GSON instances for optimal performance (singleton pattern)
* [ ] Add custom GSON type adapters/serializers as needed
* [ ] Remove redundant JSON utility methods
* [ ] Update all JSON processing tests

*Verification Steps:*

* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-library`
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-integration-quarkus`
* [ ] Verify JSON output format remains compatible
* [ ] Test JSON round-trip serialization
* [ ] Check performance of JSON processing
* [ ] Commit with message: `refactor: D2. Standardize JSON Processing with GSON`

== Documentation Improvements

=== DOC1. Document Module Responsibilities
* [ ] *Priority:* High

*Description:* Create clear documentation defining the responsibilities and boundaries of each benchmark module (benchmark-library, benchmark-integration-quarkus, cui-benchmarking-common).

*Rationale:* Current module boundaries are unclear, leading to code placement confusion and duplication.

*Implementation Checklist:*

* [ ] Create `Architecture.adoc` in benchmarking root
* [ ] Document cui-benchmarking-common responsibilities
* [ ] Document benchmark-library responsibilities
* [ ] Document benchmark-integration-quarkus responsibilities
* [ ] Create module dependency diagram
* [ ] Define clear rules for code placement
* [ ] Add examples of what belongs in each module
* [ ] Update README files in each module

*Verification Steps:*
* [ ] Review documentation for clarity and completeness
* [ ] Validate module dependencies match documentation
* [ ] Check for any circular dependencies
* [ ] Ensure examples are accurate
* [ ] Get team review of architecture documentation
* [ ] Commit with message: `docs: DOC1. Document Module Responsibilities`

=== DOC2. Create Benchmark Development Guide
* [ ] *Priority:* Medium

*Description:* Document how to create new benchmarks, including which base classes to use, how to configure metrics, and how to integrate with the reporting system.

*Rationale:* Lack of documentation makes it difficult for new developers to contribute benchmarks correctly.

*Implementation Checklist:*

* [ ] Create `Development.adoc` in benchmarking root
* [ ] Document benchmark types (library vs integration)
* [ ] Explain base class selection criteria
* [ ] Provide step-by-step benchmark creation guide
* [ ] Document metrics configuration options
* [ ] Explain report integration process
* [ ] Add troubleshooting section
* [ ] Include example benchmark implementation

*Verification Steps:*

* [ ] Follow guide to create a sample benchmark
* [ ] Verify all steps are accurate and complete
* [ ] Test example code compiles and runs
* [ ] Check metrics and reports generate correctly
* [ ] Get feedback from team members
* [ ] Commit with message: `docs: DOC2. Create Benchmark Development Guide`
s
== Implementation Approach

=== Implementation Guidelines

* Focus on one task at a time
* Complete all verification steps before marking task complete
* Run full benchmark suite after each task
* Update documentation as part of task completion
* Use task identifiers in commit messages
* Ensure no performance regression occurs

== Success Criteria

Each task is considered complete when:

1. All implementation checklist items are checked
2. All verification steps pass successfully
3. Pre-commit build passes: `./mvnw -Ppre-commit clean install`
4. Full benchmark execution completes without errors
5. Performance metrics show no regression
6. Documentation is updated
7. Changes are committed with appropriate message

== Conclusion

This refactoring plan addresses the identified opportunities for improvement in the benchmark module, focusing on code consolidation, reusability, and maintainability. The detailed checklists and verification steps ensure systematic implementation with quality assurance at each stage.