= Benchmark Module Refactoring Plan
:toc: left
:toclevels: 3
:toc-title: Table of Contents
:sectnums:
:source-highlighter: highlight.js

[IMPORTANT]
====
This document follows the refactoring process standards defined in the CUI process documentation.

Pre-1.0 rule: No deprecation nor transitionally comments
====

== Purpose

This document outlines the refactoring tasks identified for the cui-jwt benchmark module to improve code reuse, reduce duplication, and enhance maintainability through better distribution of responsibilities between modules.

== Code Structure and Design

=== C7. Extract JFR Utilities to Common Module
* [ ] *Priority:* Low

*Description:* Move JFR event handling and instrumentation utilities from benchmark-library to cui-benchmarking-common for potential reuse in integration benchmarks.

*Rationale:* JFR functionality could benefit integration benchmarks. Centralization enables broader usage.

*Implementation Checklist:*

* [ ] Create package `de.cuioss.benchmarking.common.jfr` in cui-benchmarking-common
* [ ] Move `JfrInstrumentation` class to common module
* [ ] Move `JfrVarianceAnalyzer` class to common module
* [ ] Move JFR event classes to common module
* [ ] Update package references in benchmark-library
* [ ] Add JFR support detection utility
* [ ] Create JFR configuration class for common settings
* [ ] Update documentation for JFR usage

*Verification Steps:*

* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-library`
* [ ] Execute JFR benchmarks (DO NOT cancel or shortcut - let it run completely): `./mvnw clean verify -pl benchmarking/benchmark-library -Pbenchmark-jfr`
* [ ] Verify JFR events are recorded correctly
* [ ] Check JFR analysis reports are generated
* [ ] Commit with message: `refactor: C7. Extract JFR Utilities to Common Module`


=== C9. Merge or Clarify Metrics Computation Responsibilities
* [ ] *Priority:* Low

*Description:* Review and refactor `MetricsComputer` and `TrendDataProcessor` to either merge overlapping functionality or clearly separate statistical computation from trend analysis.

*Rationale:* Overlapping responsibilities create confusion about which component to use for specific computations.

*Implementation Checklist:*

* [ ] Analyze current responsibilities of `MetricsComputer`
* [ ] Analyze current responsibilities of `TrendDataProcessor`
* [ ] Identify overlapping functionality
* [ ] Create `StatisticsCalculator` for pure statistical computations
* [ ] Refactor `MetricsComputer` to focus on metric-specific calculations
* [ ] Refactor `TrendDataProcessor` to focus on time-series analysis
* [ ] Update all usages to use appropriate component
* [ ] Add clear Javadoc explaining when to use each component

*Verification Steps:*

* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Execute benchmarks with trend analysis (DO NOT cancel or shortcut)
* [ ] Verify statistical calculations are correct
* [ ] Check trend detection works properly
* [ ] Compare results with previous implementation
* [ ] Commit with message: `refactor: C9. Merge or Clarify Metrics Computation Responsibilities`

== Performance Improvements

=== P1. Enhance HTTP Client Management
* [ ] *Priority:* Medium

*Description:* Reduce complexity by adding simple connection pooling only, no preset overkill. Must run baseline benchmarks first to capture performance numbers, then implement simple pooling, run again and compare. Only commit if performance really improves.

*Rationale:* Current implementation uses basic clients. Simple connection pooling would improve benchmark performance if proven by measurements.

*Baseline Performance Numbers:*

*BEFORE IMPLEMENTATION - TO BE CAPTURED:*

* [ ] Baseline benchmark execution times
* [ ] Memory usage patterns  
* [ ] Connection creation overhead
* [ ] Throughput metrics
* [ ] Error rates and connection failures

*Implementation Checklist:*

* [ ] Run baseline benchmark tests and extract performance numbers into this plan
* [ ] Examine current HTTP client management implementation
* [ ] Add simple `HttpClientPool` with basic connection pooling only
* [ ] Update `HttpClientFactory` to use pooling (keep it simple)
* [ ] No presets, no overkill - just basic pooling functionality

*Post-Implementation Performance Numbers:*

*AFTER IMPLEMENTATION - TO BE CAPTURED:*

* [ ] Post-implementation benchmark execution times
* [ ] Memory usage patterns with pooling
* [ ] Connection reuse effectiveness
* [ ] Throughput improvements (if any)
* [ ] Error rates and connection stability

*Performance Comparison & Decision:*

* [ ] Compare baseline vs post-implementation numbers
* [ ] Calculate performance improvement percentage
* [ ] Present analysis with recommendation to commit or discard
* [ ] Only commit if measurable performance improvement is achieved

*Verification Steps:*

* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/cui-benchmarking-common`
* [ ] Run `./mvnw -Ppre-commit clean install -pl benchmarking/benchmark-integration-quarkus`
* [ ] Execute integration benchmarks to capture post-implementation metrics (DO NOT cancel or shortcut)
* [ ] Check no connection leaks occur
* [ ] Verify performance numbers justify the change
* [ ] Commit with message: `refactor: P1. Enhance HTTP Client Management` ONLY if performance improves

== Documentation Improvements

=== DOC1. Document Module Responsibilities
* [x] *Priority:* High

*Description:* Create clear documentation defining the responsibilities and boundaries of each benchmark module (benchmark-library, benchmark-integration-quarkus, cui-benchmarking-common).

*Rationale:* Current module boundaries are unclear, leading to code placement confusion and duplication.

*Implementation Checklist:*

* [x] Create `Architecture.adoc` in benchmarking root
* [x] Document cui-benchmarking-common responsibilities
* [x] Document benchmark-library responsibilities
* [x] Document benchmark-integration-quarkus responsibilities
* [x] Create module dependency diagram
* [x] Define clear rules for code placement
* [x] Add examples of what belongs in each module
* [x] Update README files in each module

*Verification Steps:*

* [x] Review documentation for clarity and completeness
* [x] Validate module dependencies match documentation
* [x] Check for any circular dependencies
* [x] Ensure examples are accurate
* [ ] Get team review of architecture documentation
* [ ] Commit with message: `docs: DOC1. Document Module Responsibilities`

=== DOC2. Create Benchmark Development Guide
* [ ] *Priority:* Medium

*Description:* Document how to create new benchmarks, including which base classes to use, how to configure metrics, and how to integrate with the reporting system.

*Rationale:* Lack of documentation makes it difficult for new developers to contribute benchmarks correctly.

*Implementation Checklist:*

* [ ] Create `Development.adoc` in benchmarking root
* [ ] Document benchmark types (library vs integration)
* [ ] Explain base class selection criteria
* [ ] Provide step-by-step benchmark creation guide
* [ ] Document metrics configuration options
* [ ] Explain report integration process
* [ ] Add troubleshooting section
* [ ] Include example benchmark implementation

*Verification Steps:*

* [ ] Follow guide to create a sample benchmark
* [ ] Verify all steps are accurate and complete
* [ ] Test example code compiles and runs
* [ ] Check metrics and reports generate correctly
* [ ] Get feedback from team members
* [ ] Commit with message: `docs: DOC2. Create Benchmark Development Guide`
s
== Implementation Approach

=== Implementation Guidelines

* Focus on one task at a time
* Complete all verification steps before marking task complete
* Run full benchmark suite after each task
* Update documentation as part of task completion
* Use task identifiers in commit messages
* Ensure no performance regression occurs

== Success Criteria

Each task is considered complete when:

1. All implementation checklist items are checked
2. All verification steps pass successfully
3. Pre-commit build passes: `./mvnw -Ppre-commit clean install`
4. Full benchmark execution completes without errors
5. Performance metrics show no regression
6. Documentation is updated
7. Changes are committed with appropriate message

== Conclusion

This refactoring plan addresses the identified opportunities for improvement in the benchmark module, focusing on code consolidation, reusability, and maintainability. The detailed checklists and verification steps ensure systematic implementation with quality assurance at each stage.