= JWT Benchmark Analysis - Baseline Performance (July 30, 2025)
:source-highlighter: highlight.js

== Executive Summary

This analysis documents the baseline performance metrics for the JWT validation library as of July 30, 2025. These results establish the current performance standards for optimization efforts.

== Test Configuration

* *Token Pool*: 600 tokens (200 per issuer)
* *Cache Size*: 60 entries (10% of token pool)
* *Token Variation*: Custom data claims (50-250 chars)
* *JVM*: OpenJDK 64-Bit Server VM 21.0.7+6-LTS
* *Threads*: 100 concurrent threads
* *Iterations*: 3 measurement, 1 warmup

== Baseline Performance Metrics

=== ðŸ“Š Throughput Performance

|===
|Benchmark Type |Throughput (ops/s) |Per-Thread 

|*Standard* |100,673 |1,007 ops/s/thread 
|*Error Handling (0%)* |113,581 |1,136 ops/s/thread 
|*Error Handling (50%)* |178,652 |1,787 ops/s/thread 
|===

=== ðŸ“ˆ Response Time Metrics

|===
|Benchmark Type |Latency (Î¼s) |Notes 

|*Standard Average* |860.66 |Average response time 
|*Concurrent* |883.40 |Under concurrent load 
|===

=== ðŸŽ¯ Component-Level Performance

|===
|Operation |P50 (Î¼s) |P95 (Î¼s) |P99 (Î¼s) |P99/P50 Ratio 

|*Complete Validation* |52-84 |82-101 |112-31,675 |2.2x-377x 
|*Signature Validation* |45-62 |69-71 |124-10,195 |2.1x-221x 
|*Token Parsing* |3.7-6.1 |6.0-7.8 |6.5-14.0 |1.8x-3.4x 
|*Claims Validation* |0.7-4.0 |1.2-5.7 |1.3-7.2 |1.6x-2.3x 
|*Token Building* |2.0-7.8 |3.7-11.0 |4.6-14.0 |1.5x-2.3x 
|*Header Validation* |0.1-0.5 |0.4-1.1 |0.5-1.6 |2.5x-5x 
|*Cache Operations* |0.0-0.4 |0.2-0.8 |0.3-1.1 |2.8x-âˆž 
|===

=== ðŸ“Š Variance Analysis

The benchmark results show varying P99 latencies across different workloads:
- *Complete Validation*: P99 ranges from 112Î¼s to 31,675Î¼s depending on workload (152Î¼s in average-time mode)
- *Signature Validation*: P99 spikes up to 10,195Î¼s in mixed token scenarios (124Î¼s in average-time mode)
- *Most Stable*: Token parsing and claims validation maintain consistent P99/P50 ratios &lt; 4x
- *Mode Comparison*: Average-time mode shows stable P99 of 152Î¼s vs up to 31,675Î¼s in throughput mode (both using 100 threads)

== Key Performance Characteristics

. *Throughput Baseline*:
+
* Standard: 100,673 ops/s
* Error 0%: 113,581 ops/s
* Error 50%: 178,652 ops/s (best performance)

. *Latency Profile*:
+
* Average: 860.66Î¼s
* Concurrent: 883.40Î¼s
* P50 range: 52-84Î¼s (good median performance)

. *P99 Variance*:
+
* Complete validation P99: 112Î¼s to 31.7ms (152Î¼s in average-time mode)
* Signature validation P99: 124Î¼s to 10.2ms (stable 124Î¼s in average-time mode)
* High P99/P50 ratios in throughput mode vs stable average-time mode (both with 100 threads)

. *Component Performance*:
+
* Signature validation: Dominant component (45-62Î¼s P50)
* Token parsing: Consistent performance (3.7-6.1Î¼s P50)
* Cache operations: Minimal overhead (0.0-0.4Î¼s P50)
* Average-time mode shows more predictable P99 behavior than throughput mode

== Optimization Opportunities

=== High Priority

. *P99 Latency Spikes*: Address 31.7ms spikes in throughput mode (average-time mode shows stable 152Î¼s with same 100 threads)
. *Signature Validation*: Reduce 10.2ms+ P99 spikes in mixed scenarios (average-time mode shows stable 124Î¼s)
. *P99/P50 Ratios*: Target &lt; 50x for predictable performance (average-time mode already achieves this)

=== Medium Priority

. *Throughput Enhancement*: Target &gt;200k ops/s baseline (current: 100,673 ops/s)
. *Thread Efficiency*: Improve from 1,007 to &gt;2,000 ops/s/thread
. *Average Latency*: Reduce from 860Î¼s to &lt;500Î¼s

=== Low Priority

. *Cache Optimization*: Already performing well (0.0-0.4Î¼s P50)
. *Token Parsing*: Stable performance with minor P99 variance (6.5-14.0Î¼s)
. *Header Validation*: Small absolute times despite ratios

== âœ… Implemented Optimizations

=== Completed Tasks

*Architecture &amp; Performance*:
- âœ… Field-based TokenSignatureValidator with Provider bypass optimization
- âœ… Virtual thread compatibility with ReentrantLock patterns, immutable Map.copyOf()
- âœ… JFR instrumentation with variance analysis, ValidationContext time caching
- âœ… Thread count optimization - 100 threads configuration
- âœ… Benchmark profile separation with distinct output directories

*Library Analysis*:
- âœ… Analyzed jjwt, smallrye-jwt, jose4j, auth0 - all use JCA without Signature caching
- âœ… Component performance breakdown completed

== Optimization Roadmap

=== High Priority - P99 Latency Reduction

* [x] *JFR-Based Load Analysis* - *Identify load-related P99 spike patterns* âœ…
* [x] Implement detailed JFR instrumentation for signature validation under load
* [x] Capture thread contention, GC pressure, and CPU throttling metrics
* [x] Analyze correlation between concurrent operations and P99 spikes
* [x] Compare throughput vs average-time mode behavior under identical load
* [ ] *Add Entropy Optimization Flags to Benchmarks*

* [ ] Add `-Djava.security.egd=file:/dev/./urandom` to regular benchmark profile
* [ ] Add `-Djava.security.egd=file:/dev/./urandom` to JFR benchmark profile
* [ ] Run complete benchmark suite with entropy flags
* [ ] Compare P99 results before/after entropy optimization
* [ ] Validate that Docker entropy blocking is eliminated
* [ ] *P99 Scheduler Fairness Issue* - *67ms spikes affect 1% of operations*

* [ ] Measure production impact vs benchmark synthetic load
* [ ] Monitor Virtual Thread carrier pool under load
* [ ] Accept current performance - RSA is optimal algorithm for this workload
* [ ] *Complete Validation Stabilization* - *31.7ms P99 spikes (377x P99/P50)*

* [ ] Profile validation hotspots causing extreme spikes
* [ ] Implement circuit breaker for pathological cases
* [ ] *Token Building Object Pooling* - *14.0Î¼s P99 spikes*

* [ ] Implement Apache Commons Pool for TokenBuilder instances
* [ ] *Claims Validation Optimization* - *7.2Î¼s P99 spikes*

* [ ] Profile validation logic for expensive operations
* [ ] Cache validation results for repeated claim patterns
* [ ] Optimize date/time claim validation

=== Medium Priority - Throughput Enhancement

* [ ] *Throughput Optimization* - *Current: 100k ops/s baseline*
* [ ] Optimize synchronization points
* [ ] Reduce allocation rates
* [ ] Implement zero-copy token handling where possible
* [ ] *Thread Efficiency* - *Current: 1,007 ops/s/thread*

* [ ] Reduce thread contention
* [ ] Optimize work distribution
* [ ] Consider work-stealing patterns
* [ ] *Async Architecture*

* [ ] Implement CompletableFuture-based validation pipeline
* [ ] Separate executors for parsing, signature, and claims validation
* [ ] Non-blocking I/O for issuer configuration resolution

=== Low Priority - Production Hardening

* [ ] *JFR Overhead Reduction*
* [ ] Conditional recording (&gt;100Î¼s threshold)
* [ ] Batch event recording
* [ ] *Memory &amp; GC Optimization*

* [ ] Reduce allocation rate
* [ ] Optimize hot allocation sites
* [ ] Test with different GC configurations

== Validation Methodology

=== Benchmark Commands

[source,bash]
----
# Standard benchmarks
./mvnw verify -Pbenchmark

# Component-level analysis
./mvnw verify -Pbenchmark-jfr

# Thread scaling analysis
./mvnw verify -Pbenchmark -Djmh.threads=1,50,100,150,200
----

== Conclusion

The JWT validation library baseline performance (July 30, 2025) shows:

*Current Strengths*:

. *Good median latency*: 52-84Î¼s P50 for complete validation
. *Error handling efficiency*: 178,652 ops/s with 50% error rate
. *Stable average-time mode performance*: P99 of 152Î¼s in average-time mode vs 31.7ms in throughput mode
. *Consistent core components*: Token parsing and claims validation show low variance

*Key Insights*:

. *Mode-dependent variance*: Throughput mode shows extreme P99 spikes while average-time mode remains stable (both with 100 threads)
. *Signature validation bottleneck*: 10.2ms P99 spikes in mixed scenarios vs 124Î¼s in average-time mode
. *Thread efficiency opportunity*: 1,007 ops/s/thread baseline performance

*Optimization Priorities*:

. *P99 latency reduction*: From 31.7ms to <5ms in throughput scenarios (High Priority)
. *Throughput enhancement*: From 100,673 to 200k ops/s (Medium Priority)
. *Thread efficiency*: From 1,007 to 2,000+ ops/s/thread (Medium Priority)
. *P99/P50 ratio*: From 377x to <50x for predictability (High Priority)

*Next Steps*:

. Investigate why average-time mode shows stable P99 while throughput mode spikes (both with 100 threads)
. Profile signature validation hotspots causing 10ms+ spikes
. Consider async architecture for 2x throughput gain

*Production Readiness*: The library shows good performance characteristics in average-time mode. The extreme P99 spikes appear specific to throughput mode optimization and may not impact typical production scenarios.

== JFR Load Analysis Results (July 31, 2025)

=== Executive Summary

JFR analysis confirms that P99 latency spikes are *load-induced* rather than token-related. With identical 100-thread configuration, throughput mode shows 1,290x P99/P50 ratio while average-time mode maintains stable 6x ratio.

=== Key Findings

==== 1. *Extreme P99 Spikes in Throughput Mode*

|===
|Component |P50 (Î¼s) |P99 (Î¼s) |P99/P50 Ratio |Max (Î¼s) 

|===

| *Signature Validation* | 52 | 67,066 | 1,290x | - |
| *Complete Validation* | 86 | 130,135 | 1,513x | 526,839 |

==== 2. *Stable Performance in Average-Time Mode*

|===
|Component |P50 (Î¼s) |P99 (Î¼s) |P99/P50 Ratio 

|*Signature Validation* |46 |276 |6x 
|*Complete Validation* |60 |345 |5.8x 
|===

==== 3. *Mode Comparison (100 threads)*

|===
|Metric |Throughput Mode |Average-Time Mode |Difference 

|*Throughput* |35,182 ops/s |~465 ops/s |76x 
|*Avg Latency* |2,842 Î¼s |2,147 Î¼s |1.3x 
|*P99 Signature* |67,066 Î¼s |276 Î¼s |243x 
|*P99/P50 Ratio* |1,290x |6x |215x 
|*Max Latency* |526,839 Î¼s |~500 Î¼s |1,054x 
|===

=== Root Cause Analysis

. *JMH Scheduling Difference*: Throughput mode aggressively pushes for maximum operations, creating severe contention
. *No Token Diversity Impact*: All tokens are similar, confirming spikes are purely load-related
. *JCA Bottleneck*: Signature validation through JCA shows extreme sensitivity to concurrent load
. *Coefficient of Variation*: 1,061% indicates extreme performance unpredictability under load

=== Variance Metrics

* *Average CV*: 1,057% (extremely high variance)
* *CV Range*: 729% - 1,390%
* *Max Concurrent Operations*: Limited to 1 (suggesting serialization bottleneck)

=== Implications

. *Production Impact*: Limited if using request-scoped validation (average-time pattern)
. *Batch Processing Risk*: High if using throughput-optimized patterns
. *Primary Bottleneck*: Signature validation accounts for 77% of P99 latency
. *Optimization Priority*: Signature validation caching is critical

== JFR Hotspot Analysis (July 31, 2025)

=== Key Findings

==== 1. *RSA Signature Validation CPU Bottleneck*

Component metrics clearly show signature validation as the bottleneck:

|===
|Benchmark |Component |P50 |P99 |P99/P50 Ratio 

|*measureThroughput* |signature_validation |52Î¼s |*67,066Î¼s* |1,290x 
|*measureAverageTime* |signature_validation |46Î¼s |276Î¼s |6x 
|*measureConcurrent* |signature_validation |66Î¼s |1,297Î¼s |20x 
|*validateMixedTokens50* |signature_validation |51Î¼s |*16,989Î¼s* |333x 
|===

Cache operations remain consistently fast (P99: 0.3-1.2Î¼s), confirming the issue is NOT cache-related.

==== 2. *Root Cause: CPU Saturation from RSA Operations*

JFR execution samples show:
- `RSASignature.engineVerify()` dominating CPU time
- `RSACore.rsa()` performing expensive modular exponentiation
- Multiple threads executing RSA operations concurrently

With 100 threads performing RSA signature verification:
- CPU becomes saturated
- Thread scheduling delays cause P99 spikes
- Some threads wait 67ms+ for CPU time

==== 3. *Why Throughput Mode Shows Extreme Spikes*

* *Average-time mode*: Coordinated thread execution, stable scheduling
* *Throughput mode*: Maximum pressure, thread starvation occurs
* *100 threads*: Far exceeds available CPU cores for RSA operations

==== 4. *Monitor Wait Analysis*

The 262ms monitor waits in JFR are a symptom, not the cause:
- Threads waiting for CPU to complete RSA operations
- JVM internal synchronization during high CPU contention
- Not related to application-level locks

=== Context: Token Caching Already Implemented

The system already caches validated tokens effectively:
- Token cache provides 90%+ hit rate in typical scenarios
- Cache hits are fast: 0.3-1.2Î¼s for lookup/store operations
- *Problem occurs on cache misses*: New tokens require full validation

=== The JCA Bottleneck on Cache Misses

When a token is not in cache (new token, cache eviction, or rotation):

. Full validation pipeline executes, including signature verification
. JCA's `Signature.verify()` performs expensive RSA operations
. With 100 concurrent threads hitting cache misses, CPU saturates
. P99 latency spikes to 67ms due to thread scheduling delays

=== Recommendations

. *Accept Current Performance* (Primary)
+
* 99% of operations complete in â‰¤71Î¼s (excellent)
* 1% tail latency may be acceptable for the workload
* Measure actual production impact vs synthetic benchmarks

. *Virtual Thread Tuning* (If needed)
+
* Monitor carrier thread behavior under load
* Tune Virtual Thread carrier pool size
* Consider platform thread comparison for CPU-intensive work

. *Entropy Optimization* (For startup)
+
* JVM mode: Add `-Djava.security.egd=file:/dev/./urandom`
* Native mode: Configure `--initialize-at-run-time` for SecureRandom
* Prevents blocking on entropy gathering in Docker

. *Algorithm Analysis Complete*
+
* ES256 tested and found to be slower than RS256
* Current RSA implementation is optimal for this workload
* No viable algorithm alternatives available

== JCA Performance Analysis - Final Findings (July 31, 2025)

=== The Real Problem: JCA Thread Safety with Virtual Threads

. *Signature Instances are NOT Thread-Safe*
+
* JCA `Signature.getInstance()` maintains internal state
* Cannot be shared between threads
* ThreadLocal is *not an option* with Virtual Threads
* Must create new instance for each operation

. *Performance Impact with 100 Threads*
+
* Each thread creates new Signature instance
* RSA operations are CPU-intensive
* 100 concurrent RSA validations saturate CPU
* Result: 67ms P99 spikes on cache misses

=== Secondary Issue: Entropy Blocking in Docker

. *Docker Container Entropy Starvation*
+
* Default `/dev/random` can block for 21+ seconds
* Waiting for entropy in containerized environments
* Affects SecureRandom initialization

. *Solution for JVM Mode Only*
+
----
-Djava.security.egd=file:/dev/./urandom
----
+
* Non-blocking entropy source
* The `./` syntax is required (Java quirk)
* *Note*: May not work in Quarkus native mode

=== What DOESN'T Work

. *ThreadLocal* - Incompatible with Virtual Threads
. *Alternative JCA Providers* - Break Quarkus native compilation
. *Hardware Acceleration* - Not available in Docker containers
. *Caching Optimizations* - Token cache already exists and works well

=== Viable Solutions

. *Reduce Concurrent Load*
+
* Limit concurrent signature validations with semaphore
* Queue requests to prevent CPU saturation
* Match concurrency to available CPU cores

. *For Quarkus Native Mode*
+
----
quarkus.native.additional-build-args=--initialize-at-run-time=sun.security.provider.SecureRandom
----
+
* Prevents build-time initialization of SecureRandom
* Required for proper entropy handling

. *Architectural Review*
+
* ES256 tested and found slower than current RSA implementation
* Token cache size already optimized (good hit rates)
* Monitor actual cache miss patterns vs production load

=== Critical Analysis: Why Semaphores Won't Help

*The Data Shows:*
- P50: 52Î¼s, P95: 71Î¼s, P99: 67,066Î¼s
- 99% of operations complete normally (â‰¤71Î¼s)
- Only 1% experience extreme delays (67ms = 1,000x slower)

*This is NOT CPU saturation* - it's *thread scheduling unfairness*.

*Why Semaphores Are Wrong:*

. *Same total work*: 100 RSA operations still need 100 operations worth of CPU
. *Added overhead*: Semaphore synchronization reduces performance
. *Doesn't fix root cause*: OS scheduler fairness issues remain
. *Modern CPUs*: Designed to handle thread oversubscription efficiently

=== Revised Analysis

The P99 spikes are caused by:

. *OS scheduler starvation* - Some threads get delayed while others run
. *Thread safety requirement* - New Signature instances per operation
. *Virtual Threads coordination* - Platform thread carrier scheduling
. *Entropy blocking* in Docker (secondary, affects startup)

*The issue is scheduler fairness, not CPU capacity.*