= JWT Library Benchmarks
:source-highlighter: highlight.js

Performance benchmarking for JWT token validation core library using JMH.

== Building and Running

=== Quick Start

[source,bash]
----
# Run all benchmarks
./mvnw clean verify -pl benchmarking/benchmark-library -Pbenchmark

# Quick validation run (faster)
./mvnw clean verify -pl benchmarking/benchmark-library -Pbenchmark \
  -Djmh.iterations=1 -Djmh.warmupIterations=1

# With JFR profiling for detailed analysis
./mvnw clean verify -pl benchmarking/benchmark-library -Pbenchmark-jfr
----

== Configuration

=== JMH Parameters

[source,bash]
----
-Djmh.iterations=5           # Measurement iterations (default: 5)
-Djmh.warmupIterations=3     # Warmup iterations (default: 3)
-Djmh.forks=2               # JVM forks (default: 2)
-Djmh.threads=4             # Concurrent threads (default: 4)
-Djmh.time=10s              # Time per iteration
-Djmh.includes=*Benchmark   # Benchmark pattern filter
----

=== Running Specific Benchmarks

[source,bash]
----
# Single benchmark
./mvnw clean verify -pl benchmarking/benchmark-library -Pbenchmark \
  -Djmh.includes=TokenValidatorBenchmark

# Multiple benchmarks
./mvnw clean verify -pl benchmarking/benchmark-library -Pbenchmark \
  -Djmh.includes="TokenValidator|JwksClient"
----

== Benchmarks

* `TokenValidatorBenchmark` - Core JWT validation performance
* `JwksClientBenchmark` - JWKS key retrieval performance
* `MultiIssuerValidatorBenchmark` - Multi-issuer validation
* `ErrorLoadBenchmark` - Error handling impact
* `ConcurrentTokenValidationBenchmark` - Concurrent validation
* `JfrInstrumentedBenchmark` - Detailed variance analysis (JFR profile only)

== Output

Results in `target/`:

* `benchmark-results/jmh-result.json` - Raw JMH results
* `benchmark-results/performance-<timestamp>.json` - Performance tracking data
* `benchmark-results/jfr-benchmark.jfr` - JFR recording (with -Pbenchmark-jfr)

== Performance Score

Composite metric combining:

* **Throughput (57%)** - Operations per second
* **Latency (40%)** - Response time (inverted)
* **Error Resilience (3%)** - Error handling efficiency

Score = (Throughput Ã— 0.57) + (Latency_Inverted Ã— 0.40) + (Error_Resilience Ã— 0.03)

== Analysis

=== View Results

[source,bash]
----
# Throughput summary
jq '.[] | {benchmark: .benchmark, score: .primaryMetric.score, unit: .primaryMetric.scoreUnit}' \
  target/benchmark-results/jmh-result.json

# Performance score
jq '.performanceScore' target/benchmark-results/performance-*.json
----

=== JFR Analysis

[source,bash]
----
# Analyze variance
java -cp "target/classes:target/dependency/*" \
  de.cuioss.jwt.validation.benchmark.jfr.JfrVarianceAnalyzer \
  target/benchmark-results/jfr-benchmark.jfr

# View hot methods
jfr print --events jdk.ExecutionSample \
  target/benchmark-results/jfr-benchmark.jfr
----

== Performance Tracking

=== Automatic Tracking

Each run generates:

* `performance-YYYYMMDD-HHMMSS.json` - Timestamped metrics
* Updates to `performance-tracking.json` - Last 10 runs

=== GitHub Pages

View trends at: https://cuioss.github.io/cui-jwt/benchmarks/performance-trends.html

Features:

* Interactive performance charts
* Trend indicators with percentage changes
* Performance badges (â†— improving, â†’ stable, â†˜ declining)

== Maven Dependency

For custom benchmarking:

[source,xml]
----
<dependency>
    <groupId>de.cuioss.jwt</groupId>
    <artifactId>benchmark-library</artifactId>
    <scope>test</scope>
</dependency>
----

== Documentation

For comprehensive documentation on benchmarking, analysis, and visualization:

ðŸ“š **link:../doc/README.adoc[Complete Documentation]**

=== Quick Links

* link:../doc/workflow.adoc[Benchmark Workflow] - Complete workflow guide
* link:doc/Analysis-08.2025.adoc[Performance Analysis] - Latest benchmark insights
* link:../doc/performance-scoring.adoc[Performance Scoring] - Methodology details
* link:../doc/local-testing.adoc[Local Testing] - Development setup

=== Related

* link:../benchmark-integration-quarkus/README.adoc[Integration Benchmarks]