= CUI JWT Benchmark Development Guide
:toc: left
:toclevels: 2

== Overview

The CUI JWT project includes a comprehensive benchmarking framework for measuring JWT validation performance at both library and integration levels.

== Project Structure

[cols="1,3", options="header"]
|===
|Module |Purpose

|`benchmarking/cui-benchmarking-common`
|Shared benchmark infrastructure, metrics collection, JFR integration

|`benchmarking/benchmark-library`
|Library-level microbenchmarks for JWT validation core

|`benchmarking/benchmark-integration-quarkus`
|Integration benchmarks for Quarkus REST endpoints
|===

== Running Benchmarks

[source,bash]
----
# Run all benchmarks (library + integration)
./mvnw clean verify -Pbenchmark

# Run only library benchmarks
./mvnw clean verify -Pbenchmark -pl benchmarking/benchmark-library

# Run only Quarkus integration benchmarks  
./mvnw clean verify -Pbenchmark -pl benchmarking/benchmark-integration-quarkus

# Run specific benchmark class
./mvnw clean verify -Pbenchmark -pl benchmarking/benchmark-library \
  -Djmh.includes=".*ErrorLoadBenchmark.*"
----

== Library Benchmarks (`benchmark-library`)

=== Core Benchmarks

==== `ErrorLoadBenchmark`
Tests error handling performance with various token failure scenarios:
- Expired tokens
- Malformed tokens
- Invalid signatures
- Mixed error loads (0%, 50% error rates)

==== `SimpleCoreValidationBenchmark`
Measures core validation throughput and latency for valid tokens.

==== `SimpleErrorLoadBenchmark`
Streamlined error scenario testing with reduced token sets.

=== JFR-Enabled Benchmarks

==== `UnifiedJfrBenchmark`
Combined benchmark with Java Flight Recorder integration for detailed profiling.

==== `CoreJfrBenchmark`, `ErrorJfrBenchmark`, `MixedJfrBenchmark`
Specialized JFR benchmarks for different validation scenarios.

=== Infrastructure

==== `LibraryBenchmarkRunner`
Main runner that:
- Collects all benchmark classes
- Configures JMH settings
- Produces JSON results in `target/benchmark-results/micro-result.json`
- Initializes `BenchmarkKeyCache` for JWT key management

==== `JfrBenchmarkRunner`
Alternative runner with JFR profiling enabled for deep performance analysis.

== Integration Benchmarks (`benchmark-integration-quarkus`)

=== Benchmark Classes

==== `JwtValidationBenchmark`
Tests `/jwt/validate` endpoint with valid tokens, measuring:
- Throughput (ops/sec)
- Sample time distribution

==== `JwtHealthBenchmark`
Tests health check endpoint performance under load.

=== Infrastructure

==== `AbstractIntegrationBenchmark`
Base class providing:
- HTTP client setup
- Token generation
- Request/response validation
- Quarkus server interaction

== Common Infrastructure (`cui-benchmarking-common`)

=== Core Components

==== `AbstractBenchmarkRunner`
Base runner providing:
- JMH configuration from Maven properties
- Result processing and validation
- Metrics collection
- Report generation

==== `BenchmarkConfiguration`
Configuration management for:
- Benchmark type (MICRO, MACRO, INTEGRATION)
- JMH parameters (forks, iterations, warmup)
- Output directories and formats

==== `BenchmarkMetrics`
Standardized metrics collection:
- Throughput measurements
- Latency percentiles (p50, p95, p99)
- Error rates
- JVM statistics

=== JFR Integration

==== `BenchmarkPhaseEvent`
Custom JFR events for tracking:
- Warmup phases
- Measurement iterations
- Error occurrences

== Configuration

=== Maven Profile
The `benchmark` profile in parent POM:
- Skips regular tests
- Enables benchmark execution
- Configures JMH plugin
- Sets output directories

=== JMH Parameters (via Maven)
[source,bash]
----
-Djmh.forks=2           # Number of JVM forks
-Djmh.wi=3              # Warmup iterations  
-Djmh.i=5               # Measurement iterations
-Djmh.time=1            # Time per iteration (seconds)
-Djmh.threads=1         # Thread count
----

== Output and Reports

=== Result Files
- Library: `benchmarking/benchmark-library/target/benchmark-results/micro-result.json`
- Integration: `benchmarking/benchmark-integration-quarkus/target/benchmark-results/integration-result.json`

=== Metrics Format
[source,json]
----
{
  "benchmark": "ErrorLoadBenchmark.validateExpiredToken",
  "mode": "avgt",
  "score": 145.234,
  "scoreError": 2.341,
  "scoreUnit": "ns/op",
  "params": {
    "errorRate": "50"
  }
}
----

== Adding New Benchmarks

=== Library Benchmark
1. Extend `AbstractBenchmark` or create standalone JMH benchmark
2. Place in `benchmarking/benchmark-library/src/main/java`
3. Use existing token generators from `TestTokenGenerators`
4. Benchmark will be auto-discovered by `LibraryBenchmarkRunner`

=== Integration Benchmark  
1. Extend `AbstractIntegrationBenchmark`
2. Place in `benchmarking/benchmark-integration-quarkus`
3. Use provided HTTP client and token helpers
4. Ensure Quarkus server is running during benchmark

== Best Practices

1. **Use existing infrastructure** - Don't reinvent token generation or HTTP clients
2. **Keep benchmarks focused** - Test one specific scenario per method
3. **Use realistic data** - Token sizes and claim counts should match production
4. **Document parameters** - Explain what each `@Param` value represents
5. **Validate results** - Check response codes and error handling
6. **Monitor resource usage** - Watch for memory leaks in long-running benchmarks