= Local Testing Guide
:source-highlighter: highlight.js

== Quick Start

To view benchmark results locally after running benchmarks:

=== Step 1: Run Benchmarks

[source,bash]
----
# Run micro benchmarks
./mvnw verify -pl benchmarking/benchmark-library -Pbenchmark

# Run integration benchmarks
./mvnw verify -pl benchmarking/benchmark-integration-quarkus -Pbenchmark-testing
----

=== Step 2: Serve Results Locally

The benchmarks automatically generate HTML pages and artifacts. To view them:

[source,bash]
----
cd benchmarking/scripts
./serve-local.sh
# Or specify a port:
./serve-local.sh 3000
----

Alternative using Node.js:

[source,bash]
----
cd benchmarking/scripts
node serve-local.js
# Or specify a port:
node serve-local.js 3000
----

=== Step 3: View Results

Open your browser to http://localhost:8080 to view the generated artifacts.

== Generated Artifacts Location

After running benchmarks, artifacts are generated in:

* **Micro Benchmarks**: `benchmarking/benchmark-library/target/benchmark-results/`
* **Integration Benchmarks**: `benchmarking/benchmark-integration-quarkus/target/benchmark-results/`

Each location contains:
- `index.html` - Main results page
- `trends.html` - Historical trends visualization
- `badges/` - Performance badges
- `data/` - Metrics JSON files
- `gh-pages-ready/` - GitHub Pages deployment structure

== Manual HTTP Server Options

You can also serve the results directly from the benchmark directories:

[source,bash]
----
# For micro benchmark results
cd benchmarking/benchmark-library/target/benchmark-results
python3 -m http.server 8080

# For integration benchmark results
cd benchmarking/benchmark-integration-quarkus/target/benchmark-results
python3 -m http.server 8081
----

Alternative servers:

[source,bash]
----
# Node.js (requires http-server)
npx http-server -p 8080

# PHP
php -S localhost:8080

# Ruby
ruby -run -ehttpd . -p8080
----

== Why HTTP Server?

Modern browsers enforce strict security policies that prevent JavaScript from loading local files directly (CORS policy). An HTTP server provides the proper protocol and headers for the templates to function correctly.

== Viewing Specific Components

The generated artifacts include:

* **Performance Badges** - View in `badges/` directory
* **Metrics Data** - JSON files in `data/` directory  
* **Summary Report** - `benchmark-summary.json` with quality gates
* **Raw Results** - JMH output in `*-benchmark-result.json`

== Troubleshooting

=== No Results Generated

- Ensure benchmarks ran successfully
- Check for compilation errors
- Verify the correct Maven profile was used

=== "Failed to fetch" Errors

- Make sure you're accessing via `http://localhost:8080`, not `file://`
- Check that the HTTP server is running

=== 404 Errors

- Verify benchmark results were generated
- Check the correct directory is being served
- Ensure artifacts exist in `target/benchmark-results/`

=== Port Already in Use

- Try a different port number
- Check for other running servers: `lsof -i :8080`

== Development Tips

For rapid iteration during development:

[source,bash]
----
# Quick benchmark run (1 iteration only)
./mvnw verify -pl benchmarking/benchmark-library -Pbenchmark \
  -Djmh.iterations=1 -Djmh.warmupIterations=1

# View results immediately
cd benchmarking/benchmark-library/target/benchmark-results && \
  python3 -m http.server 8080
----