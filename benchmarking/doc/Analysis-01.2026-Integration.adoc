= JWT Validation Performance Analysis
:toc: left
:toclevels: 3
:toc-title: Table of Contents
:sectnums:
:source-highlighter: highlight.js


== Executive Summary

Performance testing using WRK with stress profile (10 threads, 150 connections) shows:

* **JWT Validation (cache enabled, size 20)**: 23,800 ops/s (strong performance)
* **Health Check**: 88,700 ops/s (excellent scaling)
* **Target Achievement**: 48% of minimum target (50k ops/s)
* **Error Rate**: 0% (no timeouts)

== WRK Stress Profile Results

=== Configuration

* **Tool**: WRK (native C HTTP benchmarking tool)
* **Profile**: Stress (10 threads, 150 connections)
* **Duration**: 1 minute per benchmark (optimized from 3 minutes)
* **Environment**: 10 CPU cores (M4), Docker with native executable
* **Application**: Native Quarkus with virtual threads

=== Performance Results

[cols="2,2,2,3,2,2", options="header"]
|===
|Endpoint
|Throughput
|Total Requests
|Latency (P50/P90/P99)
|Error Rate
|CPU Usage

|JWT Validation (cache ON, 20)
|23,800 ops/s
|1.43M
|5.49ms / 15.73ms / 32.59ms
|0 timeouts (0%)
|85.0% peak (79.7% avg)

|Health Check
|88,700 ops/s
|5.32M
|1.25ms / 4.86ms / 12.06ms
|0 timeouts (0%)
|72.8% peak (67.3% avg)
|===

NOTE: Previous benchmarks (October 2025) included a separate cache-OFF run. The January 2026 benchmark suite tests JWT with cache enabled only, as cache-OFF is not a production configuration.

=== Resource Utilization

* **CPU**: JWT (cache ON): 85.0% peak (79.7% avg) | Health: 72.8% peak (67.3% avg)
* **Memory**: Heap peak 126.0MB (JWT) | GC overhead: 0%
* **Threads**: JWT: 135 peak (114 avg) | Health: 41 peak (35 avg)
* **Cache**: 266,649 hits, 100% hit rate
* **GC**: Zero overhead (native compilation)

== Performance Analysis

=== Connection Count Investigation Results

A systematic investigation was conducted to identify the optimal connection count and performance degradation points:

[cols="1,2,1,2,1,1,1", options="header"]
|===
|Connections
|Health P50/P90/P99 (ms)
|Health Throughput
|JWT P50/P90/P99 (ms)
|JWT Throughput
|JWT CPU Peak
|JWT Threads (peak)

|50 (Default)
|0.426 / 1.98 / 7.89
|89.9K ops/s
|1.93 / 4.47 / 12.59
|22.7K ops/s
|81.5%
|81

|100
|0.77 / 3.28 / 9.15
|95.4K ops/s
|3.56 / 9.97 / 22.81
|24.3K ops/s
|85.0%
|127

|150 (Stress)
|1.25 / 4.86 / 12.06
|88.7K ops/s
|5.49 / 15.73 / 32.59
|23.8K ops/s
|85.0%
|135

|200
|1.70 / 6.39 / 14.55
|86.9K ops/s
|7.66 / 21.18 / 42.48
|22.7K ops/s
|84.3%
|149

|250
|2.04 / 7.73 / 16.62
|89.4K ops/s
|9.51 / 25.15 / 49.00
|23.3K ops/s
|83.6%
|184

|300 (Max)
|2.50 / 9.22 / 19.13
|87.9K ops/s
|11.55 / 28.91 / 56.13
|23.0K ops/s
|82.5%
|232
|===

**Key Findings:**

* **100 connections** achieves peak health throughput (95.4K ops/s) - optimal balance point
* **JWT performance** remains remarkably stable (22.7-24.3K ops/s) across all connection counts
* **50 connections** provides excellent baseline (89.9K ops/s health, 22.7K ops/s JWT) for CI/CD
* **150-250 connections** maintains consistent 87-89K ops/s health throughput with stable JWT performance
* **300 connections** shows slight degradation (87.9K ops/s health) but JWT remains stable at 23.0K ops/s
* **Cache effectiveness** confirmed with 100% hit rate across all runs (cache size 20)

=== Health Endpoint as Control Experiment

The health endpoint (`/q/health/live`) and JWT endpoint (`/api/v1/jwt/extract`) share the entire infrastructure stack: Docker bridge networking, TLS termination, HTTP/2, Quarkus routing, and CDI container. The health endpoint therefore serves as a **control experiment** — any performance difference between the two endpoints must be caused by endpoint-specific processing, not shared infrastructure.

==== Constant Latency Ratio

The JWT/Health P50 latency ratio remains remarkably constant across all connection levels:

[cols="1,2,2,1", options="header"]
|===
|Connections
|Health P50 (ms)
|JWT P50 (ms)
|JWT/Health Ratio

|50
|0.426
|1.93
|**4.53x**

|100
|0.77
|3.56
|**4.62x**

|150
|1.25
|5.49
|**4.39x**

|200
|1.70
|7.66
|**4.51x**

|250
|2.04
|9.51
|**4.66x**

|300
|2.50
|11.55
|**4.62x**
|===

**Mean ratio: 4.55x** (standard deviation: 0.09)

This constant ratio is the most significant finding of the connection sweep. It means:

1. **Shared infrastructure scales identically** for both endpoints — latency increases linearly with connection count, but the multiplicative gap stays fixed
2. **JWT-specific overhead is a constant multiplier**, not an additive cost that grows under load
3. **No JWT-specific bottleneck emerges at high concurrency** — the validation pipeline scales as well as the health endpoint

==== Observed Characteristics

1. **Token cache effectiveness** — Cache size of 20 achieved 100% hit rate (266,649/266,649 validations at 150 conns)
2. **Library validation time** — 77µs (0.077ms) P50 from JMH micro-benchmark
3. **JWKS refresh** — Periodic Keycloak communication for public key updates

=== Latency Decomposition (Measured with Mock Endpoint)

A **mock JWT endpoint** (`/mock-jwt/validate`) was created to directly measure the latency decomposition. The mock endpoint replicates the JWT endpoint's full HTTP processing path — CDI injection, `HttpServletRequestResolver.resolveHeaderMap()`, `Authorization` header extraction, and `ValidationResponse` JSON serialization — but **skips the actual JWT library validation** (`tokenValidator.createAccessToken()`). This turns the inferred decomposition into a measured one.

==== Mock Endpoint Benchmark Results (50 connections)

[cols="2,2,2,2,2", options="header"]
|===
|Endpoint
|P50 (ms)
|P90 (ms)
|P99 (ms)
|Throughput

|Health (`/q/health/live`)
|0.416
|2.25
|7.78
|87,400 ops/s

|**Mock JWT** (`/mock-jwt/validate`)
|**0.833**
|**3.14**
|**9.04**
|**45,877 ops/s**

|JWT (`/jwt/validate`)
|1.95
|4.49
|12.26
|22,454 ops/s
|===

==== Three-Layer Decomposition

[cols="3,2,2", options="header"]
|===
|Component
|Latency (P50)
|Percentage

|**Shared infrastructure** (Docker, TLS, HTTP/2, Quarkus routing) — measured by health endpoint
|0.416ms
|21%

|**Header extraction + response serialization** (`resolveHeaderMap()`, `Authorization` parsing, `HashMap` construction, `ValidationResponse` JSON) — measured by mock endpoint minus health
|0.417ms
|21%

|**CDI producer chain + JWT validation** (`Instance.get()`, producer resolution, `createAccessToken()`, claims checking) — measured by JWT minus mock endpoint
|1.117ms
|**57%**
|===

**Total JWT P50**: 1.95ms = 0.416ms (shared) + 0.417ms (serialization) + 1.117ms (CDI + validation)

==== Key Insight: JMH vs Integration Validation Cost

The JMH micro-benchmark measures isolated JWT library validation at 0.077ms (77µs). The mock endpoint experiment reveals the actual cost difference between JWT and mock is 1.117ms — **14.5x more** than the isolated library. This gap includes:

* **CDI `Instance<BearerTokenResult>` proxy resolution** — CDI creates and invokes the producer per request
* **`InjectionPoint` metadata extraction** — CDI inspects `@BearerToken` annotation parameters
* **`@Timed` micrometer instrumentation** — metrics recording overhead on every validation
* **`tokenValidator.createAccessToken()` under concurrency** — 50 concurrent connections competing for the lock-free cache, CPU cache line invalidation, memory barriers
* **`AccessTokenContent` construction** — real object allocation from parsed claims (vs JMH reusing pre-parsed data)

The previous (inferred) decomposition attributed 74% to "CDI/serialization" and only 4% to the JWT library. The measured data reverses this: header extraction and serialization cost only 21% (0.417ms), while the CDI producer chain combined with JWT validation accounts for 57% (1.117ms).

==== Scaling Observation

At higher connection counts, the shared infrastructure cost grows (queuing), but the multiplicative ratios stay stable. The 4.55x JWT/Health ratio from the connection sweep (see <<Constant Latency Ratio>>) is consistent with the 2.0x Mock/Health ratio and 2.3x JWT/Mock ratio observed at 50 connections (2.0 × 2.3 ≈ 4.7x).

=== Performance Ceiling

* **Health check capacity**: 95.4K ops/s at 100 connections (peak performance)
* **JWT validation capacity**: 24.3K ops/s at 100 connections (cache enabled, 100% hit rate)
* **Performance gap**: 71.1K ops/s between health and JWT endpoints at peak
* **Stability range**: JWT maintains 22.7-24.3K ops/s across 50-300 connections (excellent stability)

**Throughput gap explanation:**

The ~3.9x throughput difference (95K health vs 24K JWT at 100 conns) is fully explained by the ~4.6x per-request latency ratio at that connection level (0.77ms health vs 3.56ms JWT P50). With a fixed number of connections, throughput is inversely proportional to per-request latency — if each JWT request takes 4.6x longer to process, throughput drops by approximately that factor. The slight difference between the throughput ratio (3.9x) and the latency ratio (4.6x) is accounted for by JWT's higher thread count (127 peak vs health baseline) and correspondingly higher CPU usage (85.0% peak vs health baseline), which partially compensates for the longer per-request time.

The measured latency decomposition (see <<Latency Decomposition (Measured with Mock Endpoint)>>) shows JWT-specific overhead at 50 connections breaks into: 0.417ms for header extraction and response serialization, plus 1.117ms for the CDI producer chain and JWT validation — none of which the health endpoint requires.

=== Endpoint Implementation Comparison

Both endpoints use CDI injection and beans. CDI itself is NOT the differentiator.

[cols="2,2,2,2", options="header"]
|===
|Factor
|Health (`/q/health/live`)
|JWT (`/jwt/validate`)
|Notes

|Request-scoped producers
|None
|Yes (`Instance<BearerTokenResult>`)
|Observable from code

|Producer invocation
|None
|`basicToken.get()` per request
|Observable from code

|Response complexity
|Simple status object
|Nested maps with collections
|Observable from code

|Business logic
|Check if list is empty
|Extract claims, build map, authorization checks
|Observable from code

|JWT validation
|None
|Core library validates in 77µs (micro-benchmark)
|From JMH data
|===

=== Per-Thread and Per-CPU Efficiency (50 connections)

[cols="2,2,2,2", options="header"]
|===
|Endpoint
|Throughput
|Threads (avg)
|ops/s per thread

|Health
|89,900 ops/s
|35
|**2,569**

|JWT
|22,700 ops/s
|74
|**307**
|===

JWT endpoint has 8.4x worse per-thread efficiency, expected given its longer processing time (1.93ms P50 vs 0.426ms P50 for health).

[cols="2,2,2,2", options="header"]
|===
|Endpoint
|Throughput
|CPU (peak)
|ops/s per 1% CPU

|Health
|89,900 ops/s
|75.6%
|**1,189**

|JWT
|22,700 ops/s
|81.5%
|**279**
|===

JWT endpoint is 4.3x less CPU-efficient. Since both endpoints share the same HTTP/Quarkus/CDI infrastructure, the difference is attributable to JWT-specific processing: cryptographic signature verification, token claims extraction, request-scoped producers, and complex JSON response serialization.

== Conclusion

Comprehensive WRK stress testing across 50-300 connections reveals:

* **Peak performance**: 95.4K ops/s health (100 conns), 24.3K ops/s JWT (100 conns)
* **Excellent stability**: JWT maintains 22.7-24.3K ops/s across all connection counts (50-300)
* **Optimal configuration**: 100 connections provides best balance (95.4K health, 24.3K JWT)
* **Constant latency ratio**: JWT/Health P50 ratio stays at 4.55x (SD 0.09) across all connection levels — JWT-specific overhead is a fixed multiplier, not a load-dependent bottleneck
* **Measured latency decomposition** (at 50 conns, using mock endpoint): 21% shared infrastructure (0.416ms), 21% header extraction + response serialization (0.417ms), 57% CDI producer chain + JWT validation (1.117ms)
* **Throughput gap explained**: The ~3.9x throughput difference is fully accounted for by the ~4.5x per-request latency ratio
* **Cache effectiveness**: Lock-free cache achieves 100% hit rate (size 20), zero performance collapse
* **JMH vs integration gap**: Core library validates in 77µs (JMH isolation), but integration validation costs 1.117ms — 14.5x more due to CDI producer resolution, `@Timed` instrumentation, concurrent cache contention, and real object allocation
* **Dominant cost**: CDI producer chain combined with JWT validation accounts for 57% (1.117ms) of request latency — the primary optimization target. Header extraction and serialization add another 21% (0.417ms)