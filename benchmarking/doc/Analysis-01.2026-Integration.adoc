= JWT Validation Performance Analysis
:toc: left
:toclevels: 3
:toc-title: Table of Contents
:sectnums:
:source-highlighter: highlight.js


== Executive Summary

Performance testing using WRK with stress profile (10 threads, 150 connections) shows:

* **JWT Validation (cache enabled, size 20)**: 23,800 ops/s (strong performance)
* **Health Check**: 88,700 ops/s (excellent scaling)
* **Target Achievement**: 48% of minimum target (50k ops/s)
* **Error Rate**: 0% (no timeouts)

== WRK Stress Profile Results

=== Configuration

* **Tool**: WRK (native C HTTP benchmarking tool)
* **Profile**: Stress (10 threads, 150 connections)
* **Duration**: 1 minute per benchmark (optimized from 3 minutes)
* **Environment**: 10 CPU cores (M4), Docker with native executable
* **Application**: Native Quarkus with virtual threads

=== Performance Results

[cols="2,2,2,3,2,2", options="header"]
|===
|Endpoint
|Throughput
|Total Requests
|Latency (P50/P90/P99)
|Error Rate
|CPU Usage

|JWT Validation (cache ON, 20)
|23,800 ops/s
|1.43M
|5.49ms / 15.73ms / 32.59ms
|0 timeouts (0%)
|85.0% peak (79.7% avg)

|Health Check
|88,700 ops/s
|5.32M
|1.25ms / 4.86ms / 12.06ms
|0 timeouts (0%)
|72.8% peak (67.3% avg)
|===

NOTE: Previous benchmarks (October 2025) included a separate cache-OFF run. The January 2026 benchmark suite tests JWT with cache enabled only, as cache-OFF is not a production configuration.

=== Resource Utilization

* **CPU**: JWT (cache ON): 85.0% peak (79.7% avg) | Health: 72.8% peak (67.3% avg)
* **Memory**: Heap peak 126.0MB (JWT) | GC overhead: 0%
* **Threads**: JWT: 135 peak (114 avg) | Health: 41 peak (35 avg)
* **Cache**: 266,649 hits, 100% hit rate
* **GC**: Zero overhead (native compilation)

== Performance Analysis

=== Connection Count Investigation Results

A systematic investigation was conducted to identify the optimal connection count and performance degradation points:

[cols="1,2,1,2,1,1,1", options="header"]
|===
|Connections
|Health P50/P90/P99 (ms)
|Health Throughput
|JWT P50/P90/P99 (ms)
|JWT Throughput
|JWT CPU Peak
|JWT Threads (peak)

|50 (Default)
|0.426 / 1.98 / 7.89
|89.9K ops/s
|1.93 / 4.47 / 12.59
|22.7K ops/s
|81.5%
|81

|100
|0.77 / 3.28 / 9.15
|95.4K ops/s
|3.56 / 9.97 / 22.81
|24.3K ops/s
|85.0%
|127

|150 (Stress)
|1.25 / 4.86 / 12.06
|88.7K ops/s
|5.49 / 15.73 / 32.59
|23.8K ops/s
|85.0%
|135

|200
|1.70 / 6.39 / 14.55
|86.9K ops/s
|7.66 / 21.18 / 42.48
|22.7K ops/s
|84.3%
|149

|250
|2.04 / 7.73 / 16.62
|89.4K ops/s
|9.51 / 25.15 / 49.00
|23.3K ops/s
|83.6%
|184

|300 (Max)
|2.50 / 9.22 / 19.13
|87.9K ops/s
|11.55 / 28.91 / 56.13
|23.0K ops/s
|82.5%
|232
|===

**Key Findings:**

* **100 connections** achieves peak health throughput (95.4K ops/s) - optimal balance point
* **JWT performance** remains remarkably stable (22.7-24.3K ops/s) across all connection counts
* **50 connections** provides excellent baseline (89.9K ops/s health, 22.7K ops/s JWT) for CI/CD
* **150-250 connections** maintains consistent 87-89K ops/s health throughput with stable JWT performance
* **300 connections** shows slight degradation (87.9K ops/s health) but JWT remains stable at 23.0K ops/s
* **Cache effectiveness** confirmed with 100% hit rate across all runs (cache size 20)

=== Health Endpoint as Control Experiment

The health endpoint (`/q/health/live`) and JWT endpoint (`/api/v1/jwt/extract`) share the entire infrastructure stack: Docker bridge networking, TLS termination, HTTP/2, Quarkus routing, and CDI container. The health endpoint therefore serves as a **control experiment** — any performance difference between the two endpoints must be caused by endpoint-specific processing, not shared infrastructure.

==== Constant Latency Ratio

The JWT/Health P50 latency ratio remains remarkably constant across all connection levels:

[cols="1,2,2,1", options="header"]
|===
|Connections
|Health P50 (ms)
|JWT P50 (ms)
|JWT/Health Ratio

|50
|0.426
|1.93
|**4.53x**

|100
|0.77
|3.56
|**4.62x**

|150
|1.25
|5.49
|**4.39x**

|200
|1.70
|7.66
|**4.51x**

|250
|2.04
|9.51
|**4.66x**

|300
|2.50
|11.55
|**4.62x**
|===

**Mean ratio: 4.55x** (standard deviation: 0.09)

This constant ratio is the most significant finding of the connection sweep. It means:

1. **Shared infrastructure scales identically** for both endpoints — latency increases linearly with connection count, but the multiplicative gap stays fixed
2. **JWT-specific overhead is a constant multiplier**, not an additive cost that grows under load
3. **No JWT-specific bottleneck emerges at high concurrency** — the validation pipeline scales as well as the health endpoint

==== Observed Characteristics

1. **Token cache effectiveness** — Cache size of 20 achieved 100% hit rate (266,649/266,649 validations at 150 conns)
2. **Library validation time** — 77µs (0.077ms) P50 from JMH micro-benchmark
3. **JWKS refresh** — Periodic Keycloak communication for public key updates

=== Latency Decomposition

Using the health endpoint as a baseline, the JWT request latency can be decomposed into three layers. The 50-connection level provides the cleanest measurement (lowest queuing contention):

[cols="3,2,2", options="header"]
|===
|Component
|Latency (P50)
|Percentage

|**Shared infrastructure** (Docker, TLS, HTTP/2, Quarkus routing) — measured by health endpoint
|0.426ms
|22%

|**JWT library validation** (signature verification, claims parsing) — measured by JMH
|0.077ms
|4%

|**JWT integration-specific overhead** (token extraction, request-scoped producers, claims extraction, response serialization)
|1.427ms
|74%
|===

**Total JWT P50**: 1.93ms = 0.426ms (shared) + 0.077ms (library) + 1.427ms (JWT integration)

Key observations:

* The shared infrastructure cost (0.426ms) is measured directly by the health endpoint — it includes Docker bridge networking, TLS, HTTP/2, Quarkus routing, and CDI container overhead
* The JWT library itself (signature verification + claims parsing) contributes only **4% of total latency** — it is not a bottleneck
* The dominant cost (74%) is JWT integration-specific processing: request-scoped CDI producers (`Instance<BearerTokenResult>`), HTTP Authorization header extraction, claims map construction, and complex JSON response serialization
* At higher connection counts, the shared infrastructure cost grows (queuing), but the 4.55x ratio stays constant — meaning the JWT-specific overhead scales proportionally

=== Performance Ceiling

* **Health check capacity**: 95.4K ops/s at 100 connections (peak performance)
* **JWT validation capacity**: 24.3K ops/s at 100 connections (cache enabled, 100% hit rate)
* **Performance gap**: 71.1K ops/s between health and JWT endpoints at peak
* **Stability range**: JWT maintains 22.7-24.3K ops/s across 50-300 connections (excellent stability)

**Throughput gap explanation:**

The ~3.9x throughput difference (95K health vs 24K JWT at 100 conns) is fully explained by the ~4.6x per-request latency ratio at that connection level (0.77ms health vs 3.56ms JWT P50). With a fixed number of connections, throughput is inversely proportional to per-request latency — if each JWT request takes 4.6x longer to process, throughput drops by approximately that factor. The slight difference between the throughput ratio (3.9x) and the latency ratio (4.6x) is accounted for by JWT's higher thread count (127 peak vs health baseline) and correspondingly higher CPU usage (85.0% peak vs health baseline), which partially compensates for the longer per-request time.

The latency decomposition (see <<Latency Decomposition>>) shows the 1.5ms JWT-specific overhead at 50 connections comes from: request-scoped CDI producers, HTTP Authorization header extraction, claims map construction, and complex JSON response serialization — none of which the health endpoint requires.

=== Endpoint Implementation Comparison

Both endpoints use CDI injection and beans. CDI itself is NOT the differentiator.

[cols="2,2,2,2", options="header"]
|===
|Factor
|Health (`/q/health/live`)
|JWT (`/jwt/validate`)
|Notes

|Request-scoped producers
|None
|Yes (`Instance<BearerTokenResult>`)
|Observable from code

|Producer invocation
|None
|`basicToken.get()` per request
|Observable from code

|Response complexity
|Simple status object
|Nested maps with collections
|Observable from code

|Business logic
|Check if list is empty
|Extract claims, build map, authorization checks
|Observable from code

|JWT validation
|None
|Core library validates in 77µs (micro-benchmark)
|From JMH data
|===

=== Per-Thread and Per-CPU Efficiency (50 connections)

[cols="2,2,2,2", options="header"]
|===
|Endpoint
|Throughput
|Threads (avg)
|ops/s per thread

|Health
|89,900 ops/s
|35
|**2,569**

|JWT
|22,700 ops/s
|74
|**307**
|===

JWT endpoint has 8.4x worse per-thread efficiency, expected given its longer processing time (1.93ms P50 vs 0.426ms P50 for health).

[cols="2,2,2,2", options="header"]
|===
|Endpoint
|Throughput
|CPU (peak)
|ops/s per 1% CPU

|Health
|89,900 ops/s
|75.6%
|**1,189**

|JWT
|22,700 ops/s
|81.5%
|**279**
|===

JWT endpoint is 4.3x less CPU-efficient. Since both endpoints share the same HTTP/Quarkus/CDI infrastructure, the difference is attributable to JWT-specific processing: cryptographic signature verification, token claims extraction, request-scoped producers, and complex JSON response serialization.

== Conclusion

Comprehensive WRK stress testing across 50-300 connections reveals:

* **Peak performance**: 95.4K ops/s health (100 conns), 24.3K ops/s JWT (100 conns)
* **Excellent stability**: JWT maintains 22.7-24.3K ops/s across all connection counts (50-300)
* **Optimal configuration**: 100 connections provides best balance (95.4K health, 24.3K JWT)
* **Constant latency ratio**: JWT/Health P50 ratio stays at 4.55x (SD 0.09) across all connection levels — JWT-specific overhead is a fixed multiplier, not a load-dependent bottleneck
* **Latency decomposition** (at 50 conns): 22% shared infrastructure (0.426ms), 4% JWT library validation (0.077ms), 74% JWT integration-specific processing (1.427ms)
* **Throughput gap explained**: The ~3.9x throughput difference is fully accounted for by the ~4.5x per-request latency ratio
* **Cache effectiveness**: Lock-free cache achieves 100% hit rate (size 20), zero performance collapse
* **Library performance**: Core library validates in 77µs (0.077ms) at 159,100 ops/s — contributes only 4% of integration latency, not a bottleneck
* **Dominant cost**: JWT integration-specific processing (request-scoped producers, token extraction, claims construction, JSON serialization) accounts for 74% of request latency — the only area where optimization would meaningfully improve throughput