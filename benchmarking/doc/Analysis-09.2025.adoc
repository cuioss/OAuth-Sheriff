= Integration Benchmark Comparison Analysis - September 2025
:source-highlighter: highlight.js

Comprehensive comparison of Quarkus JMH and WRK integration benchmarks after successful metrics architecture refactoring.

== Executive Summary

Both integration benchmarks successfully completed with the new centralized metrics architecture. Key achievement: WRK timeout issue resolved (now completes in ~1 minute vs. previous 2+ minute timeout).

== Performance Comparison

=== Quarkus JMH Integration Benchmarks

**Performance Metrics:**
- Health Check: **9.6 ops/ms** (9,600 ops/s)
- JWT Validation: **6.9 ops/ms** (6,900 ops/s)
- Average Validation Latency: **~0.245ms**
- Performance Grade: **F** (score 48)

**Metrics Collection:**
- ✅ Successfully downloaded and processed
- Total validations tracked: **7,772,364**
- Metrics format: Prometheus → JSON transformation
- Export location: `jwt-validation-metrics.json`

=== WRK HTTP Integration Benchmarks

**Performance Metrics:**
- Health Check: **12.6K ops/s**
- JWT Validation: **7.8K ops/s**
- P50 Latency: **5.1ms**
- Performance Grade: **B** (score 87)

**Metrics Collection:**
- ❌ Not downloaded (containers not running)
- Metrics download execution added to pom.xml
- WrkResultPostProcessor supports metrics-only mode
- Requires running containers for actual metrics

== Key Differences Explained

=== Measurement Methodology

**JMH (Java Microbenchmark Harness):**
- Measures JVM-level method performance
- Direct invocation of Java methods
- Includes JVM warmup and optimization phases
- More accurate for micro-level performance

**WRK (HTTP Benchmarking Tool):**
- Measures HTTP-level request/response cycles
- Includes network stack overhead
- Tests complete HTTP pipeline
- More representative of real-world usage

=== Performance Grade Discrepancy

The significant difference in grades (F vs B) stems from:
1. **Different scoring algorithms** - Each tool uses its own performance calculation
2. **Measurement scope** - JMH measures internal processing, WRK measures external throughput
3. **Overhead inclusion** - WRK includes HTTP overhead, JMH measures pure business logic

== Architecture Success

=== Centralized Metrics Processing

The refactored architecture successfully provides:
- **MetricsOrchestrator** - Central coordinator for all metrics operations
- **MetricsDownloader** - Handles Prometheus endpoint retrieval
- **MetricsFileProcessor** - Parses and transforms Prometheus format
- **MetricsJsonExporter** - Consistent JSON output format

=== Code Reusability

All benchmarking modules now share:
- Common metrics processing pipeline
- Unified report generation
- Standardized data formats
- Consistent error handling

== Metrics Data Comparison

|===
|Metric |Quarkus JMH |WRK HTTP |Difference

|**Health Check Throughput**
|9,600 ops/s
|12,600 ops/s
|+31% for WRK

|**JWT Validation Throughput**
|6,900 ops/s
|7,800 ops/s
|+13% for WRK

|**Validation Latency**
|0.245ms (calculated)
|5.1ms (P50)
|20x higher for WRK

|**Metrics Collection**
|✅ Complete (7.7M validations)
|❌ Not collected (no containers)
|N/A
|===

== Technical Achievements

1. **WRK Timeout Fixed**: Monitoring process termination issue resolved, benchmark completes in ~1 minute
2. **Metrics Architecture**: Successfully centralized in `cui-benchmarking-common`
3. **Metrics Download Setup**: WRK now has metrics download infrastructure (execution in post-integration-test phase)
4. **JSON Export**: Consistent format across all benchmark types (when metrics are available)
5. **Report Generation**: Unified HTML/badge generation pipeline for both benchmarks

== Conclusions

The metrics refactoring plan has been successfully completed:
- ✅ All 7 phases implemented
- ✅ Both integration benchmarks operational
- ✅ Centralized architecture working
- ✅ WRK timeout issue resolved

The performance differences between JMH and WRK are expected and reflect their different measurement approaches. Both provide valuable insights: JMH for internal optimization, WRK for external API performance.