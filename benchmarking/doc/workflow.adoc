= Benchmark Workflow
:source-highlighter: highlight.js

Complete workflow guide for running and processing benchmarks.

== CI/CD Pipeline

The benchmark processing is automatically triggered in GitHub Actions:

=== Triggers

* **Pull Request Merge** - Runs on merged PRs to main branch
* **Tag Push** - Runs on all tag pushes
* **Manual Dispatch** - Can be triggered manually

=== Workflow Steps

1. **Environment Setup**
   - Checkout code with full history
   - Set up JDK 21 with Maven cache
   - Build cui-jwt-validation module

2. **Micro Benchmarks**
   - Run JMH benchmarks using `benchmark` profile
   - Automatically generates artifacts in `target/benchmark-results`
   - Creates performance badges, reports, and metrics

3. **Integration Benchmarks**
   - Build native Quarkus image
   - Run integration tests with real services
   - Automatically generates integration-specific artifacts

4. **Artifact Combination**
   - Merge artifacts from both benchmark modules
   - Create unified GitHub Pages structure
   - Add metadata and timestamps

5. **Deployment**
   - Deploy combined results to cuioss.github.io
   - Results available at `cui-jwt/benchmarks`

== Local Development

=== Running Benchmarks Locally

[source,bash]
----
# Full micro benchmark suite
./mvnw clean verify -pl benchmarking/benchmark-library -Pbenchmark

# Quick test (1 iteration)
./mvnw clean verify -pl benchmarking/benchmark-library -Pbenchmark \
  -Djmh.iterations=1 -Djmh.warmupIterations=1

# With profiling
./mvnw clean verify -pl benchmarking/benchmark-library -Pbenchmark-jfr

# Integration benchmarks
./mvnw clean verify -pl benchmarking/benchmark-integration-quarkus -Pbenchmark-testing
----

=== Viewing Generated Artifacts

After running benchmarks, artifacts are automatically generated:

[source,bash]
----
# View micro benchmark artifacts
ls -la benchmarking/benchmark-library/target/benchmark-results/

# View integration benchmark artifacts  
ls -la benchmarking/benchmark-integration-quarkus/target/benchmark-results/

# Serve artifacts locally for viewing
cd benchmarking/scripts
./serve-local.sh
# Open http://localhost:8080
----

== Generated Artifacts

=== Structure

Each benchmark module generates a consistent artifact structure:

[source]
----
target/benchmark-results/
├── badges/                      # Shields.io compatible badges
│   ├── performance-badge.json   # Overall performance score
│   ├── trend-badge.json         # Trend indicator
│   └── last-run-badge.json      # Timestamp badge
├── data/                        # Metrics data
│   ├── metrics.json            # Combined metrics
│   └── *-metrics.json          # Individual benchmark metrics
├── gh-pages-ready/             # GitHub Pages deployment structure
│   ├── index.html              # Main landing page
│   ├── trends.html             # Historical trends
│   ├── api/                    # JSON API endpoints
│   └── badges/                 # Badge files
├── benchmark-summary.json       # Overall summary with quality gates
└── *-benchmark-result.json     # Raw JMH results
----

=== Badge Types

* **Performance Score** - Weighted composite score (0-100)
* **Trend** - Up/down/stable indicator with percentage change
* **Last Run** - Timestamp of most recent benchmark
* **Integration Performance** - Integration-specific score

=== Quality Gates

Each benchmark run evaluates:

* **Throughput** - Minimum operations per second
* **Latency** - Maximum response times (P50, P90, P99)
* **Regression** - Performance change from baseline
* **Overall Status** - PASS/FAIL based on all gates

== API Endpoints

The generated artifacts include JSON API endpoints:

* `api/latest.json` - Latest benchmark results
* `api/metrics.json` - Detailed metrics breakdown
* `api/status.json` - Current quality gate status
* `api/benchmarks.json` - List of all benchmarks

== Performance Requirements

See link:performance-requirements.adoc[Performance Requirements] for specific targets and thresholds.

== Troubleshooting

=== No Artifacts Generated

Check that:
- Benchmarks completed successfully
- No compilation errors in cui-benchmarking-common
- Correct profile used (`-Pbenchmark` or `-Pbenchmark-testing`)

=== Quality Gates Failing

Review:
- Performance thresholds in `SummaryGenerator`
- Baseline comparison data availability
- Resource constraints during benchmark run

=== Local Viewing Issues

Ensure:
- Node.js installed for `serve-local.sh`
- Port 8080 available
- Generated artifacts exist in expected locations