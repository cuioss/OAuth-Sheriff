= Real-Time Metrics Collection System
:toc:
:toc-placement: preamble

== Overview

The metrics system captures real-time CPU, memory, and application metrics from the Quarkus application during benchmark execution. This ensures accurate performance data by monitoring the application under test through Prometheus time-series data collection.

== Problem Statement

Previous approaches captured metrics AFTER benchmarks completed, showing idle CPU (~2.5%). Additionally, WRK's system-metrics.log measured the load generator's CPU usage (600-800%), not the application being tested.

== Solution Architecture

=== Components

[source]
----
┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐
│                 │     │                  │     │                 │
│   WRK/JMH       │────▶│  Quarkus App     │◀────│   Prometheus    │
│  (Load Gen)     │     │  :10443/jwt      │     │   :9090         │
│                 │     │  :10443/q/metrics│     │  (Scraper)      │
└─────────────────┘     └──────────────────┘     └─────────────────┘
        │                                                  │
        │                                                  │
        ▼                                                  ▼
┌─────────────────┐                          ┌─────────────────────┐
│  Benchmark      │                          │  Time-Series        │
│  Results        │                          │  Metrics Data       │
└─────────────────┘                          └─────────────────────┘
        │                                                  │
        └──────────────────┬───────────────────────────────┘
                           ▼
                ┌──────────────────────┐
                │  MetricsOrchestrator │
                │  + Transformer       │
                └──────────────────────┘
                           │
                           ▼
                ┌──────────────────────┐
                │  Server Metrics JSON │
                │  (Unified Format)    │
                └──────────────────────┘
----

=== Data Flow

[source]
----
1. Start Prometheus (docker-compose)
     │
     ▼
2. Start Quarkus Application
     │
     ▼
3. Benchmark Runner (WRK/JMH):
     │
     ├─► Record Start Timestamp
     ├─► Run Benchmark
     ├─► Record End Timestamp
     └─► Pass timestamps to MetricsOrchestrator
              │
              ▼
4. MetricsOrchestrator:
     │
     ├─► Query Prometheus API for exact time range
     ├─► Fetch CPU, memory, JVM metrics
     ├─► Pass to BenchmarkMetricsTransformer
     └─► Generate server-metrics.json
              │
              ▼
5. Generate unified reports with real-time metrics
----

== Implementation Components

=== Core Classes

* **link:../cui-benchmarking-common/src/main/java/de/cuioss/benchmarking/common/metrics/PrometheusClient.java[PrometheusClient]** - Handles Prometheus API communication
* **link:../cui-benchmarking-common/src/main/java/de/cuioss/benchmarking/common/metrics/MetricsOrchestrator.java[MetricsOrchestrator]** - Coordinates metrics collection
* **link:../cui-benchmarking-common/src/main/java/de/cuioss/benchmarking/common/metrics/BenchmarkMetricsTransformer.java[BenchmarkMetricsTransformer]** - Transforms raw metrics to standardized format
* **link:../cui-benchmarking-common/src/main/java/de/cuioss/benchmarking/common/metrics/PrometheusMetricsManager.java[PrometheusMetricsManager]** - Manages Prometheus integration

=== Integration Points

* **link:../benchmark-integration-wrk/src/main/java/de/cuioss/jwt/wrk/benchmark/WrkResultPostProcessor.java[WrkResultPostProcessor]** - WRK benchmark integration
* **link:../benchmark-integration-quarkus/src/main/java/de/cuioss/jwt/quarkus/benchmark/QuarkusIntegrationRunner.java[QuarkusIntegrationRunner]** - JMH/Quarkus integration

== Docker Configuration

=== Prometheus Service

Prometheus is configured to scrape Quarkus metrics every 2 seconds during benchmarks. Configuration files:

* **link:../../cui-jwt-quarkus-parent/cui-jwt-quarkus-integration-tests/docker/prometheus.yml[prometheus.yml]** - Prometheus scraping configuration
* **link:../../cui-jwt-quarkus-parent/cui-jwt-quarkus-integration-tests/docker-compose.yml[docker-compose.yml]** - Container orchestration

Key configuration points:
* Scrape interval: 2 seconds
* Target: Quarkus application at :10443/q/metrics
* HTTPS with self-signed certificates
* 1-hour data retention for benchmarking

== Metrics Captured

=== System Resources

[cols="2,3,1"]
|===
|Metric |Description |Type

|process_cpu_usage
|Quarkus application CPU utilization (0-100%)
|Gauge

|system_cpu_usage
|Total system CPU (container/host)
|Gauge

|system_cpu_count
|Number of available CPU cores
|Gauge

|jvm_memory_used_bytes
|Heap and non-heap memory usage by area
|Gauge

|jvm_threads_live_threads
|Active thread count during load
|Gauge

|jvm_threads_daemon_threads
|Daemon thread count
|Gauge

|jvm_gc_overhead
|Garbage collection overhead percentage
|Gauge
|===

=== Application Metrics

[cols="2,3,1"]
|===
|Metric |Description |Type

|cui_jwt_validation_success_operations_total
|Successful JWT validations by event type
|Counter

|cui_jwt_validation_errors_total
|JWT validation errors by category
|Counter

|cui_jwt_bearer_token_validation_seconds
|JWT validation duration metrics
|Histogram
|===

NOTE: HTTP request metrics (http_server_requests_*) are not collected as Prometheus doesn't capture them reliably during benchmark execution.

== Output Format

=== Server Metrics JSON Structure

The system produces standardized `{benchmarkName}-server-metrics.json` files:

[source,json]
----
{
  "benchmark": {
    "name": "healthCheck",
    "start_time": "2025-09-27T14:55:19Z",
    "end_time": "2025-09-27T14:55:49Z",
    "duration_seconds": 30
  },
  "resources": {
    "cpu": {
      "process": {
        "average_percent": 53.5,
        "peak_percent": 80.6,
        "std_dev": 22.34,
        "percentiles": {
          "p50": 56.9,
          "p75": 71.3,
          "p90": 77.4,
          "p99": 80.6
        }
      },
      "system": {
        "average_percent": 53.6,
        "peak_percent": 80.7,
        "std_dev": 22.21
      },
      "cores_available": 4
    },
    "memory": {
      "heap": {
        "average_mb": 14.3,
        "peak_mb": 35.5,
        "final_mb": 8.0
      },
      "gc": {
        "overhead_percent": 0.0
      }
    },
    "threads": {
      "average": 34,
      "peak": 42,
      "final": 36,
      "daemon": 7
    }
  },
  "application": {
    "jwt_validations": {
      "total": 0,
      "success": 0,
      "errors": 0,
      "cache_hits": 0,
      "cache_hit_rate_percent": 0.0
    }
  }
}
----

For detailed format specifications, see: **link:metrics-requirements.adoc[Metrics Requirements Documentation]**

=== Directory Structure

[source]
----
target/
├── benchmark-results/
│   ├── wrk/                           # WRK benchmark results
│   │   └── {benchmark}-results.json   # Throughput/latency results
│   ├── prometheus/                    # Prometheus metrics
│   │   └── {benchmark}-server-metrics.json  # Server-side metrics
│   └── gh-pages-ready/                # GitHub Pages deployment
│       ├── api/                       # API endpoints
│       │   ├── status.json
│       │   ├── benchmarks.json
│       │   └── latest.json
│       └── badges/                    # Status badges
----

== Error Handling

The system implements resilient error handling to ensure build stability:

=== Error Scenarios

[cols="2,2,3"]
|===
|Error Type |Detection |Response

|Prometheus Unavailable
|Connection timeout/refused
|Log warning, continue with empty metrics

|Invalid Metric Names
|Query returns empty result
|Skip missing metrics, use available data

|Network Timeout
|Request exceeds 30s timeout
|Log warning, continue build

|Parsing Errors
|Invalid JSON response
|Log error details, use empty metrics
|===

=== Build Stability

Critical design principle: **Metrics collection failures NEVER fail the build**.

* All exceptions are caught and logged as warnings
* Builds continue even without metrics
* Empty metric structures are created when needed
* Benchmark results remain valid regardless of metrics availability

== Testing

The system includes comprehensive test coverage:

* **link:../cui-benchmarking-common/src/test/java/de/cuioss/benchmarking/common/metrics/PrometheusClientTest.java[PrometheusClientTest]** - API client testing
* **link:../cui-benchmarking-common/src/test/java/de/cuioss/benchmarking/common/metrics/MetricsTransformerTest.java[MetricsTransformerTest]** - Transformation logic
* **link:../cui-benchmarking-common/src/test/java/de/cuioss/benchmarking/common/metrics/MetricsIntegrationTest.java[MetricsIntegrationTest]** - End-to-end integration

Test data uses real Prometheus metrics captured from actual benchmark runs, stored in:
**link:../cui-benchmarking-common/src/test/resources/metrics/[test/resources/metrics/]**

== Benefits

1. **Accurate**: Measures actual application metrics, not load generator
2. **Real-time**: Captures metrics during benchmark execution
3. **Time-aligned**: Correlates metrics with benchmark execution phases
4. **Unified**: Single orchestrator handles both WRK and JMH
5. **Professional**: Industry-standard Prometheus/Grafana stack
6. **Historical**: Time-series data enables trend analysis
7. **Build-safe**: Metrics failures never break builds
8. **Maintainable**: Clean separation of concerns with dedicated components

== Future Enhancements

* Integration with Grafana for visual dashboards
* Extended JVM metrics (class loading, compilation)
* Custom application metrics via Micrometer
* Automated alerting on performance regressions
* Long-term metrics storage and analysis