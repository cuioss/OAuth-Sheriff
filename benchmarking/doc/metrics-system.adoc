= Real-Time Metrics Collection System
:toc:
:toc-placement: preamble

== Overview

The metrics system captures real-time CPU and memory utilization from the Quarkus application during benchmark execution, not after. This ensures accurate performance data by monitoring the application under test, not the load generator.

== Problem Statement

Previous approach captured metrics AFTER benchmarks completed, showing idle CPU (~2.5%). Additionally, WRK's system-metrics.log measured the load generator's CPU usage (600-800%), not the application being tested.

== Solution Architecture

=== Components

[source]
----
┌─────────────────┐     ┌──────────────────┐     ┌─────────────────┐
│                 │     │                  │     │                 │
│   WRK/JMH       │────▶│  Quarkus App     │◀────│   Prometheus    │
│  (Load Gen)     │     │  :10443/jwt      │     │   :9090         │
│                 │     │  :10443/q/metrics│     │  (Scraper)      │
└─────────────────┘     └──────────────────┘     └─────────────────┘
        │                                                  │
        │                                                  │
        ▼                                                  ▼
┌─────────────────┐                          ┌─────────────────────┐
│  Benchmark      │                          │  Time-Series        │
│  Results        │                          │  Metrics Data       │
└─────────────────┘                          └─────────────────────┘
        │                                                  │
        └──────────────────┬───────────────────────────────┘
                           ▼
                ┌──────────────────────┐
                │  MetricsOrchestrator │
                │  (Post-Processing)   │
                └──────────────────────┘
                           │
                           ▼
                ┌──────────────────────┐
                │  Unified Reports     │
                │  with CPU Metrics    │
                └──────────────────────┘
----

=== Data Flow

[source]
----
1. Start Prometheus (docker-compose)
     │
     ▼
2. Start Quarkus Application
     │
     ▼
3. Benchmark Runner (WRK/JMH):
     │
     ├─► Record Start Timestamp
     ├─► Run Benchmark
     ├─► Record End Timestamp
     └─► Pass timestamps to MetricsOrchestrator
              │
              ▼
4. MetricsOrchestrator (Java):
     │
     ├─► Query Prometheus API for exact time range
     ├─► Fetch CPU, memory, request metrics
     ├─► Calculate system statistics (avg, max)
     └─► Store as {benchmark}-metrics.json
              │
              ▼
5. Generate unified reports with real-time metrics
----

== Implementation Details

=== Docker Compose Configuration

Prometheus service added to scrape Quarkus metrics every 2 seconds during benchmarks.

==== prometheus.yml Configuration

[source,yaml]
----
global:
  scrape_interval: 2s
  evaluation_interval: 2s

scrape_configs:
  - job_name: 'quarkus-benchmark'
    static_configs:
      - targets: ['quarkus-app:10443']
    metrics_path: '/q/metrics'
    scrape_interval: 2s
    scrape_timeout: 1s
    scheme: https
    tls_config:
      insecure_skip_verify: true  # For self-signed certificates
    honor_labels: true
    honor_timestamps: true
----

==== Docker Compose Service

[source,yaml]
----
services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=1h'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    networks:
      - benchmark-network
----

=== Java-Based Metrics Collection

Both WRK and JMH record benchmark execution timestamps and pass them to MetricsOrchestrator:

[source,java]
----
public class MetricsOrchestrator {

    public void collectBenchmarkMetrics(
            String benchmarkName,    // "jwt-validation" or "health"
            Instant startTime,        // Benchmark start timestamp
            Instant endTime,          // Benchmark end timestamp
            Path outputDir) {         // Where to store metrics

        // Query Prometheus for exact time window
        PrometheusClient client = new PrometheusClient("http://localhost:9090");

        try {
            // Fetch all relevant metrics for time range
            Map<String, TimeSeries> metrics = client.queryRange(
                List.of("process_cpu_usage",
                        "system_cpu_usage",
                        "jvm_memory_used_bytes",
                        "http_server_requests_seconds_count"),
                startTime,
                endTime,
                Duration.ofSeconds(2)
            );

            // Calculate system statistics (no percentiles for system resources)
            Map<String, Object> statistics = calculateSystemStatistics(metrics);

            // Store results separately from benchmark data
            Path metricsFile = outputDir.resolve(benchmarkName + "-metrics.json");
            exportMetrics(statistics, metricsFile);

        } catch (PrometheusException e) {
            // Create error result JSON with same structure
            Map<String, Object> errorResult = createErrorResult(benchmarkName, e);
            Path errorFile = outputDir.resolve(benchmarkName + "-metrics.json");
            exportMetrics(errorResult, errorFile);

            // Log detailed error but do not fail the build
            LOGGER.warn("Failed to collect Prometheus metrics for {}: {}", benchmarkName, e.getMessage());
        }
    }
}

public class PrometheusClient {

    private final String prometheusUrl;
    private final Duration timeout;

    public PrometheusClient(String prometheusUrl) {
        this.prometheusUrl = prometheusUrl;
        this.timeout = Duration.ofSeconds(30);
    }

    /**
     * Query Prometheus for time-series data in a specific time range.
     *
     * @param metricNames List of metric names to query
     * @param startTime Start of time range (inclusive)
     * @param endTime End of time range (inclusive)
     * @param step Step size between data points
     * @return Map of metric name to time series data
     * @throws PrometheusException if query fails or Prometheus unavailable
     */
    public Map<String, TimeSeries> queryRange(
            List<String> metricNames,
            Instant startTime,
            Instant endTime,
            Duration step) throws PrometheusException {
        // Implementation details handled in Phase 2
    }
}

public static class TimeSeries {
    private final String metricName;
    private final Map<String, String> labels;
    private final List<DataPoint> values;

    public static class DataPoint {
        private final Instant timestamp;
        private final double value;
        // getters, constructors
    }
}

public static class PrometheusException extends Exception {
    private final int statusCode;
    private final String prometheusError;

    // Constructor variants for different error scenarios:
    // - Network timeout
    // - HTTP error (404, 500, 503)
    // - Invalid JSON response
    // - Empty result set
    // - Prometheus service unavailable
}
----

=== Directory Structure

[source]
----
target/
├── benchmark-results/
│   └── benchmark-data.json      # Throughput/latency results (client-side)
└── prometheus/
    ├── health-metrics.json      # Health check CPU/memory statistics (server-side)
    └── jwt-validation-metrics.json  # JWT validation CPU/memory statistics (server-side)
----

=== Metrics Processing Pipeline

[source]
----
PrometheusMetricsCollector
         │
         ├─► Fetch time-series data from Prometheus API
         │
         ├─► Parse JSON response
         │
         ├─► Calculate system statistics (avg, max only)
         │
         └─► Export to common format
                    │
                    ▼
         MetricsTransformer
         (Existing infrastructure)
----

== Key Metrics Captured

[cols="2,3,1"]
|===
|Metric |Description |Source

|process_cpu_usage
|Quarkus application CPU utilization
|Prometheus

|system_cpu_usage
|Total system CPU (container/host)
|Prometheus

|jvm_memory_used_bytes
|Heap and non-heap memory usage
|Prometheus

|http_server_requests_seconds
|Request latency distribution
|Prometheus

|jvm_threads_current
|Active thread count during load
|Prometheus
|===

== Integration Examples

=== WRK Integration

[source,java]
----
// In WrkBenchmarkConverter or WrkResultPostProcessor
public void processBenchmark(Path wrkOutputFile) {
    // Parse WRK output
    BenchmarkResult result = parseWrkOutput(wrkOutputFile);

    // Record timestamps from WRK output or wrapper script
    Instant startTime = result.getStartTime();
    Instant endTime = result.getEndTime();
    String benchmarkName = result.getBenchmarkName(); // "jwt-validation" or "health"

    // Delegate to MetricsOrchestrator
    metricsOrchestrator.collectBenchmarkMetrics(
        benchmarkName, startTime, endTime, outputDir);
}
----

=== JMH Integration

[source,java]
----
// In QuarkusIntegrationRunner
public class QuarkusIntegrationRunner extends AbstractBenchmarkRunner {

    private Instant benchmarkStartTime;
    private Instant benchmarkEndTime;

    @Override
    protected void prepareBenchmark(BenchmarkConfiguration config) throws IOException {
        super.prepareBenchmark(config);
        benchmarkStartTime = Instant.now(); // Capture before JMH execution
    }

    @Override
    protected void processResults(Collection<RunResult> results, BenchmarkConfiguration config) {
        benchmarkEndTime = Instant.now(); // Capture after JMH execution

        for (RunResult result : results) {
            String benchmarkName = extractBenchmarkName(result);

            // Use captured timestamps for entire benchmark duration
            metricsOrchestrator.collectBenchmarkMetrics(
                benchmarkName, benchmarkStartTime, benchmarkEndTime, outputDir);
        }
    }
}
----

== Error Handling and Resilience

The system implements comprehensive error handling to ensure build stability while maintaining result integrity.

=== Error Scenarios and Responses

[cols="2,2,3"]
|===
|Error Type |Detection |Response

|Prometheus Unavailable
|Connection timeout/refused
|Create error-metrics.json with connection status, log warning, continue build

|HTTP Error (404, 500, 503)
|HTTP status codes
|Create error-metrics.json with HTTP details, log warning, continue build

|Invalid JSON Response
|JSON parse failure
|Create error-metrics.json with parse error, log warning, continue build

|Empty Metrics Data
|Zero data points returned
|Create error-metrics.json with empty data notice, log warning, continue build

|Network Timeout
|Request timeout (30s)
|Create error-metrics.json with timeout details, log warning, continue build
|===

=== Error Result JSON Structure

When Prometheus metrics collection fails, the system creates a compliant JSON file with error information:

[source,json]
----
{
  "quarkus-runtime-metrics": {
    "timestamp": "2025-09-25T10:30:45.123Z",
    "status": "error",
    "error": {
      "type": "PrometheusUnavailable",
      "message": "Connection to Prometheus failed: Connection refused",
      "details": {
        "prometheusUrl": "http://localhost:9090",
        "httpStatus": null,
        "timeout": "30s",
        "benchmarkName": "jwt-validation",
        "timeRange": {
          "start": "2025-09-25T10:25:00.000Z",
          "end": "2025-09-25T10:30:00.000Z"
        }
      }
    },
    "system": {},
    "http_server_requests": {},
    "cui_jwt_validation_success_operations_total": {},
    "cui_jwt_validation_errors": {}
  }
}
----

=== Build Failure Prevention

Critical design principle: **Prometheus failures NEVER fail the build**.

1. **Catch all PrometheusException**: Comprehensive exception handling
2. **Log warnings, not errors**: Use LOGGER.warn() to avoid build failures
3. **Create compliant output**: Always produce expected JSON structure
4. **Preserve file naming**: Same file name/location as successful metrics
5. **Continue processing**: Other benchmarks proceed normally

=== Error Logging Strategy

[source,java]
----
// Error logging examples that do not fail builds
LOGGER.warn("Prometheus metrics collection failed for {}: {}", benchmarkName, e.getMessage());
LOGGER.warn("Using empty metrics structure due to Prometheus unavailability");
LOGGER.warn("Benchmark {} completed without real-time metrics", benchmarkName);

// Detailed error written to error-metrics.json, not console
private Map<String, Object> createErrorResult(String benchmarkName, PrometheusException e) {
    return Map.of(
        "quarkus-runtime-metrics", Map.of(
            "timestamp", Instant.now().toString(),
            "status", "error",
            "error", Map.of(
                "type", e.getClass().getSimpleName(),
                "message", e.getMessage(),
                "details", Map.of(
                    "prometheusUrl", prometheusUrl,
                    "httpStatus", e.getStatusCode(),
                    "benchmarkName", benchmarkName,
                    "timeRange", Map.of(
                        "start", startTime.toString(),
                        "end", endTime.toString()
                    )
                )
            ),
            "system", Map.of(),
            "http_server_requests", Map.of(),
            "cui_jwt_validation_success_operations_total", Map.of(),
            "cui_jwt_validation_errors", Map.of()
        )
    );
}
----

== Benefits

1. **Accurate**: Measures actual application CPU, not load generator
2. **Time-aligned**: Correlates metrics with benchmark execution phases
3. **Unified**: Single MetricsOrchestrator handles both WRK and JMH
4. **No duplication**: Prometheus querying logic in one place
5. **Professional**: Industry-standard Prometheus/Grafana stack
6. **Historical**: Time-series data enables trend analysis
7. **Build-safe**: Prometheus failures never break builds
8. **Transparent**: Error details captured in JSON for debugging

== Migration Path

1. Phase 1: Add Prometheus to Docker Compose
2. Phase 2: Implement PrometheusClient and extend MetricsOrchestrator
3. Phase 3: Integrate with WRK benchmark runner
4. Phase 4: Integrate with JMH benchmark runner
5. Phase 5: Completely remove post-benchmark metrics collection
6. Phase 6: Update reports to show CPU utilization graphs