= JFR Profiling Analysis - January 2026
:toc:
:toc-placement: preamble
:toclevels: 3

== Executive Summary

JFR (Java Flight Recorder) profiling of the OAuth Sheriff integration tests under load provides CPU hotspot analysis for the JWT validation endpoint.

=== Recording Configuration

* **Platform**: Apple M4, 10 CPU cores
* **Runtime**: Mandrel jdk-25 native image
* **Framework**: Quarkus 3.27.2 (LTS)
* **JFR Settings**: `profile` preset, 240 seconds duration
* **Load**: WRK benchmark with 50 concurrent connections

=== Recording Statistics

[source]
----
Recording Duration: 240 seconds
Total Execution Samples: 48,027
Unique Hot Methods: 3,181
----

== CPU Hotspot Analysis

The following table shows methods at the **top of the call stack** when execution samples were taken. These are the methods actively consuming CPU.

=== Top 15 Hot Methods

[cols="1,3,1,1", options="header"]
|===
| Rank | Method | Samples | %

| 1 | `GHASH.blockMult()` | 5,126 | 10.68%
| 2 | `AESCrypt.implEncryptBlock()` | 2,871 | 5.98%
| 3 | `FileOutputStream.writeBytes()` | 1,337 | 2.78%
| 4 | `SocketDispatcher.write0()` | 1,298 | 2.70%
| 5 | `CounterMode.implCrypt()` | 973 | 2.03%
| 6 | `PlatformDependent.getByte()` | 927 | 1.93%
| 7 | `CodeInfoDecoder.lookupCodeInfo()` | 906 | 1.89%
| 8 | `JfrOldObjectSampler.scavenge()` | 840 | 1.75%
| 9 | `Pthread.pthread_cond_timedwait()` | 839 | 1.75%
| 10 | `EPoll.wait()` | 814 | 1.70%
| 11 | `JavaMonitor.tryRelease()` | 545 | 1.14%
| 12 | `EventFD.set0()` | 461 | 0.96%
| 13 | `SocketDispatcher.read0()` | 460 | 0.96%
| 14 | `StringLatin1.charAt()` | 434 | 0.90%
| 15 | `UnmanagedMemoryUtil.copyLongsForward()` | 434 | 0.90%
|===

=== Categorized CPU Usage

[cols="2,1,1,3", options="header"]
|===
| Category | Samples | % | Components

| TLS/Crypto
| 8,970
| 18.7%
| GHASH (5,126), AES (2,871), CounterMode (973)

| Network I/O
| 4,369
| 9.1%
| FileOutputStream (1,337), SocketDispatcher write (1,298), read (460), EPoll (814), EventFD (461)

| JVM Runtime (SubstrateVM)
| ~4,500
| 9.4%
| CodeInfoDecoder, JFR sampling, GC, monitors, memory ops

| Netty Buffers
| ~1,500
| 3.1%
| PlatformDependent, ByteBuf operations

| Quarkus/Arc Framework
| ~600
| 1.3%
| VertxLocalsHelper, RequestHandler, ContextInstances

| Vert.x HTTP
| ~500
| 1.0%
| HttpUtils, header validation

| CUI Sheriff OAuth
| ~160
| 0.3%
| See detailed breakdown below

| BigInteger (RSA)
| 46
| 0.1%
| addOne (31), mulsub (5), montReduce (3), other (7)
|===

== CUI Sheriff OAuth Library Analysis

Methods from `de.cuioss.sheriff.oauth` at top of call stack:

[cols="3,1", options="header"]
|===
| Method | Samples

| `TokenContent.getClaimOption()` | 9
| `CustomAccessLogFilter.filter()` (proxy) | 8
| `VertxHttpServletRequestAdapter.getHeaders()` | 7
| `LogRecordModel.format()` | 7
| `CustomAccessLogFilter.filter()` | 6
| `ClientIpExtractor.extractFromHeader()` | 5
| `CustomAccessLogFilter.formatLogEntry()` | 5
| `BearerTokenProducer.getBearerTokenResult()` | 4
| `AccessTokenContent.getScopes()` | 4
| `CollectionClaimHandler.providesValues()` | 4
| Other methods (41 unique) | ~100
| **Total** | **~160**
|===

Key observations:

* **Logging dominates**: `CustomAccessLogFilter` and logging utilities account for ~50% of CUI library samples
* **Token validation minimal**: `TokenValidator.createAccessToken()` appears only 1 time
* **Token content access**: `TokenContent.getClaimOption()` at 9 samples is the most frequent token operation

== BigInteger / RSA Analysis

RSA signature verification uses BigInteger for modular exponentiation:

[cols="2,1,1", options="header"]
|===
| Method | Samples | %

| `BigInteger.addOne()` | 31 | 0.065%
| `MutableBigInteger.mulsub()` | 5 | 0.010%
| `BigInteger.implMulAddCheck()` | 4 | 0.008%
| `BigInteger.montReduce()` | 3 | 0.006%
| `MutableBigInteger.divideMagnitude()` | 1 | 0.002%
| `BigInteger.subN()` | 1 | 0.002%
| `BigInteger.implMontgomerySquare()` | 1 | 0.002%
| **Total** | **46** | **0.096%**
|===

At 46 samples out of 48,027 (0.1%), BigInteger operations represent minimal CPU usage.

== TLS/HTTPS Encryption

TLS encryption for HTTPS is the largest single CPU consumer:

[cols="2,1,1,2", options="header"]
|===
| Method | Samples | % | Purpose

| `GHASH.blockMult()` | 5,126 | 10.68% | GCM authentication tag
| `AESCrypt.implEncryptBlock()` | 2,871 | 5.98% | AES block encryption
| `CounterMode.implCrypt()` | 973 | 2.03% | CTR mode processing
| **Total** | **8,970** | **18.7%** |
|===

This is the cost of HTTPS transport security, not specific to JWT validation.

== Thread Activity

Threads observed in execution samples:

* `vert.x-eventloop-thread-*` - HTTP request handling
* `vert.x-internal-blocking-*` - Blocking operations
* `executor-thread-*` - Worker pool
* `JFR recorder` / `JFR Periodic Tasks` - Profiling overhead
* `HttpClient-*-SelectorManager` - Outbound HTTP (Keycloak JWKS)

== JFR Analysis Commands

Commands used to extract this data:

[source,bash]
----
# Recording summary
jfr summary jwt-profile.jfr

# Top hot methods (top of stack)
jfr print --events ExecutionSample jwt-profile.jfr | \
  grep -A1 "stackTrace = \[" | grep -v "stackTrace" | \
  sed 's/^[[:space:]]*//' | sort | uniq -c | sort -rn | head -50

# BigInteger samples
jfr print --events ExecutionSample jwt-profile.jfr | \
  grep -A1 "stackTrace = \[" | grep "BigInteger" | sort | uniq -c

# CUI library samples
jfr print --events ExecutionSample jwt-profile.jfr | \
  grep -A1 "stackTrace = \[" | grep "de.cuioss" | sort | uniq -c | sort -rn
----

== Container Configuration

JFR output requires a volume mount due to read-only container filesystem:

[source,yaml]
----
# docker-compose.jfr.yml
services:
  oauth-sheriff-integration-tests:
    image: "oauth-sheriff-integration-tests:jfr"
    volumes:
      - ./target/jfr-output:/tmp/jfr-output:rw
----

== Non-CPU Overhead Analysis

The 14x gap between JMH (77µs) and integration (1,087µs) cannot be explained by CPU time alone. JFR shows CUI library at only 0.3% of CPU samples, yet latency decomposition shows token validation at 57% of request time. This section analyzes non-CPU overhead.

=== Thread Parking Analysis

JFR recorded **32,549 ThreadPark events** over the 240-second benchmark, indicating significant wait time:

[cols="2,1,2", options="header"]
|===
| Thread Type | Count | Purpose

| `executor-thread-*` | ~15,000 | Worker pool threads waiting for tasks
| `vert.x-eventloop-thread-*` | ~8,000 | Event loop waiting for I/O
| `HttpClient-*-Worker` | ~4,000 | HTTP client connection pool
| `ForkJoinPool.commonPool-*` | ~3,000 | Async completion handlers
| JFR/scheduler threads | ~2,500 | Profiling and scheduling overhead
|===

These park events are **normal** for a reactive architecture under load — threads wait efficiently when no work is available. They do not directly explain the 14x gap.

=== Lock Contention Analysis

JFR recorded only **14 JavaMonitorEnter events** (10-14ms each) over 240 seconds:

[cols="2,1,2", options="header"]
|===
| Monitor Class | Events | Context

| `Http1xServerConnection` | 6 | Vert.x HTTP/1.x connection write queue
| `java.lang.Object` (EPollSelectorImpl) | 8 | NIO selector interrupt clearing
|===

**Conclusion**: Lock contention is **not** the cause of the 14x gap. The 14 events over 240 seconds represent negligible overhead.

=== Garbage Collection Analysis

JFR recorded **1,312 GC events** during a 60-second benchmark (22 GCs/second):

[cols="2,2", options="header"]
|===
| Metric | Value

| Total GC events | 1,312
| Total GC pause time | 2,223 ms (2.2 seconds)
| Average pause | 1.69 ms
| GC frequency | ~22 GCs/second
|===

**Pause distribution:**

[cols="2,1,2", options="header"]
|===
| Pause Duration | Events | Percentage

| < 1ms | 0 | 0%
| 1-2ms | 1,174 | 89%
| 2-5ms | 11 | 1%
| > 10ms | 127 | 10%
|===

**Heap usage (sawtooth pattern):**

* Minimum: 10 MB (after GC)
* Maximum: 100 MB (before GC)
* Average: 45 MB

**Key observation**: The Serial GC in SubstrateVM runs frequently (22/s) to manage high allocation rate. Most pauses are 1-2ms (fast incremental GCs), but 127 events are >10ms stop-the-world pauses. This GC activity accounts for **~3.7% of wall-clock time** (2.2s / 60s).

**Does GC explain the 14x gap?** No. At 22 GCs/second and ~21,000 requests/second, only ~1 in 950 requests encounters a GC pause. The 3.7% overhead is amortized — most individual requests complete without GC interference. GC contributes to tail latency (P99) but is not the primary cause of the 1.1ms median token validation cost.

=== Memory Allocation Analysis

JFR recorded **45,048 ObjectAllocationSample events**. Top allocation types:

[cols="2,1,2", options="header"]
|===
| Object Class | Samples | Purpose

| `byte[]` | 12,216 | Buffers for I/O, serialization
| `Object[]` | 2,607 | Array allocations
| `long[]` | 2,482 | Primitive arrays (timestamps, etc.)
| `HashMap$Node` | 2,040 | Map entries (headers, claims)
| `FrameInfoQueryResult` | 1,655 | SubstrateVM stack traces (JFR)
| `HashMap$Node[]` | 1,064 | Map backing arrays
| `String` | 1,042 | String allocations
| `HashMap` | 811 | New HashMap instances
| `ArrayList$Itr` | 808 | Iterator allocations
| **`JavaMonitor`** | **766** | Monitor objects for synchronization
| `ArrayList` | 749 | List allocations
| `StringBuilder` | 708 | String building
| `HeadersMultiMap$MapEntry` | 463 | Vert.x HTTP headers
| **`CreationalContextImpl`** | **250** | CDI bean creation contexts
|===

**Key observations**:

1. **CDI CreationalContextImpl (250 samples)** — Each `Instance<BearerTokenResult>.get()` call creates a CDI creational context
2. **JavaMonitor allocations (766 samples)** — SubstrateVM allocates monitor objects for synchronized operations
3. **HashMap allocations (811 + 2,040 + 1,064)** — Significant overhead from header maps and claim maps

=== Why CPU% Doesn't Match Latency%

The JFR data reveals the 14x gap is primarily due to:

1. **CDI `Instance.get()` overhead** — The producer method creates a new `CreationalContextImpl` and resolves dependencies for each injection
2. **InjectionPoint introspection** — CDI extracts `@BearerToken` annotation parameters per request
3. **Object allocation** — Each validation creates new HashMaps, Sets, and result objects
4. **Thread scheduling** — 50 concurrent connections cause frequent context switches (not visible in JFR as individual events)
5. **Memory barriers** — Lock-free cache operations require memory ordering guarantees

**These operations consume wall-clock time without proportionally consuming CPU cycles** — the CPU is often waiting on memory, cache coherence, or thread scheduling.

=== Access Log Filter Overhead

The `CustomAccessLogFilter` is registered via `@Provider` and runs on **all JAX-RS endpoints**, including both health and JWT endpoints. JFR shows ~50% of CUI library CPU samples in the filter.

**Measured Impact (Filter Disabled Benchmark):**

[cols="2,2,2,2", options="header"]
|===
| Endpoint | Filter Enabled | Filter Disabled | Difference

| Health P50
| 0.510ms
| 0.417ms
| **-0.093ms (18%)**

| JWT P50
| 1.91ms
| 1.96ms
| +0.05ms (within variance)

| JWT/Health ratio
| 4.55x
| 4.70x
| ~same
|===

**Critical findings:**

1. **Filter overhead is ~0.1ms per request** — visible on the health endpoint (0.510ms → 0.417ms)
2. **Filter does NOT explain JWT-health gap** — JWT stayed at ~1.96ms, health improved to 0.417ms
3. **JWT validation (1.1ms) dominates** — filter overhead (0.1ms) is negligible in comparison
4. **50% CUI library CPU in filter is misleading** — that's 50% of only 0.3% total CPU

The filter is configured with `min-status-code=206`, so it only **logs** errors but still **executes** `shouldLog()` and `isPathIncluded()` on every response. These methods create `Path.of(path)` objects per request.

For benchmark isolation, the filter can be disabled:
[source,properties]
----
cui.http.access-log.filter.enabled=false
----

=== CDI Producer Scope Analysis

The `BearerTokenProducer.produceBearerTokenResult()` method is implicitly `@Dependent` scoped (no explicit scope annotation on producer method). This means:

* A new `BearerTokenResult` is created for **every** `Instance.get()` call
* Each creation involves `InjectionPoint` introspection to extract annotation parameters
* Three `Set.copyOf(List.of(...))` allocations occur per request for scopes/roles/groups

**Optimization consideration**: If the producer were `@RequestScoped`, the result would be cached for the duration of a request, eliminating redundant validation if `Instance.get()` is called multiple times. However, the current endpoint design calls it only once per request, so the benefit would be minimal unless the usage pattern changes.

== CDI Instance.get() Research Findings

This section documents research into Quarkus CDI `Instance.get()` overhead and optimization options, explaining why the 1.087ms token validation cost cannot be reduced without API changes.

=== Quarkus @WithCaching Annotation

Quarkus ArC provides `@io.quarkus.arc.WithCaching` specifically to address `Instance.get()` performance:

[quote, Quarkus CDI Reference]
When applied to an injected Instance, the result of `Instance#get()` is *cached on the first call*, and the same value is returned for all subsequent calls, *even for @Dependent beans*.

**Source**: https://quarkus.io/guides/cdi-reference

=== What Instance.get() Does Internally

Each uncached `Instance.get()` call performs (from ArC source code analysis):

1. Creates a **child CreationalContext** (`this.creationalContext.child(bean)`)
2. Constructs **InjectionPointImpl** with type, qualifiers, annotations, member info
3. Sets InjectionPoint via `InjectionPointProvider.setCurrent()`
4. Creates bean instance (for @Dependent scope)
5. Cleans up InjectionPoint in try-finally

**Source**: https://github.com/quarkusio/quarkus/blob/main/independent-projects/arc/runtime/src/main/java/io/quarkus/arc/impl/InstanceImpl.java

This overhead is confirmed by JFR allocation samples showing 250 `CreationalContextImpl` allocations during the benchmark.

=== Memory Leak Risk with @Dependent

**CRITICAL**: Instances from `Instance.get()` for `@Dependent` beans are never GC'd when the holding bean is `@ApplicationScoped`:

[quote, Quarkus Issue #5209]
Instances obtained from Instance.get() are *dependent objects of the Instance itself*. For @ApplicationScoped beans holding an Instance, dependent instances accumulate for the application's lifetime.

**Mitigation**: Use `Instance.Handle` with try-with-resources or call `instance.destroy(myInstance)`.

**Sources**:

* https://github.com/quarkusio/quarkus/issues/5209
* https://rmannibucau.wordpress.com/2015/03/02/cdi-and-instance-3-pitfalls-you-need-to-know/

=== Why @WithCaching Cannot Be Applied

The `JwtValidationEndpoint` uses **5 different Instance fields** with different `@BearerToken` annotation parameters:

[source,java]
----
@BearerToken Instance<BearerTokenResult> basicToken;
@BearerToken(requiredScopes = {"read"}) Instance<BearerTokenResult> tokenWithScopes;
@BearerToken(requiredRoles = {"user"}) Instance<BearerTokenResult> tokenWithRoles;
@BearerToken(requiredGroups = {"test-group"}) Instance<BearerTokenResult> tokenWithGroups;
@BearerToken(requiredScopes = {"read"}, requiredRoles = {"user"}, ...) Instance<BearerTokenResult> tokenWithAll;
----

Each field requires **different authorization validation** via `InjectionPoint` introspection. Applying `@WithCaching` would:

* Return the `basicToken` result when `tokenWithScopes` is requested → **security bypass**
* Skip scope/role/group validation entirely

**Conclusion**: The design requires per-request `InjectionPoint` introspection to extract annotation parameters, making `@WithCaching` inapplicable for security reasons. This is the same constraint that blocked `@RequestScoped` refactoring.

=== Estimated Overhead Breakdown

[cols="2,1,1", options="header"]
|===
| Component | Estimated | Confidence

| Child CreationalContext allocation
| ~100-200µs
| HIGH (documented)

| InjectionPoint construction + annotation extraction
| ~50-100µs
| MEDIUM (reflection)

| Set.copyOf() × 3 for scopes/roles/groups
| ~10-20µs
| LOW

| @Timed Micrometer instrumentation
| ~1-5µs
| HIGH (Micrometer docs)

| TokenValidator.createAccessToken() under load
| ~600-800µs
| MEDIUM (JMH=77µs, 8-10x concurrency multiplier)

| **Total**
| **~760-1125µs**
| -
|===

**Matches observed**: 1,087µs

=== Historical Benchmark Context

From struberg/cdi-performance (2015, 10M invocations, 100 threads):

[cols="2,1,1,1", options="header"]
|===
| Scope | OpenWebBeans | Weld | Baseline

| @ApplicationScoped
| 13ms
| 17,751ms
| 8ms

| @RequestScoped
| 648ms
| 2,593ms
| 8ms
|===

**Note**: Data is outdated, Quarkus ArC not included, but shows magnitude of scope overhead differences across implementations.

=== Investigation Conclusion

The 14x JMH-to-integration gap (77µs → 1,087µs) is **understood but not optimizable** without API changes:

1. **Root cause identified**: CDI `Instance.get()` with per-request `InjectionPoint` introspection
2. **Optimization blocked**: Security requires different annotation parameters per injection point
3. **Performance acceptable**: 23K ops/s at 1.9ms P50 exceeds targets (>10K ops/s, <5ms)
4. **Investigation complete**: Further measurement would confirm but not change the outcome

== Related Documents

* link:Analysis-01.2026-Integration.adoc[Integration Benchmark Analysis] - Throughput and latency measurements
* link:Analysis-01.2026-Latency-Decomposition.adoc[Latency Decomposition] - Ablation study results
* link:Analysis-01.2026-Micro.adoc[Microbenchmark Analysis] - JMH component-level timings
* link:../oauth-sheriff-quarkus-parent/doc/performance/jfr-profiling-guide.adoc[JFR Profiling Guide] - Setup instructions
