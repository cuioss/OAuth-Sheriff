= JWT Integration Benchmark Performance Scoring
:toc:
:toclevels: 2

This document describes the integration-specific aspects of the JWT performance scoring system. For complete methodology, scoring calculations, and interpretation guidelines, see xref:../../cui-jwt-benchmarking/doc/performance-scoring.adoc[JWT Performance Scoring System].

== Overview

The integration benchmark module uses the **identical weighted scoring formula** as the xref:../../cui-jwt-benchmarking/doc/performance-scoring.adoc[micro-benchmark module]:

[source,text]
----
Performance Score = (Throughput × 0.57) + (Latency_Inverted × 0.40) + (Error_Resilience × 0.03)
----

== Integration-Specific Implementation

=== Time Scale Differences

[cols="1,1,1", options="header"]
|===
|Aspect |Integration Benchmarks |Micro-Benchmarks

|**Measurement Unit**
|Milliseconds
|Microseconds

|**Latency Conversion**
|`1,000 ÷ avgTimeInMillis`
|`1,000,000 ÷ avgTimeInMicros`

|**Measurement Scope**
|HTTP request/response + validation
|Pure JWT validation only

|**Expected Values**
|~1000x higher times (due to HTTP overhead)
|Raw library performance
|===

=== Benchmark Method Mapping

The integration benchmark methods are named to match the micro-benchmark extraction patterns:

* **Throughput (57% weight)**: `PerformanceIndicatorBenchmark.measureThroughput()`
* **Latency (40% weight)**: `PerformanceIndicatorBenchmark.measureAverageTime()`
* **Error Resilience (3% weight)**: `ErrorLoadBenchmark.validateMixedTokens()` (0% error rate)

=== Score Calculation Implementation

The performance score calculation is implemented in the GitHub Actions workflow scripts using shell/awk:

[source,bash]
----
# Convert average time to operations per second (inverted metric)  
latency_ops_per_sec=$(echo "1000 / $avg_time_millis" | bc -l)

# Weighted score: 57% throughput, 40% latency, 3% error resilience
performance_score=$(echo "($throughput_ops_per_sec * 0.57) + ($latency_ops_per_sec * 0.40) + ($error_resilience_ops_per_sec * 0.03)" | bc -l)
----

== Integration Environment

=== Infrastructure Components

* **Quarkus Native Application**: Containerized for production-like testing
* **Keycloak Integration**: Real JWT issuer for authentic token generation
* **Docker Networking**: HTTP/HTTPS communication overhead included
* **Container Orchestration**: Maven lifecycle manages container startup/shutdown

=== Performance Characteristics

* **Scale**: Results ~1000x slower than micro-benchmarks due to HTTP overhead
* **Comparability**: Same weighted formula ensures relative performance tracking
* **Use Case**: System-level performance validation and deployment readiness

== Usage Guidelines

=== When to Use Integration Benchmarks

* Validate end-to-end system performance
* Measure HTTP/container overhead impact
* Test real Keycloak integration performance
* Verify performance in production-like environments

=== Result Interpretation

* **Focus on Trends**: Compare relative changes rather than absolute values
* **Baseline Comparison**: Use micro-benchmark results as baseline for library performance
* **System Performance**: Integration results show total system performance including infrastructure

== See Also

* xref:../../cui-jwt-benchmarking/doc/performance-scoring.adoc[JWT Performance Scoring System] - Complete methodology and scoring calculations
* xref:../../cui-jwt-benchmarking/README.adoc[JWT Micro-Benchmarking Module] - Library component performance testing
* xref:README.adoc[Integration Benchmark Module Overview] - Usage and configuration