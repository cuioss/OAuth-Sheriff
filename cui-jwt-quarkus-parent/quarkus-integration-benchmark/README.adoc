= JWT Quarkus Integration Benchmarking
:toc: macro
:toclevels: 3

This module provides integration-level performance benchmarking for the JWT Quarkus extension using containerized environments with **identical performance scoring** to the micro-benchmark module.

toc::[]

== Overview

While the xref:../../cui-jwt-benchmarking/README.adoc[JWT Micro-Benchmarking Module] focuses on in-memory testing of library components, this module provides end-to-end integration performance testing using:

* Native Quarkus application in containers
* Keycloak integration for realistic token issuing
* Memory-based token issuers for testing
* Real HTTP/HTTPS communication patterns

== Configuration

The module is configured to:

* Skip compilation during normal builds (`skip.benchmark=true`)
* Exclude from Sonar analysis (`sonar.skip=true`)
* Build only when explicitly enabled
* Use the same performance metrics as micro-benchmarks

== Usage

=== Local Testing
[source,bash]
----
# The benchmark build requires the integration-tests profile to build the native executable first.

./mvnw clean package -Pintegration-tests -pl cui-jwt-quarkus-parent/cui-jwt-quarkus-integration-tests

# Run integration benchmarks locally
./mvnw clean verify -pl /cui-jwt-quarkus-parent/quarkus-integration-benchmark -Dskip.benchmark=false
----

=== CI/CD Execution
Integration benchmarks are executed automatically via GitHub Actions workflow alongside micro-benchmarks.

== Architecture

The benchmarks run in a containerized environment similar to `cui-jwt-quarkus-integration-tests` but with JMH performance measurement capabilities.

=== Container Setup
* Quarkus native application
* Keycloak container (issuer1)
* Memory-based issuer
* Shared container network

=== Performance Metrics

Uses the **identical weighted scoring formula** as the xref:../../cui-jwt-benchmarking/doc/performance-scoring.adoc[micro-benchmark module]:

**`Performance Score = (Throughput × 0.57) + (Latency_Inverted × 0.40) + (Error_Resilience × 0.03)`**

Metrics measured:
* **Throughput (57% weight)**: HTTP requests per second under maximum concurrent load
* **Latency (40% weight)**: Average HTTP response time (converted to ops/sec via inversion)
* **Error Resilience (3% weight)**: Baseline throughput with 0% error rate
* **Performance Score**: Weighted composite metric comparable to micro-benchmarks

=== Benchmark Comparison

[cols="1,1,1", options="header"]
|===
|Aspect |Integration Benchmarks |Micro-Benchmarks

|**Measurement Scope**
|End-to-end HTTP validation
|In-memory library calls

|**Time Scale** 
|Milliseconds (HTTP overhead)
|Microseconds (pure library)

|**Infrastructure**
|Docker containers + Keycloak
|Direct JVM execution

|**Scoring Formula**
|**Identical** (same weights)
|**Identical** (same weights)

|**Results Comparability**
|Relative trends & ratios
|Absolute performance values

|**Use Case**
|System-level performance
|Library optimization
|===

== Results

Results are automatically:
* Collected in JSON format
* Uploaded as GitHub artifacts
* Processed for GitHub Pages visualization
* Displayed as README badges alongside micro-benchmark results

== Documentation

=== Performance Scoring
* xref:performance-scoring.adoc[Integration Benchmark Performance Scoring] - Integration-specific implementation details
* xref:../../cui-jwt-benchmarking/doc/performance-scoring.adoc[JWT Performance Scoring System] - Complete methodology and scoring calculations

=== Related Modules
* xref:../../cui-jwt-benchmarking/README.adoc[JWT Micro-Benchmarking Module] - Library component performance testing
* xref:../cui-jwt-quarkus-integration-tests/README.adoc[JWT Quarkus Integration Tests] - Integration testing infrastructure