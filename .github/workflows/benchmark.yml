name: JMH Benchmark

on:
  # Allow manual triggering
  workflow_dispatch:
  # Run on merges to main branch
  pull_request:
    types:
      - closed
    branches:
      - 'main'
  # Run on specific tags to capture performance at release points
  push:
    tags:
      - 'v*.*.*'  # Run on version tags

# Declare default permissions as read only
permissions: read-all

# Prevent concurrent benchmark runs to avoid interference
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false  # Don't cancel in-progress runs as benchmarks are expensive

jobs:
  benchmark:
    name: Run JMH Benchmarks
    runs-on: ubuntu-latest
    # Only run on merged PRs, not just closed ones
    if: github.event_name != 'pull_request' || github.event.pull_request.merged == true
    # Add timeout to prevent long-running jobs
    timeout-minutes: 30
    permissions:
      # Needed to upload artifacts
      contents: write

    steps:
      - name: Harden the runner (Audit all outbound calls)
        uses: step-security/harden-runner@0634a2670c59f64b4a01f0f96f84700a4088b9f0 # v2.12.0
        with:
          egress-policy: audit

      - name: Checkout code
        uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2
        with:
          fetch-depth: 0  # Fetch all history for proper versioning

      - name: Set up JDK 21
        uses: actions/setup-java@99b8673ff64fbf99d8d325f52d9a5bdedb8483e9 # v4.2.1
        with:
          java-version: '21'
          distribution: 'temurin'
          cache: maven

      - name: Run JMH Benchmarks
        run: |
          # Create directory for benchmark results
          mkdir -p benchmark-results

          # Run benchmarks with JSON output format, skipping tests to avoid duplicate runs
          # Configure JMH parameters for CI environment: fewer iterations for faster execution
          ./mvnw --no-transfer-progress clean verify -Pbenchmark \
            -Djmh.result.format=JSON \
            -Djmh.result.filePrefix=benchmark-results/jmh-result \
            -Djmh.iterations=3 \
            -Djmh.warmupIterations=2 \
            -Djmh.forks=1 \
            -Djmh.threads=2

          # Add timestamp to results
          echo "{ \"timestamp\": \"$(date -u +"%Y-%m-%dT%H:%M:%SZ")\", \"commit\": \"${{ github.sha }}\" }" > benchmark-results/metadata.json

      - name: Upload benchmark results
        uses: actions/upload-artifact@ea165f8d65b6e75b540449e92b4886f43607fa02 # v4.6.2
        with:
          name: benchmark-results
          path: benchmark-results/
          retention-days: 90  # Keep results for 90 days

      - name: Generate benchmark visualization
        run: |
          # Create directory for GitHub Pages
          mkdir -p gh-pages

          # Copy benchmark results to gh-pages directory
          cp -r benchmark-results/* gh-pages/

          # Find the JMH result file and create a symlink with the expected name
          # The file is likely named with a timestamp, but the visualization expects jmh-result.json
          find benchmark-results -name "jmh-result*.json" -type f -exec cp {} gh-pages/jmh-result.json \;

          # Create index.html with visualization
          cat > gh-pages/index.html << 'EOL'
          <!DOCTYPE html>
          <html>
          <head>
            <title>JWT Validation Benchmark Results</title>
            <!-- Use specific versions with integrity hashes for better security -->
            <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" 
                    integrity="sha384-1hX5s6k/n6FvE0WOYQrXl/j0zGtCi3Ipdz9uYj+LKMNnuiXC0Wf3AwkTbzKFBGnw" 
                    crossorigin="anonymous"></script>
            <script src="https://cdn.jsdelivr.net/npm/moment@2.30.1/min/moment.min.js" 
                    integrity="sha384-QRk9GBJqP8UxkPnkpGKmRXxZsQQuRsEOHckwF/mTcGM1TSSDxG9jAA+uHAcHZwP6" 
                    crossorigin="anonymous"></script>
            <style>
              body { font-family: Arial, sans-serif; margin: 20px; }
              .container { max-width: 1200px; margin: 0 auto; }
              .chart-container { height: 400px; margin-bottom: 30px; }
              h1, h2 { color: #333; }
              .benchmark-selector { margin-bottom: 20px; }
              select { padding: 8px; font-size: 16px; }
            </style>
          </head>
          <body>
            <div class="container">
              <h1>JWT Validation Benchmark Results</h1>

              <div class="benchmark-selector">
                <label for="benchmark-select">Select Benchmark:</label>
                <select id="benchmark-select"></select>
              </div>

              <div class="chart-container">
                <canvas id="benchmarkChart"></canvas>
              </div>

              <h2>Latest Results</h2>
              <div id="latest-results"></div>
            </div>

            <script>
              // Fetch all benchmark results
              async function fetchBenchmarkData() {
                try {
                  const response = await fetch('jmh-result.json');
                  const data = await response.json();
                  return data;
                } catch (error) {
                  console.error('Error fetching benchmark data:', error);
                  return null;
                }
              }

              // Process benchmark data for visualization
              function processBenchmarkData(data) {
                if (!data) return null;

                const benchmarks = {};

                // Group benchmarks by name
                data.forEach(benchmark => {
                  const name = benchmark.benchmark;
                  const mode = benchmark.mode;
                  const score = benchmark.primaryMetric.score;
                  const error = benchmark.primaryMetric.scoreError;
                  const unit = benchmark.primaryMetric.scoreUnit;

                  if (!benchmarks[name]) {
                    benchmarks[name] = {
                      name,
                      modes: {}
                    };
                  }

                  benchmarks[name].modes[mode] = {
                    score,
                    error,
                    unit
                  };
                });

                return Object.values(benchmarks);
              }

              // Populate benchmark selector
              function populateBenchmarkSelector(benchmarks) {
                const select = document.getElementById('benchmark-select');

                benchmarks.forEach(benchmark => {
                  const option = document.createElement('option');
                  option.value = benchmark.name;
                  option.textContent = benchmark.name;
                  select.appendChild(option);
                });

                // Set up change event
                select.addEventListener('change', () => {
                  const selectedBenchmark = benchmarks.find(b => b.name === select.value);
                  displayBenchmarkResults(selectedBenchmark);
                });

                // Display first benchmark by default
                if (benchmarks.length > 0) {
                  displayBenchmarkResults(benchmarks[0]);
                }
              }

              // Display benchmark results
              function displayBenchmarkResults(benchmark) {
                const latestResults = document.getElementById('latest-results');
                latestResults.innerHTML = '';

                // Create table for results
                const table = document.createElement('table');
                table.style.width = '100%';
                table.style.borderCollapse = 'collapse';

                // Create header row
                const headerRow = document.createElement('tr');
                ['Mode', 'Score', 'Error', 'Unit'].forEach(header => {
                  const th = document.createElement('th');
                  th.textContent = header;
                  th.style.padding = '8px';
                  th.style.borderBottom = '1px solid #ddd';
                  th.style.textAlign = 'left';
                  headerRow.appendChild(th);
                });
                table.appendChild(headerRow);

                // Add data rows
                Object.entries(benchmark.modes).forEach(([mode, data]) => {
                  const row = document.createElement('tr');

                  const modeCell = document.createElement('td');
                  modeCell.textContent = mode;
                  modeCell.style.padding = '8px';
                  modeCell.style.borderBottom = '1px solid #ddd';
                  row.appendChild(modeCell);

                  const scoreCell = document.createElement('td');
                  scoreCell.textContent = data.score.toFixed(3);
                  scoreCell.style.padding = '8px';
                  scoreCell.style.borderBottom = '1px solid #ddd';
                  row.appendChild(scoreCell);

                  const errorCell = document.createElement('td');
                  errorCell.textContent = 'Â±' + data.error.toFixed(3);
                  errorCell.style.padding = '8px';
                  errorCell.style.borderBottom = '1px solid #ddd';
                  row.appendChild(errorCell);

                  const unitCell = document.createElement('td');
                  unitCell.textContent = data.unit;
                  unitCell.style.padding = '8px';
                  unitCell.style.borderBottom = '1px solid #ddd';
                  row.appendChild(unitCell);

                  table.appendChild(row);
                });

                latestResults.appendChild(table);

                // Update chart
                updateChart(benchmark);
              }

              // Update chart with benchmark data
              function updateChart(benchmark) {
                const ctx = document.getElementById('benchmarkChart').getContext('2d');

                // Prepare data for chart
                const labels = Object.keys(benchmark.modes);
                const scores = labels.map(mode => benchmark.modes[mode].score);
                const errors = labels.map(mode => benchmark.modes[mode].error);

                // Create or update chart
                if (window.benchmarkChart) {
                  window.benchmarkChart.data.labels = labels;
                  window.benchmarkChart.data.datasets[0].data = scores;
                  window.benchmarkChart.data.datasets[0].label = benchmark.name;
                  window.benchmarkChart.update();
                } else {
                  window.benchmarkChart = new Chart(ctx, {
                    type: 'bar',
                    data: {
                      labels: labels,
                      datasets: [{
                        label: benchmark.name,
                        data: scores,
                        backgroundColor: 'rgba(54, 162, 235, 0.5)',
                        borderColor: 'rgba(54, 162, 235, 1)',
                        borderWidth: 1,
                        errorBars: {
                          show: true,
                          color: 'rgba(0, 0, 0, 0.2)',
                          lineWidth: 2
                        }
                      }]
                    },
                    options: {
                      responsive: true,
                      maintainAspectRatio: false,
                      scales: {
                        y: {
                          beginAtZero: true,
                          title: {
                            display: true,
                            text: 'Score'
                          }
                        }
                      }
                    }
                  });
                }
              }

              // Initialize visualization
              async function initVisualization() {
                const data = await fetchBenchmarkData();
                if (!data) {
                  document.getElementById('latest-results').innerHTML = 
                    '<p>No benchmark data available. Please run benchmarks first.</p>';
                  return;
                }

                const benchmarks = processBenchmarkData(data);
                if (benchmarks && benchmarks.length > 0) {
                  populateBenchmarkSelector(benchmarks);
                } else {
                  document.getElementById('latest-results').innerHTML = 
                    '<p>No benchmark data available or data format is incorrect.</p>';
                }
              }

              // Start visualization when page loads
              window.addEventListener('DOMContentLoaded', initVisualization);
            </script>
          </body>
          </html>
          EOL

      - name: Deploy to cuioss.github.io
        uses: JamesIves/github-pages-deploy-action@6c2d9db40f9296374acc17b90404b6e8864128c8 # v4.7.3
        with:
          folder: gh-pages
          repository-name: cuioss/cuioss.github.io
          target-folder: cui-jwt-validation/benchmarks
          branch: main
          token: ${{ secrets.PAGES_DEPLOY_TOKEN }}

      - name: Create benchmark badges
        run: |
          # Create directory for badges
          mkdir -p gh-pages/badges

          # Get current date for badge timestamp
          TIMESTAMP=$(date -u +"%Y-%m-%d")

          # Function to create a badge for a benchmark
          create_badge() {
            local benchmark_name=$1
            local display_name=$2
            local badge_name=$3
            local color=$4

            # Extract average time for benchmark (if available)
            local score=$(grep -o "\"$benchmark_name\".*\"score\":[0-9.]*" benchmark-results/jmh-result.json | grep -o '"score":[0-9.]*' | cut -d':' -f2 | head -1 || echo "N/A")

            if [ "$score" != "N/A" ]; then
              # Round to 2 decimal places
              local formatted_score=$(printf "%.2f" $score)

              # Create badge JSON with timestamp
              echo "{\"schemaVersion\":1,\"label\":\"$display_name\",\"message\":\"${formatted_score} ms ($TIMESTAMP)\",\"color\":\"$color\"}" > "gh-pages/badges/$badge_name.json"

              # Create badge markdown for README
              echo "[![$display_name](https://img.shields.io/endpoint?url=https://cuioss.github.io/cui-jwt-validation/benchmarks/badges/$badge_name.json)](https://cuioss.github.io/cui-jwt-validation/benchmarks/)" >> gh-pages/badge-markdown.txt

              echo "Created badge for $display_name: $formatted_score ms"
            else
              echo "Warning: Could not find benchmark results for $benchmark_name"
            fi
          }

          # Create header for badge markdown
          echo "## Benchmark Results ($TIMESTAMP)" > gh-pages/badge-markdown.txt

          # Create badges for key benchmarks
          create_badge "de.cuioss.jwt.validation.benchmark.TokenValidatorBenchmark.validateAccessToken" "Access Token Validation" "validator-badge" "blue"
          create_badge "de.cuioss.jwt.validation.benchmark.MultiIssuerValidatorBenchmark.validateToken" "Multi-Issuer Validation" "multi-issuer-badge" "green"
          create_badge "de.cuioss.jwt.validation.benchmark.JwksClientBenchmark.getKey" "JWKS Key Retrieval" "jwks-badge" "orange"

          # Create a combined badge for all benchmarks
          echo "{\"schemaVersion\":1,\"label\":\"JWT Benchmarks\",\"message\":\"Updated $TIMESTAMP\",\"color\":\"brightgreen\"}" > gh-pages/badges/all-benchmarks.json

          # Copy the validator badge to the root for backward compatibility
          cp -f gh-pages/badges/validator-badge.json gh-pages/validator-badge.json || true
