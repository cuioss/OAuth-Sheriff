= JWT Performance Scoring System
:toc:
:toclevels: 3
:sectnums:
:toc-title: Table of Contents

== Overview

The JWT validation library uses a comprehensive performance scoring system that combines multiple key performance indicators into a single, weighted score. This document describes the methodology, calculation, and interpretation of these performance metrics.

== Key Performance Indicators

The performance scoring system focuses on two critical metrics that represent the most important aspects of JWT validation performance:

=== 1. Token Throughput (Operations per Second)

**What it measures**: The number of token validations that can be performed per second under maximum concurrent load.

**Benchmark Configuration**:
[source,java]
----
@BenchmarkMode(Mode.Throughput)
@OutputTimeUnit(TimeUnit.SECONDS)
@Threads(Threads.MAX)
----

**Why it matters**: 
- Indicates how well the system performs under high load
- Critical for applications with many concurrent users
- Reflects the scalability of the validation process
- Shows how effectively the library utilizes system resources

**Typical Values**: 
- Good performance: >10,000 ops/sec
- Excellent performance: >50,000 ops/sec

=== 2. Average Validation Time (Microseconds)

**What it measures**: The average time required to validate a single token in a single-threaded environment.

**Benchmark Configuration**:
[source,java]
----
@BenchmarkMode(Mode.AverageTime)
@OutputTimeUnit(TimeUnit.MICROSECONDS)
@Threads(1)
----

**Why it matters**:
- Represents the baseline latency for token validation
- Important for request-response scenarios where low latency is critical
- Shows the efficiency of the validation algorithm itself
- Affects user experience in interactive applications

**Typical Values**:
- Good performance: <100 microseconds
- Excellent performance: <50 microseconds

== Performance Score Calculation

=== Weighted Formula

The performance score uses a weighted combination of both metrics:

[source,text]
----
Performance Score = (Throughput × 0.6) + (Latency_Inverted × 0.4)

Where:
- Throughput = Operations per second from concurrent benchmark
- Latency_Inverted = 1,000,000 ÷ Average_Time_Microseconds
----

=== Rationale for Weighting

The 60/40 weighting scheme reflects the relative importance of each metric:

**Throughput (60% weight)**:
- More critical for most production scenarios
- Represents real-world performance under load
- Indicates system scalability and resource efficiency
- Has higher impact on overall system capacity

**Latency (40% weight)**:
- Important for user experience
- Shows algorithmic efficiency
- Critical for latency-sensitive applications
- Provides baseline performance measurement

=== Latency Inversion

The average validation time is converted to "operations per second" equivalent for consistent scoring:

[source,text]
----
Latency_Inverted = 1,000,000 ÷ Average_Time_Microseconds
----

This inversion ensures that:
- Lower validation times contribute positively to the score
- The metric is in the same scale as throughput (ops/sec)
- Both metrics contribute positively to higher scores

== Example Calculations

=== Scenario 1: High Throughput, Good Latency
[source,text]
----
Throughput: 45,000 ops/sec
Average Time: 80 microseconds

Latency_Inverted = 1,000,000 ÷ 80 = 12,500 ops/sec
Performance Score = (45,000 × 0.6) + (12,500 × 0.4)
                  = 27,000 + 5,000 = 32,000
----

=== Scenario 2: Moderate Throughput, Excellent Latency
[source,text]
----
Throughput: 25,000 ops/sec
Average Time: 40 microseconds

Latency_Inverted = 1,000,000 ÷ 40 = 25,000 ops/sec
Performance Score = (25,000 × 0.6) + (25,000 × 0.4)
                  = 15,000 + 10,000 = 25,000
----

=== Scenario 3: Balanced Performance
[source,text]
----
Throughput: 35,000 ops/sec
Average Time: 60 microseconds

Latency_Inverted = 1,000,000 ÷ 60 = 16,667 ops/sec
Performance Score = (35,000 × 0.6) + (16,667 × 0.4)
                  = 21,000 + 6,667 = 27,667
----

== Score Interpretation

=== Performance Bands

[cols="1,1,3"]
|===
|Score Range |Performance Level |Description

|> 40,000
|Exceptional
|Outstanding performance suitable for high-scale applications

|30,000 - 40,000
|Excellent
|Very good performance for most production scenarios

|20,000 - 30,000
|Good
|Solid performance suitable for typical applications

|10,000 - 20,000
|Moderate
|Acceptable performance for low to medium load scenarios

|< 10,000
|Needs Improvement
|May require optimization for production use
|===

=== Badge Display Format

The performance badge displays the information in a compact format:

[source,text]
----
Performance Score: 32000 (45k ops/s, 80μs)
                   ↑      ↑         ↑
                   |      |         └─ Average validation time
                   |      └─ Throughput (rounded to thousands)
                   └─ Weighted performance score
----

== Implementation Details

=== Benchmark Class

The metrics are measured by `PerformanceIndicatorBenchmark.java`:

[source,java]
----
@Benchmark
@BenchmarkMode(Mode.Throughput)
@Threads(Threads.MAX)
public AccessTokenContent measureThroughput() {
    // Measures operations per second under concurrent load
}

@Benchmark
@BenchmarkMode(Mode.AverageTime)
@Threads(1)
public AccessTokenContent measureAverageTime() {
    // Measures average time per operation
}
----

=== Badge Generation

The GitHub Actions workflow extracts the metrics from JMH JSON results and calculates the score:

[source,bash]
----
# Extract metrics from JMH results
throughput=$(grep "measureThroughput" jmh-result.json | ...)
avg_time=$(grep "measureAverageTime" jmh-result.json | ...)

# Calculate performance score
latency_ops_per_sec=$(echo "1000000 / $avg_time" | bc -l)
performance_score=$(echo "($throughput * 0.6) + ($latency_ops_per_sec * 0.4)" | bc -l)
----

== Usage Guidelines

=== When to Use This Metric

The performance score is most useful for:

- **Regression Testing**: Detecting performance degradations in CI/CD pipelines
- **Release Comparisons**: Comparing performance between different versions
- **Optimization Tracking**: Measuring the impact of performance improvements
- **Capacity Planning**: Understanding system performance characteristics

=== Limitations

Consider these limitations when interpreting the score:

- **Environment Dependent**: Results vary based on hardware and system load
- **Workload Specific**: Based on standard test tokens, may not reflect all real-world scenarios
- **Single Library**: Doesn't account for network, database, or other application overhead
- **Synthetic Workload**: Uses generated test data rather than production tokens

=== Best Practices

1. **Trend Analysis**: Focus on trends over time rather than absolute values
2. **Environment Consistency**: Run benchmarks in consistent environments for meaningful comparisons
3. **Multiple Runs**: Consider multiple benchmark runs to account for variance
4. **Context Awareness**: Understand the test environment and workload when interpreting results

== Maintenance and Updates

=== Updating Weights

If the weighting scheme needs adjustment, modify the calculation in:

1. **Benchmark Class**: Update `calculatePerformanceScore()` method
2. **Workflow**: Update the badge creation script
3. **Documentation**: Update this document with new rationale

=== Adding New Metrics

To extend the scoring system:

1. Add new benchmark methods to `PerformanceIndicatorBenchmark`
2. Update the score calculation formula
3. Modify the badge generation workflow
4. Update documentation to reflect changes

== See Also

- link:README.adoc[Benchmark Visualization Template]
- link:../README.adoc[Benchmarking Module Overview]
- link:../../doc/specification/benchmark.adoc[Benchmark Specification]