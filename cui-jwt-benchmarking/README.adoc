= JWT Token Validation Benchmarking Module
:toc:
:toclevels: 3
:toc-title: Table of Contents
:sectnums:

This module contains benchmarking code for the JWT token validation library. It uses JMH (Java Microbenchmark Harness) to measure performance of various components.

== Overview

The benchmarking module provides performance measurements for key components of the JWT validation library:

* Token validation performance (average time and throughput)
* JWKS key retrieval performance  
* Multi-issuer validation performance
* Error handling performance
* Concurrent validation performance
* **Performance Score**: A weighted metric combining throughput and latency

== Running Benchmarks Manually

Benchmarks are configured to be skipped by default to prevent them from slowing down regular builds. To run the benchmarks manually, you need to explicitly enable them.

=== Using Maven Command Line

To run benchmarks from the command line:

[source,bash]
----
# From the project root directory
./mvnw clean verify -pl cui-jwt-benchmarking -Dskip.benchmark=false

# With custom JMH parameters
./mvnw clean verify -pl cui-jwt-benchmarking -Dskip.benchmark=false \
  -Djmh.iterations=5 \
  -Djmh.warmupIterations=3 \
  -Djmh.forks=2 \
  -Djmh.threads=4
----

=== JMH Parameters

You can customize the benchmark execution with the following JMH parameters:

* `jmh.iterations` - Number of measurement iterations (default: 5)
* `jmh.warmupIterations` - Number of warmup iterations (default: 3)
* `jmh.forks` - Number of JVM forks (default: 2)
* `jmh.threads` - Number of threads (default: 4)
* `jmh.result.format` - Result format (default: JSON)
* `jmh.result.filePrefix` - Result file prefix

=== Running Specific Benchmarks

To run specific benchmarks, you can use the `jmh.includes` parameter:

[source,bash]
----
./mvnw clean verify -pl cui-jwt-benchmarking -Dskip.benchmark=false \
  -Djmh.includes=TokenValidatorBenchmark
----

== Benchmark Results

After running the benchmarks, results will be available in the `target` directory. If you've configured JSON output, you can visualize the results using the HTML visualizer template included in this module.

== Benchmark Classes

=== TokenValidatorBenchmark

Measures the performance of token validation operations.

=== JwksClientBenchmark

Measures the performance of JWKS key retrieval operations.

=== MultiIssuerValidatorBenchmark

Measures the performance of multi-issuer token validation.

=== ErrorLoadBenchmark

Measures the performance impact of error handling.

=== ConcurrentTokenValidationBenchmark

Measures the performance of concurrent token validation.

== GitHub Actions Integration

This module is configured to run benchmarks automatically via GitHub Actions:

* On merges to the main branch
* On version tag pushes
* Manually via workflow dispatch

Results are published to GitHub Pages for visualization and comparison over time.

== Performance Tracking System

The benchmarking module includes an advanced performance tracking system that monitors trends over time:

=== Individual Run Tracking

Each benchmark run creates a timestamped JSON file containing:

* Performance metrics (score, throughput, latency, error resilience)
* Environment information (Java version, OS, commit hash)
* Raw JMH measurement data

Files are stored as `performance-YYYYMMDD-HHMMSS.json` in the tracking directory.

=== Consolidated Tracking

A master `performance-tracking.json` file maintains the last 10 benchmark runs for trend analysis.

=== Performance Trends Visualization

Access comprehensive performance trends at: https://cuioss.github.io/cui-jwt/benchmarks/trends.html

**Features:**
* Interactive charts for all key metrics
* Trend indicators with percentage changes
* Percentage change calculation between first and last of last 10 runs
* Technical variation filtering for accurate trend detection

=== Performance Trend Badge

The **Performance Trend** badge shows:
* **↗ X.X% (improving)** - Green badge for performance gains > 2%
* **↘ X.X% (declining)** - Orange badge for performance drops > 2%  
* **→ X.X% (stable)** - Grey badge for changes ≤ 2%

Clicking the badge opens the detailed trends visualization.

== Understand Performance Metrics

The benchmarking module includes a comprehensive performance scoring system that combines multiple metrics into a single indicator. This system focuses on the most critical performance aspects of JWT validation.

=== Performance Score Composition

The **JWT Performance Score** badge displayed in the main README shows a weighted metric combining:

**1. Token Throughput (57% weight)**
- Operations per second under maximum concurrent load
- Measures scalability under high-volume scenarios
- Higher values indicate better concurrent performance

**2. Average Validation Time (40% weight)**  
- Milliseconds per operation in single-threaded scenarios
- Measures baseline validation latency
- Lower values indicate faster individual validations

**3. Error Resilience (3% weight)**
- Performance stability when processing invalid tokens
- Ensures consistent performance under error conditions
- Critical for production reliability

=== Weighted Score Calculation

The performance score uses the enhanced formula:
```
Score = (Throughput × 0.57) + ((1,000,000 ÷ AvgTimeμs) × 0.40) + (ErrorResilience × 0.03)
```

For backward compatibility, when error resilience data is unavailable:
```
Score = (Throughput × 0.60) + ((1,000,000 ÷ AvgTimeμs) × 0.40)
```

=== Performance Badge Format

The system automatically generates a performance badge that shows:
- **Weighted Performance Score**: Combined metric for overall performance
- **Throughput**: Raw operations per second (k ops/s format)
- **Average Time**: Raw validation time in milliseconds

Example: `Performance Score: 10,843 (13.7k ops/s, 0.15ms)`

=== Understanding the Metrics

**High Performance Score Indicators:**
- **Score > 10,000**: Excellent performance suitable for high-volume production
- **Throughput > 10k ops/s**: Good concurrent processing capability  
- **Average Time < 0.2ms**: Fast individual token validation

**Interpreting Changes:**
- **Score increases**: Overall performance improvement
- **Throughput increases**: Better concurrency handling
- **Average time decreases**: Faster individual operations

== Documentation

For detailed information about the benchmarking system:

* link:doc/performance-scoring.adoc[Performance Scoring System] - Complete methodology and calculation details
* link:doc/README.adoc[Benchmark Documentation] - Visualization templates and additional documentation

=== Templates and Scripts

The `doc/templates/` directory contains:

**HTML Templates:**
* `index-visualizer.html` - JMH Visualizer integration template
* `performance-trends.html` - Interactive performance trends visualization

**Data Templates:**
* `performance-run.json` - Individual benchmark run data template (uses `envsubst` variables)

**Processing Scripts:**
* `scripts/create-performance-badge.sh` - Generates comprehensive performance badge from JMH results
* `scripts/create-performance-tracking.sh` - Creates timestamped performance tracking files 
* `scripts/update-performance-trends.sh` - Updates consolidated tracking and manages trend analysis
* `scripts/calculate-trend-badge.sh` - Calculates trend metrics and generates trend badge

The scripts architecture separates complex badge and tracking logic from the GitHub workflow, making the system more maintainable and testable.