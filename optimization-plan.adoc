= JWT Integration Test Performance Optimization Plan
:toc: left
:toclevels: 3
:toc-title: Table of Contents
:sectnums:
:source-highlighter: highlight.js

== Executive Summary

**Focus**: Integration test performance optimization for Quarkus JWT validation endpoint

**Target**: Reduce P95 latency from 186.6ms to <20ms through infrastructure and framework optimization

**Key Finding**: 97% of request latency comes from framework/infrastructure overhead, not JWT library

== Current Performance Baseline (July 26, 2025)

=== Integration Test Results

**Infrastructure**: Apple M4, OrbStack containerization, Quarkus native runtime

|===
| Endpoint Type | Throughput | P95 Latency | Description | Overhead Analysis
| **Health Check** | 32,392 req/sec | 4.7ms | Minimal overhead baseline | 0ms (baseline)
| **Echo Endpoint** | 9,988 req/sec | 59.0ms | CDI + virtual threads | +54.3ms framework
| **JWT Validation** | 1,314 req/sec | 186.6ms | Full JWT processing | +127.6ms JWT pipeline
|===

=== Performance Gap Analysis

**Integration vs Microbenchmarks (Updated July 29, 2025)**:
- **Microbenchmarks**: 57Œºs P50, 27.3ms P99 (pure library) - see xref:optimization-plan-mb.adoc[Microbenchmark Optimization Plan]
- **Integration Tests**: 186.6ms P95 latency
- **Gap**: 3,270x difference (P50) indicates infrastructure dominates performance
- **Note**: Microbenchmarks achieved 66% improvement (90,347 ops/s) with thread optimization

=== Latency Breakdown (186.6ms P95 Total)

```
Total P95 Latency:           186.6ms (100%)
‚îú‚îÄ Infrastructure (Echo):     59.0ms (32%) - CDI + Virtual Threads
‚îú‚îÄ JWT Library:               27.3ms (15%) - Core validation logic (P99 from microbenchmarks)
‚îú‚îÄ Issuer Resolution:         21.5ms (12%) - Config lookup/caching
‚îú‚îÄ HTTP Processing:           26.7ms (14%) - Request/response handling
‚îî‚îÄ Untracked Overhead:       ~52.1ms (28%) - Thread pools, GC, contention
```

== Critical Integration Test Bottlenecks

=== 1. Virtual Thread + CDI Overhead üî¥ **MAJOR BOTTLENECK**

**Problem**: 54.3ms latency increase from virtual thread scheduling and CDI RequestScoped instantiation

**Impact**: 
- 69% throughput reduction (32,392 ‚Üí 9,988 req/sec)
- Single largest controllable overhead source

**Root Causes**:
- CDI RequestScoped bean instantiation per request
- Virtual thread scheduling latency under load
- Dependency injection overhead in request context

=== 2. Issuer Configuration Resolution üî¥ **JWT-SPECIFIC BOTTLENECK**

**Problem**: 21.5ms average for issuer config lookup

**Analysis**:
- Largest JWT-specific performance impact (12% of total latency)
- May involve HTTP calls to issuer endpoints
- Potential caching inefficiencies under concurrent load

=== 3. Untracked Overhead üî¥ **INVESTIGATION REQUIRED**

**Problem**: 61.7ms (33%) of request time unaccounted for

**Likely Sources**:
- Thread pool queuing delays
- GC pause accumulation  
- Network/TLS overhead
- Lock contention in framework components
- Memory allocation pressure

== Priority Action Plan

=== Priority 1: CDI and Virtual Thread Optimization üöÄ **IMMEDIATE ACTION**

**Goal**: Reduce framework overhead from 54.3ms to <10ms

**Investigation Tasks**:

1. **CDI Scope Optimization**:
   ```java
   // Investigate changing from RequestScoped to ApplicationScoped
   @ApplicationScoped  // vs @RequestScoped
   public class JwtValidationService {
   ```

2. **Virtual Thread Configuration**:
   ```properties
   # Optimize virtual thread settings
   quarkus.virtual-threads.name-prefix=jwt-worker
   quarkus.virtual-threads.shutdown-timeout=5s
   ```

3. **Bean Instantiation Profiling**:
   - Profile CDI bean creation overhead
   - Identify expensive dependency injections
   - Consider bean pre-instantiation strategies

**Expected Impact**: 30-40ms latency reduction (20-25% improvement)

=== Priority 2: Issuer Resolution Optimization üìä **HIGH PRIORITY**

**Goal**: Reduce issuer config resolution from 21.5ms to <2ms

**Action Items**:

1. **Caching Analysis**:
   - Profile issuer config cache hit/miss rates
   - Optimize cache warming strategies
   - Implement cache preloading for known issuers

2. **Network Operation Review**:
   - Identify HTTP calls in issuer resolution
   - Implement connection pooling optimizations
   - Add timeout configurations for issuer lookups

3. **Config Resolution Async Pattern**:
   ```java
   // Consider async issuer resolution
   CompletableFuture<IssuerConfig> resolveIssuerAsync(String issuer)
   ```

**Expected Impact**: 15-20ms latency reduction (10-12% improvement)

=== Priority 3: Untracked Overhead Investigation üîç **INVESTIGATION FOCUS**

**Goal**: Identify and optimize the 61.7ms unaccounted overhead

**Investigation Strategy**:

1. **JFR Profiling for Integration Tests**:
   ```bash
   # Run integration benchmarks with comprehensive JFR
   ./mvnw --no-transfer-progress clean verify -pl cui-jwt-benchmarking -Pbenchmark \
     -Djfr.duration=300s -Djfr.settings=profile
   ```

2. **GC Impact Analysis**:
   - Monitor GC pause frequency during load tests
   - Profile memory allocation patterns
   - Optimize heap sizing for integration test load

3. **Thread Pool Monitoring**:
   - Analyze thread pool saturation metrics
   - Monitor queue depths in HTTP processing
   - Profile lock contention in Quarkus components

4. **Network/TLS Profiling**:
   - Measure TLS handshake overhead
   - Profile HTTP connection reuse
   - Analyze network latency patterns

**Expected Impact**: 30-40ms latency reduction (20-25% improvement)

=== Priority 4: HTTP Processing Optimization ‚ö†Ô∏è **MEDIUM PRIORITY**

**Goal**: Reduce HTTP processing overhead from 26.7ms to <10ms

**Optimization Areas**:

1. **Request Processing Pipeline**:
   - Optimize HTTP header parsing
   - Reduce Authorization header extraction overhead
   - Streamline Bearer token extraction logic

2. **Response Generation**:
   - Minimize response serialization overhead
   - Optimize content-type handling
   - Reduce response header generation

**Expected Impact**: 10-15ms latency reduction (5-8% improvement)

== Success Metrics and Timeline

=== Target Performance Goals

|===
| Metric | Current | Target | Improvement
| **P95 Latency** | 186.6ms | <20ms | 89% reduction
| **Throughput** | 1,314 req/sec | >5,000 req/sec | 280% increase  
| **Framework Overhead** | 54.3ms | <10ms | 82% reduction
| **Issuer Resolution** | 21.5ms | <2ms | 91% reduction
|===

=== Implementation Timeline

==== Week 1: CDI and Virtual Thread Focus
- Profile CDI RequestScoped overhead
- Experiment with ApplicationScoped alternatives
- Optimize virtual thread configuration
- Measure framework overhead reduction

==== Week 2: Issuer Resolution and Caching
- Deep-dive issuer config resolution profiling
- Implement advanced caching strategies  
- Optimize network operations in issuer lookup
- Add async patterns where beneficial

==== Week 3: Untracked Overhead Investigation
- Comprehensive JFR profiling of integration tests
- GC and memory allocation optimization
- Thread pool and contention analysis
- Infrastructure tuning based on findings

==== Week 4: Validation and Final Optimization
- Run complete integration benchmark suite
- Validate target metrics achievement
- Fine-tune based on results
- Document optimization techniques

=== Validation Protocol

**Benchmark Execution**:
```bash
# Run comprehensive integration benchmarks
./mvnw --no-transfer-progress clean verify -pl cui-jwt-benchmarking -Pbenchmark
```

**Success Criteria**:
- P95 latency: <20ms (from 186.6ms)
- Throughput: >5,000 req/sec (from 1,314)
- Framework overhead: <10ms (from 54.3ms)
- Consistent results across multiple runs

== Dismissed Integration Test Optimization Approaches

=== BouncyCastle JCA Provider for Integration Tests
**Status:** ‚ùå DISMISSED - Adds complexity without addressing framework overhead

**Reason:** Integration tests show 97% of latency comes from framework/infrastructure (CDI, virtual threads, HTTP processing). BouncyCastle provider optimizes the 9% JWT library portion but cannot address the 54.3ms CDI overhead or 61.7ms untracked infrastructure overhead. Focus should be on framework optimization.

=== Profile-Guided Optimization (PGO) for Quarkus Native
**Status:** ‚ùå DISMISSED - Not available in Mandrel, limited benefit for infrastructure bottlenecks

**Reason:** PGO is only available in Oracle GraalVM Enterprise Edition. Quarkus uses Mandrel (GraalVM Community Edition) by default. Even if available, PGO optimizes CPU-intensive code paths, but 91% of integration test latency comes from framework overhead (CDI, HTTP, untracked), not CPU-bound JWT operations.

=== Container Optimization Beyond OrbStack
**Status:** ‚ùå DISMISSED - OrbStack already provides excellent baseline performance

**Reason:** Health check endpoint achieves 4.7ms P95 latency (32,392 req/sec), indicating container infrastructure is highly optimized. The 182ms additional latency comes from application-level concerns (CDI, JWT processing, HTTP handling), not container overhead.

=== Native Image Compilation Flags Tuning
**Status:** ‚ùå DISMISSED - Infrastructure overhead dominates, not native compilation efficiency

**Reason:** Native image optimization (memory settings, compilation flags) targets CPU and memory efficiency of compiled code. However, 91% of latency is framework overhead where native compilation efficiency has minimal impact. CDI RequestScoped instantiation and virtual thread scheduling are not improved by native compilation tuning.

=== HTTP/2 and Connection Optimization
**Status:** ‚ùå DISMISSED - Single request latency focus, not connection reuse

**Reason:** Integration test measures single request P95 latency (186.6ms), not sustained throughput over persistent connections. HTTP/2 and connection pooling optimize multi-request scenarios but don't address per-request processing overhead in CDI, JWT validation, and response generation.

=== JVM Garbage Collector Tuning for Integration Tests
**Status:** ‚ùå DISMISSED - GC impact appears in untracked overhead, requires investigation first

**Reason:** The 61.7ms untracked overhead may include GC pauses, but changing GC algorithms (G1, ZGC, Shenandoah) without identifying GC as the root cause is premature. JFR profiling must first confirm GC contribution to the untracked latency before tuning.

=== Quarkus Virtual Thread Replacement with Platform Threads
**Status:** ‚ùå DISMISSED - Virtual threads are architectural choice, not optimization target

**Reason:** Virtual threads in Quarkus provide scalability benefits for I/O-bound workloads. The 54.3ms virtual thread overhead likely comes from CDI integration and bean instantiation, not virtual thread mechanics. Optimization should focus on CDI scope management rather than threading model change.

=== Database/External Service Caching for Issuer Resolution
**Status:** ‚ùå DISMISSED - 21.5ms issuer resolution suggests local processing, not external calls

**Reason:** Issuer config resolution shows 21.5ms average latency. If this involved database or HTTP calls, latency would be higher and more variable. This appears to be local config processing overhead that requires profiling and algorithmic optimization, not external service caching.

== Related Documentation

- **Microbenchmark Optimization**: xref:optimization-plan-mb.adoc[Core Library Performance Plan] - Updated July 29, 2025 with 57Œºs P50, 27.3ms P99 results
- **Benchmark Analysis**: xref:cui-jwt-benchmarking/analysis.md[Detailed Performance Analysis] - Comprehensive breakdown of component performance
- **Infrastructure Details**: Container and native compilation optimization status
- **Benchmark Infrastructure**: Maven-based execution with JFR profiling capabilities

== Key Strategy

**Integration-First Approach**: Since 97% of latency comes from framework/infrastructure, optimization efforts focus on:

1. **CDI and Virtual Thread efficiency** (32% of total latency)
2. **Issuer resolution optimization** (12% of total latency)  
3. **Untracked overhead investigation** (33% of total latency)
4. **HTTP processing streamlining** (14% of total latency)

**Library optimization** (9% of latency) is addressed separately in the microbenchmark optimization plan.

**Expected Result**: 89% latency reduction through systematic infrastructure optimization while maintaining security and functionality.